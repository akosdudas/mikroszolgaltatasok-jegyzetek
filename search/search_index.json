{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":true,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Jegyzetek, feladatok \u00e9s p\u00e9lda k\u00f3dok BMEVIAUAV42 Mikroszolg\u00e1ltat\u00e1sok \u00e9s kont\u00e9neralap\u00fa szoftverfejleszt\u00e9s t\u00e1rgyhoz. A f\u00e9l\u00e9v \u00fctemez\u00e9s\u00e9t l\u00e1sd itt . Bevezet\u0151 el\u0151ad\u00e1s : motiv\u00e1ci\u00f3, architekt\u00fara alapjai. Az itt tal\u00e1lhat\u00f3 oktat\u00e1si seg\u00e9danyagok a BMEVIAUAV42 t\u00e1rgy hallgat\u00f3inak k\u00e9sz\u00fcltek. Az anyagok oly m\u00f3d\u00fa felhaszn\u00e1l\u00e1sa, amely a t\u00e1rgy oktat\u00e1s\u00e1hoz nem szorosan kapcsol\u00f3dik, csak a szerz\u0151(k) \u00e9s a forr\u00e1s megjel\u00f6l\u00e9s\u00e9vel t\u00f6rt\u00e9nhet. Az anyagok a t\u00e1rgy keret\u00e9ben oktatott kontextusban \u00e9rtelmezhet\u0151ek. Az anyagok\u00e9rt egy\u00e9b felhaszn\u00e1l\u00e1s eset\u00e9n a szerz\u0151(k) felel\u0151ss\u00e9get nem v\u00e1llalnak.","title":"T\u00e1rgy ismertet\u0151"},{"location":"hazi-feladat/","text":"A h\u00e1zi feladat otthon, \u00f6n\u00e1ll\u00f3an elk\u00e9sz\u00edtend\u0151 mikroszolg\u00e1ltat\u00e1sok architekt\u00far\u00e1ra \u00e9p\u00fcl\u0151 \u00e9s kont\u00e9nertechnol\u00f3gi\u00e1t haszn\u00e1l\u00f3 szoftverrendszer elk\u00e9sz\u00edt\u00e9se \u00e9s m\u0171k\u00f6d\u0151k\u00e9pes \u00e1llapotban val\u00f3 bemutat\u00e1sa . A rendszer egyetlen alkalmaz\u00e1st kell megval\u00f3s\u00edtson, ahol az alkalmaz\u00e1s funkci\u00f3k szerint t\u00f6bb szolg\u00e1ltat\u00e1sra van darabolva. Az alkalmaz\u00e1s nem felt\u00e9tlen\u00fcl kell felhaszn\u00e1l\u00f3i fel\u00fclettel rendelkezzen, de en\u00e9lk\u00fcl is tudni kell demonstr\u00e1lni a m\u0171k\u00f6d\u00e9s\u00e9t (pl. REST API-n kereszt\u00fcl). Az elk\u00e9sz\u00edtett rendszer egyes k\u00e9pess\u00e9geire az al\u00e1bbiak szerint pontok kaphat\u00f3ak. R\u00e9szpontsz\u00e1m csak k\u00fcl\u00f6nleges esetben kaphat\u00f3. A v\u00e9gs\u0151 jegy az \u00f6sszpontsz\u00e1mb\u00f3l ad\u00f3dik. Megl\u00e9v\u0151 (pl. Microsoft-os) dem\u00f3k, mintaalkalmaz\u00e1sok (elemei) felhaszn\u00e1lhat\u00f3k, de ezt k\u00fcl\u00f6n jelezni kell bemutat\u00e1skor. A nem jelzett, de \u00e1tvett r\u00e9szletek pl\u00e1giumnak sz\u00e1m\u00edtanak. Az \u00f3rai dem\u00f3kban vagy mintaalkalmaz\u00e1sban megval\u00f3s\u00edtott funkci\u00f3k \u00e1tv\u00e9tel\u00e9\u00e9rt pont nem adhat\u00f3, de azok tov\u00e1bb \u00e1tdolgozhat\u00f3ak saj\u00e1t implement\u00e1ci\u00f3nak. A pontrendszer \u00e1ltalatok is m\u00f3dos\u00edthat\u00f3/b\u0151v\u00edthet\u0151. Ezt Pull Request form\u00e1j\u00e1ban egy megfelel\u0151 indokl\u00e1ssal ny\u00fajthatj\u00e1tok be, de a PR beny\u00fajt\u00e1sa nem jelenti annak az automatikus elfogad\u00e1s\u00e1t, arr\u00f3l minden esetben a t\u00e1rgy oktat\u00f3i d\u00f6ntenek. Jegy sz\u00e1m\u00edt\u00e1s \u00b6 0-24: nem teljes\u00edtette 25-29: el\u00e9gs\u00e9ges 30-34: k\u00f6zepes 35-40: j\u00f3 41-: jeles Pontok az al\u00e1bbiak\u00e9rt kaphat\u00f3ak \u00b6 Mikroszolg\u00e1ltatat\u00e1sok architekt\u00fara: 5+3 pont \u00b6 A rendszer t\u00f6bb, f\u00fcggetlen mikroszolg\u00e1ltat\u00e1sb\u00f3l \u00e9p\u00fcl fel. Ebbe bele\u00e9rtend\u0151 a frontend is, amennyiben azt a t\u00f6bbit\u0151l f\u00fcggetlen webszerver szolg\u00e1lja ki, de az adatb\u00e1zis szerver(ek) k\u00fcl\u00f6n nem sz\u00e1m\u00edtanak bele. Minimum 3 szolg\u00e1ltat\u00e1ssal: 5 pont Minimum 4 szolg\u00e1ltat\u00e1ssal: +3 pont A pont a t\u00f6bbnyire korrekt rendszer part\u00edcion\u00e1l\u00e1s eset\u00e9ben j\u00e1r. (Egyszer\u0171 t\u00f6bbr\u00e9teg\u0171 architekt\u00far\u00e1ra nem j\u00e1r a pont, akkor se, ha a r\u00e9tegek k\u00fcl\u00f6n szolg\u00e1ltat\u00e1sokban kapnak helyet!) T\u00f6bb implement\u00e1ci\u00f3s nyelv haszn\u00e1lata: 5 pont \u00b6 A backend szolg\u00e1ltat\u00e1sok legal\u00e1bb 2 k\u00fcl\u00f6nb\u00f6z\u0151 nyelven k\u00e9sz\u00fcltek. (A frontend ebbe nem sz\u00e1m\u00edt bele!) Kont\u00e9nerekben t\u00f6rt\u00e9n\u0151 futtat\u00e1s: 7 pont \u00b6 Minden szolg\u00e1ltat\u00e1s kont\u00e9nerben fut, bele\u00e9rtve a haszn\u00e1lt adatb\u00e1zis rendszereket is. R\u00e9szpontsz\u00e1m kaphat\u00f3, ha nem minden szolg\u00e1ltat\u00e1s kont\u00e9neriz\u00e1lt. Legal\u00e1bb k\u00e9t fajta adatb\u00e1zis haszn\u00e1lata: 5 pont \u00b6 K\u00e9t elt\u00e9r\u0151 technol\u00f3gi\u00e1j\u00fa adatb\u00e1zis haszn\u00e1lata perzisztenci\u00e1ra. Egyik lehet rel\u00e1ci\u00f3s is. (Redis csak cache-k\u00e9nt val\u00f3 haszn\u00e1lata ebbe nem sz\u00e1m\u00edt bele.) Redis alap\u00fa cache haszn\u00e1lata: 4 pont \u00b6 Redis haszn\u00e1lata kifejezetten cache-el\u00e9sre legal\u00e1bb 1 m\u0171velet eset\u00e9n. Http/gRPC alap\u00fa kommunik\u00e1ci\u00f3 mikroszolg\u00e1ltat\u00e1sok k\u00f6z\u00f6tt: 4+4 pont \u00b6 Legal\u00e1bb egy olyan m\u0171velet, amelyben egy mikroszolg\u00e1ltat\u00e1s egy m\u00e1sikkal http alapon kommunik\u00e1l: 4 pont Legal\u00e1bb egy olyan m\u0171velet, amelyben egy mikroszolg\u00e1ltat\u00e1s egy m\u00e1sikkal gRPC alapon kommunik\u00e1l: 4 pont Polly vagy hasonl\u00f3 technol\u00f3gia alkalmaz\u00e1sa a mikroszolg\u00e1ltat\u00e1sok k\u00f6z\u00f6tti kommunik\u00e1ci\u00f3ra: 3 pont \u00b6 Az el\u0151z\u0151vel egy\u00fctt teljes\u00edthet\u0151. A mikroszolg\u00e1ltat\u00e1sok k\u00f6z\u00f6tti kommunik\u00e1ci\u00f3 sor\u00e1n Backoff/Retry/Throttling/... mint\u00e1k alkalmaz\u00e1sa. \u00dczenetsor alap\u00fa kommunik\u00e1ci\u00f3 mikroszolg\u00e1ltat\u00e1sok k\u00f6z\u00f6tt: 5 pont \u00b6 Legal\u00e1bb egy olyan m\u0171velet, amelyben egy mikroszolg\u00e1ltat\u00e1s egy m\u00e1sikkal \u00fczenetsor alapon kommunik\u00e1l (mind a termel\u0151, mind a fogyaszt\u00f3 oldalt bele\u00e9rtve). Kommunik\u00e1ci\u00f3s platform lehet Redis, RabbitMQ, ill. b\u00e1rmely ismert message queue szolg\u00e1ltat\u00e1s. API Gateway haszn\u00e1lata: 5+3 pont \u00b6 Traefik haszn\u00e1lata \u00fatvonalv\u00e1laszt\u00e1sra: 5 pont M\u00e1s API gateway haszn\u00e1lata: +3 pont Forward authentik\u00e1ci\u00f3: 5 pont \u00b6 Az el\u0151z\u0151n fel\u00fcl teljes\u00edthet\u0151. API Gateway eset\u00e9n az authentik\u00e1ci\u00f3t forward authentication m\u00f3dszerrel az API Gateway biztos\u00edtja. (Nem sz\u00fcks\u00e9ges teljes, val\u00f3di authentik\u00e1ci\u00f3, mint p\u00e9ld\u00e1ul JWT haszn\u00e1lata. El\u00e9g, ha a m\u0171k\u00f6d\u00e9s valamilyen m\u00f3don demonstr\u00e1lhat\u00f3.) Futtat\u00e1s docker-compose alapokon: 5 pont \u00b6 Az \u00f6sszes szolg\u00e1ltat\u00e1s egyetlen compose parancs seg\u00edts\u00e9g\u00e9vel elind\u00edthat\u00f3. Futtat\u00e1s Kubernetes-ben: 10 pont \u00b6 A bemutat\u00e1s sor\u00e1n az alkalmaz\u00e1s Kubernetes-ben fut. (Ak\u00e1r helyben, ak\u00e1r felh\u0151ben.) Futtat\u00e1s publikus felh\u0151ben Kubernetes-ben: 5 pont \u00b6 Az el\u0151z\u0151n fel\u00fcl teljes\u00edthet\u0151. Az alkalmaz\u00e1s publikus felh\u0151ben hostolt Kubernetes platformon fut, \u00e9rv\u00e9nyes (nem self-signed) HTTPS tanus\u00edtv\u00e1nnyal rendelkezik, \u00e9s publikus domain n\u00e9ven kereszt\u00fcl el\u00e9rhet\u0151. Helm chart Kubernetes-hez: 7 pont \u00b6 A Kubernetes-be t\u00f6rt\u00e9n\u0151 telep\u00edt\u00e9st helm chart v\u00e9gzi. Sz\u00fcks\u00e9ges demonstr\u00e1lni a rendszer friss\u00edt\u00e9s\u00e9t a chart seg\u00edts\u00e9g\u00e9vel. Kubernetes-ben Job, CronJob, ConfigMap, Secret haszn\u00e1lata: 5 pont \u00b6 Legal\u00e1bb egy Job, CronJob, ConfigMap, vagy Secret haszn\u00e1lata az alkalmaz\u00e1sban. OpenTracing (pl. Jaeger) be\u00fczemel\u00e9se: 5 pont \u00b6 Az alkalmaz\u00e1sban k\u00f6vethet\u0151ek a k\u00e9r\u00e9sek pl. Jaeger haszn\u00e1lat\u00e1val. Continuous Integration be\u00fczemel\u00e9se: 5 pont \u00b6 Az alkalmaz\u00e1s teljes eg\u00e9sze CI rendszerben lefordul \u00e9s kont\u00e9nerek k\u00e9sz\u00fclnek bel\u0151le. Web/mobil felhaszn\u00e1l\u00f3i fel\u00fclet: 5 pont \u00b6 Az alkalmaz\u00e1s rendelkezik modern webes / mobilos felhaszn\u00e1l\u00f3i fel\u00fclettel. Teljess\u00e9g \u00e9s ig\u00e9nyess\u00e9g f\u00fcggv\u00e9ny\u00e9ben r\u00e9szpontsz\u00e1m is kaphat\u00f3. Eventually Consistent elosztott m\u0171k\u00f6d\u00e9s: 5+3 pont \u00b6 A mikroszolg\u00e1ltat\u00e1sok k\u00f6z\u00f6tt eventually consistent \u00e1llapotkezel\u00e9s megval\u00f3s\u00edt\u00e1sa a kapcsol\u00f3d\u00f3 tervez\u00e9si mint\u00e1kkal. (5 pont) Gondoljunk a m\u0171veletek idempotens mivolt\u00e1ra is a tervez\u00e9s sor\u00e1n. (3 pont) Tervez\u00e9si mint\u00e1k haszn\u00e1lata az implement\u00e1ci\u00f3 sor\u00e1n: 5 pont \u00b6 Domain Driven Design (DDD), Command Query Segregation Principle (CQRS). Ezeken fel\u00fcl el\u0151zetes egyeztet\u00e9ssel a t\u00e1rgy oktat\u00f3ival. Visszacsatol\u00e1s: 0-1 pont, \u00f6sszesen max. 3 pont \u00b6 A v\u00e9gleges\u00edtett pontrendszer vagy tananyag jav\u00edt\u00e1sa, b\u0151v\u00edt\u00e9se, m\u00f3dos\u00edt\u00e1sa pull request-tel. Helyes\u00edr\u00e1si hiba is lehet, de az oktat\u00f3k d\u00f6ntenek, hogy pontot \u00e9r-e a m\u00f3dos\u00edt\u00e1s. T\u00f6bbsz\u00f6r is megszerezhet\u0151.","title":"H\u00e1zi feladat le\u00edr\u00e1sa \u00e9s megszerezhet\u0151 pontok"},{"location":"hazi-feladat/#jegy-szamitas","text":"0-24: nem teljes\u00edtette 25-29: el\u00e9gs\u00e9ges 30-34: k\u00f6zepes 35-40: j\u00f3 41-: jeles","title":"Jegy sz\u00e1m\u00edt\u00e1s"},{"location":"hazi-feladat/#pontok-az-alabbiakert-kaphatoak","text":"","title":"Pontok az al\u00e1bbiak\u00e9rt kaphat\u00f3ak"},{"location":"hazi-feladat/#mikroszolgaltatatasok-architektura-53-pont","text":"A rendszer t\u00f6bb, f\u00fcggetlen mikroszolg\u00e1ltat\u00e1sb\u00f3l \u00e9p\u00fcl fel. Ebbe bele\u00e9rtend\u0151 a frontend is, amennyiben azt a t\u00f6bbit\u0151l f\u00fcggetlen webszerver szolg\u00e1lja ki, de az adatb\u00e1zis szerver(ek) k\u00fcl\u00f6n nem sz\u00e1m\u00edtanak bele. Minimum 3 szolg\u00e1ltat\u00e1ssal: 5 pont Minimum 4 szolg\u00e1ltat\u00e1ssal: +3 pont A pont a t\u00f6bbnyire korrekt rendszer part\u00edcion\u00e1l\u00e1s eset\u00e9ben j\u00e1r. (Egyszer\u0171 t\u00f6bbr\u00e9teg\u0171 architekt\u00far\u00e1ra nem j\u00e1r a pont, akkor se, ha a r\u00e9tegek k\u00fcl\u00f6n szolg\u00e1ltat\u00e1sokban kapnak helyet!)","title":"Mikroszolg\u00e1ltatat\u00e1sok architekt\u00fara: 5+3 pont"},{"location":"hazi-feladat/#tobb-implementacios-nyelv-hasznalata-5-pont","text":"A backend szolg\u00e1ltat\u00e1sok legal\u00e1bb 2 k\u00fcl\u00f6nb\u00f6z\u0151 nyelven k\u00e9sz\u00fcltek. (A frontend ebbe nem sz\u00e1m\u00edt bele!)","title":"T\u00f6bb implement\u00e1ci\u00f3s nyelv haszn\u00e1lata: 5 pont"},{"location":"hazi-feladat/#kontenerekben-torteno-futtatas-7-pont","text":"Minden szolg\u00e1ltat\u00e1s kont\u00e9nerben fut, bele\u00e9rtve a haszn\u00e1lt adatb\u00e1zis rendszereket is. R\u00e9szpontsz\u00e1m kaphat\u00f3, ha nem minden szolg\u00e1ltat\u00e1s kont\u00e9neriz\u00e1lt.","title":"Kont\u00e9nerekben t\u00f6rt\u00e9n\u0151 futtat\u00e1s: 7 pont"},{"location":"hazi-feladat/#legalabb-ket-fajta-adatbazis-hasznalata-5-pont","text":"K\u00e9t elt\u00e9r\u0151 technol\u00f3gi\u00e1j\u00fa adatb\u00e1zis haszn\u00e1lata perzisztenci\u00e1ra. Egyik lehet rel\u00e1ci\u00f3s is. (Redis csak cache-k\u00e9nt val\u00f3 haszn\u00e1lata ebbe nem sz\u00e1m\u00edt bele.)","title":"Legal\u00e1bb k\u00e9t fajta adatb\u00e1zis haszn\u00e1lata: 5 pont"},{"location":"hazi-feladat/#redis-alapu-cache-hasznalata-4-pont","text":"Redis haszn\u00e1lata kifejezetten cache-el\u00e9sre legal\u00e1bb 1 m\u0171velet eset\u00e9n.","title":"Redis alap\u00fa cache haszn\u00e1lata: 4 pont"},{"location":"hazi-feladat/#httpgrpc-alapu-kommunikacio-mikroszolgaltatasok-kozott-44-pont","text":"Legal\u00e1bb egy olyan m\u0171velet, amelyben egy mikroszolg\u00e1ltat\u00e1s egy m\u00e1sikkal http alapon kommunik\u00e1l: 4 pont Legal\u00e1bb egy olyan m\u0171velet, amelyben egy mikroszolg\u00e1ltat\u00e1s egy m\u00e1sikkal gRPC alapon kommunik\u00e1l: 4 pont","title":"Http/gRPC alap\u00fa kommunik\u00e1ci\u00f3 mikroszolg\u00e1ltat\u00e1sok k\u00f6z\u00f6tt: 4+4 pont"},{"location":"hazi-feladat/#polly-vagy-hasonlo-technologia-alkalmazasa-a-mikroszolgaltatasok-kozotti-kommunikaciora-3-pont","text":"Az el\u0151z\u0151vel egy\u00fctt teljes\u00edthet\u0151. A mikroszolg\u00e1ltat\u00e1sok k\u00f6z\u00f6tti kommunik\u00e1ci\u00f3 sor\u00e1n Backoff/Retry/Throttling/... mint\u00e1k alkalmaz\u00e1sa.","title":"Polly vagy hasonl\u00f3 technol\u00f3gia alkalmaz\u00e1sa a mikroszolg\u00e1ltat\u00e1sok k\u00f6z\u00f6tti kommunik\u00e1ci\u00f3ra: 3 pont"},{"location":"hazi-feladat/#uzenetsor-alapu-kommunikacio-mikroszolgaltatasok-kozott-5-pont","text":"Legal\u00e1bb egy olyan m\u0171velet, amelyben egy mikroszolg\u00e1ltat\u00e1s egy m\u00e1sikkal \u00fczenetsor alapon kommunik\u00e1l (mind a termel\u0151, mind a fogyaszt\u00f3 oldalt bele\u00e9rtve). Kommunik\u00e1ci\u00f3s platform lehet Redis, RabbitMQ, ill. b\u00e1rmely ismert message queue szolg\u00e1ltat\u00e1s.","title":"\u00dczenetsor alap\u00fa kommunik\u00e1ci\u00f3 mikroszolg\u00e1ltat\u00e1sok k\u00f6z\u00f6tt: 5 pont"},{"location":"hazi-feladat/#api-gateway-hasznalata-53-pont","text":"Traefik haszn\u00e1lata \u00fatvonalv\u00e1laszt\u00e1sra: 5 pont M\u00e1s API gateway haszn\u00e1lata: +3 pont","title":"API Gateway haszn\u00e1lata: 5+3 pont"},{"location":"hazi-feladat/#forward-authentikacio-5-pont","text":"Az el\u0151z\u0151n fel\u00fcl teljes\u00edthet\u0151. API Gateway eset\u00e9n az authentik\u00e1ci\u00f3t forward authentication m\u00f3dszerrel az API Gateway biztos\u00edtja. (Nem sz\u00fcks\u00e9ges teljes, val\u00f3di authentik\u00e1ci\u00f3, mint p\u00e9ld\u00e1ul JWT haszn\u00e1lata. El\u00e9g, ha a m\u0171k\u00f6d\u00e9s valamilyen m\u00f3don demonstr\u00e1lhat\u00f3.)","title":"Forward authentik\u00e1ci\u00f3: 5 pont"},{"location":"hazi-feladat/#futtatas-docker-compose-alapokon-5-pont","text":"Az \u00f6sszes szolg\u00e1ltat\u00e1s egyetlen compose parancs seg\u00edts\u00e9g\u00e9vel elind\u00edthat\u00f3.","title":"Futtat\u00e1s docker-compose alapokon: 5 pont"},{"location":"hazi-feladat/#futtatas-kubernetes-ben-10-pont","text":"A bemutat\u00e1s sor\u00e1n az alkalmaz\u00e1s Kubernetes-ben fut. (Ak\u00e1r helyben, ak\u00e1r felh\u0151ben.)","title":"Futtat\u00e1s Kubernetes-ben: 10 pont"},{"location":"hazi-feladat/#futtatas-publikus-felhoben-kubernetes-ben-5-pont","text":"Az el\u0151z\u0151n fel\u00fcl teljes\u00edthet\u0151. Az alkalmaz\u00e1s publikus felh\u0151ben hostolt Kubernetes platformon fut, \u00e9rv\u00e9nyes (nem self-signed) HTTPS tanus\u00edtv\u00e1nnyal rendelkezik, \u00e9s publikus domain n\u00e9ven kereszt\u00fcl el\u00e9rhet\u0151.","title":"Futtat\u00e1s publikus felh\u0151ben Kubernetes-ben: 5 pont"},{"location":"hazi-feladat/#helm-chart-kubernetes-hez-7-pont","text":"A Kubernetes-be t\u00f6rt\u00e9n\u0151 telep\u00edt\u00e9st helm chart v\u00e9gzi. Sz\u00fcks\u00e9ges demonstr\u00e1lni a rendszer friss\u00edt\u00e9s\u00e9t a chart seg\u00edts\u00e9g\u00e9vel.","title":"Helm chart Kubernetes-hez: 7 pont"},{"location":"hazi-feladat/#kubernetes-ben-job-cronjob-configmap-secret-hasznalata-5-pont","text":"Legal\u00e1bb egy Job, CronJob, ConfigMap, vagy Secret haszn\u00e1lata az alkalmaz\u00e1sban.","title":"Kubernetes-ben Job, CronJob, ConfigMap, Secret haszn\u00e1lata: 5 pont"},{"location":"hazi-feladat/#opentracing-pl-jaeger-beuzemelese-5-pont","text":"Az alkalmaz\u00e1sban k\u00f6vethet\u0151ek a k\u00e9r\u00e9sek pl. Jaeger haszn\u00e1lat\u00e1val.","title":"OpenTracing (pl. Jaeger) be\u00fczemel\u00e9se: 5 pont"},{"location":"hazi-feladat/#continuous-integration-beuzemelese-5-pont","text":"Az alkalmaz\u00e1s teljes eg\u00e9sze CI rendszerben lefordul \u00e9s kont\u00e9nerek k\u00e9sz\u00fclnek bel\u0151le.","title":"Continuous Integration be\u00fczemel\u00e9se: 5 pont"},{"location":"hazi-feladat/#webmobil-felhasznaloi-felulet-5-pont","text":"Az alkalmaz\u00e1s rendelkezik modern webes / mobilos felhaszn\u00e1l\u00f3i fel\u00fclettel. Teljess\u00e9g \u00e9s ig\u00e9nyess\u00e9g f\u00fcggv\u00e9ny\u00e9ben r\u00e9szpontsz\u00e1m is kaphat\u00f3.","title":"Web/mobil felhaszn\u00e1l\u00f3i fel\u00fclet: 5 pont"},{"location":"hazi-feladat/#eventually-consistent-elosztott-mukodes-53-pont","text":"A mikroszolg\u00e1ltat\u00e1sok k\u00f6z\u00f6tt eventually consistent \u00e1llapotkezel\u00e9s megval\u00f3s\u00edt\u00e1sa a kapcsol\u00f3d\u00f3 tervez\u00e9si mint\u00e1kkal. (5 pont) Gondoljunk a m\u0171veletek idempotens mivolt\u00e1ra is a tervez\u00e9s sor\u00e1n. (3 pont)","title":"Eventually Consistent elosztott m\u0171k\u00f6d\u00e9s: 5+3 pont"},{"location":"hazi-feladat/#tervezesi-mintak-hasznalata-az-implementacio-soran-5-pont","text":"Domain Driven Design (DDD), Command Query Segregation Principle (CQRS). Ezeken fel\u00fcl el\u0151zetes egyeztet\u00e9ssel a t\u00e1rgy oktat\u00f3ival.","title":"Tervez\u00e9si mint\u00e1k haszn\u00e1lata az implement\u00e1ci\u00f3 sor\u00e1n: 5 pont"},{"location":"hazi-feladat/#visszacsatolas-0-1-pont-osszesen-max-3-pont","text":"A v\u00e9gleges\u00edtett pontrendszer vagy tananyag jav\u00edt\u00e1sa, b\u0151v\u00edt\u00e9se, m\u00f3dos\u00edt\u00e1sa pull request-tel. Helyes\u00edr\u00e1si hiba is lehet, de az oktat\u00f3k d\u00f6ntenek, hogy pontot \u00e9r-e a m\u00f3dos\u00edt\u00e1s. T\u00f6bbsz\u00f6r is megszerezhet\u0151.","title":"Visszacsatol\u00e1s: 0-1 pont, \u00f6sszesen max. 3 pont"},{"location":"AKS/aks/","text":"K8S DevOps Azure DevOps-ban \u00e9s Azure Kubernetes Service-szel \u00b6 T\u00e9mak\u00f6r\u00f6k \u00b6 Azure szolg\u00e1ltat\u00e1sok Azure DevOps Azure Kubernetes Service (multitenant) Azure SQL (multitenant) Terraform DevOps m\u0171veletek Azure DevOps-ban Forr\u00e1sk\u00f3d kezel\u00e9s Azure Repos-szal CI+CD Azure Pipelines-szal Monitoroz\u00e1s Azure Monitor-ral A labor menete \u00b6 A hivatalos Azure DevOps labor anyagot k\u00f6veti: https://www.azuredevopslabs.com/labs/vstsextend/kubernetes/ . Itt csak a kieg\u00e9sz\u00edt\u00e9seket, ill. egy v\u00e1zlatot \u00edrunk le. El\u0151k\u00e9sz\u00fcletek \u00b6 Azure DevOps szervezet l\u00e9trehoz\u00e1sa Be fog k\u00e9rni nevet, emailt, orsz\u00e1got (nem kell hozz\u00e1j\u00e1rulni az emailk\u00fcld\u00e9shez) Ut\u00e1na Get started with Azure DevOps dal\u00f3gus jelenik meg -> Continue (nem kell hozz\u00e1j\u00e1rulni az emailk\u00fcld\u00e9shez) Ha bej\u00f6n az \u00faj projekt k\u00e9sz\u00edt\u0151 fel\u00fclet, akkor nem kell haszn\u00e1latba venni Helyette Azure DevOps projekt gener\u00e1l\u00e1sa a laboranyag alapj\u00e1n DevOps elm\u00e9let https://docs.microsoft.com/en-us/azure/devops/learn/what-is-devops Azure Repos: projekt felfedez\u00e9se kont\u00e9ner szempontb\u00f3l docker-compose.yml mhc-aks.yaml -1. feladat - egyedi image n\u00e9v \u00e9s adatb\u00e1zis n\u00e9v \u00b6 az image neve el\u00e9 tegy\u00fck a neptunk\u00f3dunkat docker-compose.yml-ban image: myhealth.web -> image: <neptunkod>.myhealth.web mhc-aks.yaml-ban 93. sor k\u00f6r\u00fcl image: __ACR__/myhealth.web:latest -> image: __ACR__/<neptunkod>.myhealth.web:latest src/MyHealth.Web/appsettings.json a connection string-ben Initial Catalog=mhcdb helyett Initial Catalog=__SQLDB__ ne felejts\u00fcnk el commitolni! Kit\u00e9r\u0151: terraform \u00e9s multitenant infrastrukt\u00fara \u00b6 terraform file main.tf Jelsz\u00f3: a laborg\u00e9p jelszava Azure SQL Serverless v\u00e1ltozatot nem t\u00e1mogat m\u00e9g :( Azure SQL Serverless adatb\u00e1zisok batch-elt l\u00e9trehoz\u00e1sa Azure CLI-vel For ($i=1; $i -le 5; $i++) { az sql db create -g AKSLab -s akssqlsrv -n akslabsql$i --compute-model Serverless --edition GeneralPurpose --family Gen5 --auto-pause-delay 120 --max-size 1GB --capacity 1 --min-capacity 0.5 } L\u00e9pj\u00fcnk be az Azure port\u00e1lra a labuser@autsoft.hu userrel. Jelsz\u00f3: a laborg\u00e9p jelszava F\u0151leg csak olvas\u00e1si jogokkal rendelkezik N\u00e9zz\u00fck meg a l\u00e9trej\u00f6tt er\u0151forr\u00e1sokat 0. feladat Azure \u00e9s Azure DevOps \u00f6sszek\u00f6t\u00e9se \u00b6 Project Settings -> Pipelines r\u00e9szen bel\u00fcl Service Connection -> New Service Connection -> Azure Resource Manager -> alul v\u00e1ltsunk a linkkel a teljes verzi\u00f3ra (full version) T\u00f6lts\u00fck ki a terraform file alapj\u00e1n Subscription Name: MSDN1 1. feladat \u00b6 A laboranyag alapj\u00e1n, tov\u00e1bb\u00e1 A build pipeline v\u00e1ltoz\u00f3k k\u00f6z\u00e9 vegy\u00fck fel az SQLDB -t is Mindenhol, ahol Container Registry-t kell megadni, adjuk meg a teljes login server c\u00edmet A release pipeline-ban kl\u00f3nozzuk a Create Deployments & Services in AKS l\u00e9p\u00e9st, nevezz\u00fck \u00e1t Create Namespace -re, a Command r\u00e9sz alatt \u00e1ll\u00edtsunk be inline configuration-t a k\u00f6vetkez\u0151re: { \"apiVersion\" : \"v1\" , \"kind\" : \"Namespace\" , \"metadata\" : { \"name\" : \"<neptunk\u00f3d>\" }, } A release pipeline utols\u00f3 ( Update image in AKS ) l\u00e9p\u00e9s\u00e9ben az Arguments r\u00e9szen image deployments/mhc-front mhc-front=$(ACR)/myhealth.web:$(Build.BuildId) -> image deployments/mhc-front mhc-front=$(ACR)/<neptunk\u00f3d>.myhealth.web:$(Build.BuildId) 2. feladat \u00b6 A laboranyag alapj\u00e1n, tov\u00e1bb\u00e1 a kubectl parancsok v\u00e9g\u00e9re mindig \u00edrjuk --namespace=<neptunk\u00f3d>","title":"K8S DevOps Azure DevOps-ban \u00e9s Azure Kubernetes Service-szel"},{"location":"AKS/aks/#k8s-devops-azure-devops-ban-es-azure-kubernetes-service-szel","text":"","title":"K8S DevOps Azure DevOps-ban \u00e9s Azure Kubernetes Service-szel"},{"location":"AKS/aks/#temakorok","text":"Azure szolg\u00e1ltat\u00e1sok Azure DevOps Azure Kubernetes Service (multitenant) Azure SQL (multitenant) Terraform DevOps m\u0171veletek Azure DevOps-ban Forr\u00e1sk\u00f3d kezel\u00e9s Azure Repos-szal CI+CD Azure Pipelines-szal Monitoroz\u00e1s Azure Monitor-ral","title":"T\u00e9mak\u00f6r\u00f6k"},{"location":"AKS/aks/#a-labor-menete","text":"A hivatalos Azure DevOps labor anyagot k\u00f6veti: https://www.azuredevopslabs.com/labs/vstsextend/kubernetes/ . Itt csak a kieg\u00e9sz\u00edt\u00e9seket, ill. egy v\u00e1zlatot \u00edrunk le.","title":"A labor menete"},{"location":"AKS/aks/#elokeszuletek","text":"Azure DevOps szervezet l\u00e9trehoz\u00e1sa Be fog k\u00e9rni nevet, emailt, orsz\u00e1got (nem kell hozz\u00e1j\u00e1rulni az emailk\u00fcld\u00e9shez) Ut\u00e1na Get started with Azure DevOps dal\u00f3gus jelenik meg -> Continue (nem kell hozz\u00e1j\u00e1rulni az emailk\u00fcld\u00e9shez) Ha bej\u00f6n az \u00faj projekt k\u00e9sz\u00edt\u0151 fel\u00fclet, akkor nem kell haszn\u00e1latba venni Helyette Azure DevOps projekt gener\u00e1l\u00e1sa a laboranyag alapj\u00e1n DevOps elm\u00e9let https://docs.microsoft.com/en-us/azure/devops/learn/what-is-devops Azure Repos: projekt felfedez\u00e9se kont\u00e9ner szempontb\u00f3l docker-compose.yml mhc-aks.yaml","title":"El\u0151k\u00e9sz\u00fcletek"},{"location":"AKS/aks/#-1-feladat-egyedi-image-nev-es-adatbazis-nev","text":"az image neve el\u00e9 tegy\u00fck a neptunk\u00f3dunkat docker-compose.yml-ban image: myhealth.web -> image: <neptunkod>.myhealth.web mhc-aks.yaml-ban 93. sor k\u00f6r\u00fcl image: __ACR__/myhealth.web:latest -> image: __ACR__/<neptunkod>.myhealth.web:latest src/MyHealth.Web/appsettings.json a connection string-ben Initial Catalog=mhcdb helyett Initial Catalog=__SQLDB__ ne felejts\u00fcnk el commitolni!","title":"-1. feladat - egyedi image n\u00e9v \u00e9s adatb\u00e1zis n\u00e9v"},{"location":"AKS/aks/#kitero-terraform-es-multitenant-infrastruktura","text":"terraform file main.tf Jelsz\u00f3: a laborg\u00e9p jelszava Azure SQL Serverless v\u00e1ltozatot nem t\u00e1mogat m\u00e9g :( Azure SQL Serverless adatb\u00e1zisok batch-elt l\u00e9trehoz\u00e1sa Azure CLI-vel For ($i=1; $i -le 5; $i++) { az sql db create -g AKSLab -s akssqlsrv -n akslabsql$i --compute-model Serverless --edition GeneralPurpose --family Gen5 --auto-pause-delay 120 --max-size 1GB --capacity 1 --min-capacity 0.5 } L\u00e9pj\u00fcnk be az Azure port\u00e1lra a labuser@autsoft.hu userrel. Jelsz\u00f3: a laborg\u00e9p jelszava F\u0151leg csak olvas\u00e1si jogokkal rendelkezik N\u00e9zz\u00fck meg a l\u00e9trej\u00f6tt er\u0151forr\u00e1sokat","title":"Kit\u00e9r\u0151: terraform \u00e9s multitenant infrastrukt\u00fara"},{"location":"AKS/aks/#0-feladat-azure-es-azure-devops-osszekotese","text":"Project Settings -> Pipelines r\u00e9szen bel\u00fcl Service Connection -> New Service Connection -> Azure Resource Manager -> alul v\u00e1ltsunk a linkkel a teljes verzi\u00f3ra (full version) T\u00f6lts\u00fck ki a terraform file alapj\u00e1n Subscription Name: MSDN1","title":"0. feladat Azure \u00e9s Azure DevOps \u00f6sszek\u00f6t\u00e9se"},{"location":"AKS/aks/#1-feladat","text":"A laboranyag alapj\u00e1n, tov\u00e1bb\u00e1 A build pipeline v\u00e1ltoz\u00f3k k\u00f6z\u00e9 vegy\u00fck fel az SQLDB -t is Mindenhol, ahol Container Registry-t kell megadni, adjuk meg a teljes login server c\u00edmet A release pipeline-ban kl\u00f3nozzuk a Create Deployments & Services in AKS l\u00e9p\u00e9st, nevezz\u00fck \u00e1t Create Namespace -re, a Command r\u00e9sz alatt \u00e1ll\u00edtsunk be inline configuration-t a k\u00f6vetkez\u0151re: { \"apiVersion\" : \"v1\" , \"kind\" : \"Namespace\" , \"metadata\" : { \"name\" : \"<neptunk\u00f3d>\" }, } A release pipeline utols\u00f3 ( Update image in AKS ) l\u00e9p\u00e9s\u00e9ben az Arguments r\u00e9szen image deployments/mhc-front mhc-front=$(ACR)/myhealth.web:$(Build.BuildId) -> image deployments/mhc-front mhc-front=$(ACR)/<neptunk\u00f3d>.myhealth.web:$(Build.BuildId)","title":"1. feladat"},{"location":"AKS/aks/#2-feladat","text":"A laboranyag alapj\u00e1n, tov\u00e1bb\u00e1 a kubectl parancsok v\u00e9g\u00e9re mindig \u00edrjuk --namespace=<neptunk\u00f3d>","title":"2. feladat"},{"location":"Architektura/AdatbazisApiGateway/","text":"El\u0151ad\u00e1s \u00b6 Adatt\u00e1rol\u00e1s mikroszolg\u00e1ltat\u00e1sokban Api Gateway C\u00e9l \u00b6 A labor c\u00e9lja egy mikroszolg\u00e1ltat\u00e1s architekt\u00far\u00e1j\u00fa alkalmaz\u00e1s fel\u00e9p\u00edt\u00e9s\u00e9nek megismer\u00e9se, modern adatt\u00e1rol\u00e1s m\u00f3dszerek kipr\u00f3b\u00e1l\u00e1sa, valamint az Api Gateway haszn\u00e1lat\u00e1nak meg\u00e9rt\u00e9se. El\u0151k\u00f6vetelm\u00e9ny \u00b6 Docker Desktop Visual Studio 2019 legal\u00e1bb v16.6-ra felfriss\u00edtve az al\u00e1bbi worklodokkal Web development .NET Core cross-platform development Visual Studio Code Postman Kiindul\u00f3 projekt: https://github.com/bmeviauav42/todoapp Feladat \u00b6 Checkoutoljuk a minta alkalmaz\u00e1s k\u00f3dj\u00e1t \u00e9s ismerj\u00fck meg a rendszer fel\u00e9p\u00edt\u00e9s\u00e9t. Nyissuk meg Visual Studion-ban a solution-t \u00e9s ismerj\u00fck meg a forr\u00e1sk\u00f3d fel\u00e9p\u00edt\u00e9s\u00e9t. Ismerj\u00fck meg a users mikroszolg\u00e1ltat\u00e1s funkci\u00f3it: Ind\u00edtsunk el egy mongodb-t a compose f\u00e1jlban. N\u00e9zz\u00fck meg a /api/users c\u00edmeket kiszolg\u00e1l\u00f3 k\u00e9r\u00e9sek Python k\u00f3dj\u00e1t. Pr\u00f3b\u00e1ljuk ki a lek\u00e9rdez\u00e9seket Postmanb\u00f3l (a szolg\u00e1ltat\u00e1s a http://localhost:5083/api/users c\u00edmen \u00e9rhet\u0151 el). Ismerj\u00fck meg a todos mikroszolg\u00e1ltat\u00e1s funkci\u00f3it: N\u00e9zz\u00fck meg a /api/todos c\u00edmeket kiszolg\u00e1l\u00f3 k\u00e9r\u00e9sek C# k\u00f3dj\u00e1t \u00e9s a m\u00f6g\u00f6ttes repository-t, valamint a Redis-alap\u00fa cache-el\u00e9st. Pr\u00f3b\u00e1ljuk ki a lek\u00e9rdez\u00e9seket Postmanb\u00f3l (a szolg\u00e1ltat\u00e1s a http://localhost:5081/api/todos c\u00edmen \u00e9rhet\u0151 el.). Ismertj\u00fck meg a user t\u00f6rl\u00e9s folyamat\u00e1t. A t\u00f6rl\u00e9si k\u00e9r\u00e9s a users mikroszolg\u00e1ltat\u00e1shoz \u00e9rkezzen, de a t\u00f6r\u00f6lt userhez tartoz\u00f3 note-okat is t\u00f6r\u00f6lj\u00fck. A users szolg\u00e1ltat\u00e1sban t\u00f6rl\u00e9s eset\u00e9n \u00e9rtes\u00edti a todos -nak Redis-en kereszt\u00fcl. A todos szolg\u00e1ltat\u00e1s h\u00e1tt\u00e9rben fut\u00f3 polloz\u00e1ssal dolgozza fel a t\u00f6rl\u00e9s feladatokat. Ismerj\u00fck meg a Traefik API Gateway-t. N\u00e9zz\u00fck meg a Traefik kont\u00e9ner elind\u00edt\u00e1s\u00e1t \u00e9s a t\u00f6bbi kont\u00e9neren a label-\u00f6ket. A gatway-en kereszt\u00fcl egy porton \u00e9rhet\u0151 el a teljes alkalmaz\u00e1s a http://localhost:5080 c\u00edmen. Konfigur\u00e1ljuk be a forward authentication -t. Pr\u00f3b\u00e1ljuk ki, hogyan m\u0171k\u00f6dik a REST API, ha a users mikroszolg\u00e1ltat\u00e1s k\u00f3dj\u00e1ban a /api/auth v\u00e1lasz\u00e1t lecser\u00e9lj\u00fck.","title":"Adatt\u00e1rol\u00e1s \u00e9s Api Gateway"},{"location":"Architektura/AdatbazisApiGateway/#eloadas","text":"Adatt\u00e1rol\u00e1s mikroszolg\u00e1ltat\u00e1sokban Api Gateway","title":"El\u0151ad\u00e1s"},{"location":"Architektura/AdatbazisApiGateway/#cel","text":"A labor c\u00e9lja egy mikroszolg\u00e1ltat\u00e1s architekt\u00far\u00e1j\u00fa alkalmaz\u00e1s fel\u00e9p\u00edt\u00e9s\u00e9nek megismer\u00e9se, modern adatt\u00e1rol\u00e1s m\u00f3dszerek kipr\u00f3b\u00e1l\u00e1sa, valamint az Api Gateway haszn\u00e1lat\u00e1nak meg\u00e9rt\u00e9se.","title":"C\u00e9l"},{"location":"Architektura/AdatbazisApiGateway/#elokovetelmeny","text":"Docker Desktop Visual Studio 2019 legal\u00e1bb v16.6-ra felfriss\u00edtve az al\u00e1bbi worklodokkal Web development .NET Core cross-platform development Visual Studio Code Postman Kiindul\u00f3 projekt: https://github.com/bmeviauav42/todoapp","title":"El\u0151k\u00f6vetelm\u00e9ny"},{"location":"Architektura/AdatbazisApiGateway/#feladat","text":"Checkoutoljuk a minta alkalmaz\u00e1s k\u00f3dj\u00e1t \u00e9s ismerj\u00fck meg a rendszer fel\u00e9p\u00edt\u00e9s\u00e9t. Nyissuk meg Visual Studion-ban a solution-t \u00e9s ismerj\u00fck meg a forr\u00e1sk\u00f3d fel\u00e9p\u00edt\u00e9s\u00e9t. Ismerj\u00fck meg a users mikroszolg\u00e1ltat\u00e1s funkci\u00f3it: Ind\u00edtsunk el egy mongodb-t a compose f\u00e1jlban. N\u00e9zz\u00fck meg a /api/users c\u00edmeket kiszolg\u00e1l\u00f3 k\u00e9r\u00e9sek Python k\u00f3dj\u00e1t. Pr\u00f3b\u00e1ljuk ki a lek\u00e9rdez\u00e9seket Postmanb\u00f3l (a szolg\u00e1ltat\u00e1s a http://localhost:5083/api/users c\u00edmen \u00e9rhet\u0151 el). Ismerj\u00fck meg a todos mikroszolg\u00e1ltat\u00e1s funkci\u00f3it: N\u00e9zz\u00fck meg a /api/todos c\u00edmeket kiszolg\u00e1l\u00f3 k\u00e9r\u00e9sek C# k\u00f3dj\u00e1t \u00e9s a m\u00f6g\u00f6ttes repository-t, valamint a Redis-alap\u00fa cache-el\u00e9st. Pr\u00f3b\u00e1ljuk ki a lek\u00e9rdez\u00e9seket Postmanb\u00f3l (a szolg\u00e1ltat\u00e1s a http://localhost:5081/api/todos c\u00edmen \u00e9rhet\u0151 el.). Ismertj\u00fck meg a user t\u00f6rl\u00e9s folyamat\u00e1t. A t\u00f6rl\u00e9si k\u00e9r\u00e9s a users mikroszolg\u00e1ltat\u00e1shoz \u00e9rkezzen, de a t\u00f6r\u00f6lt userhez tartoz\u00f3 note-okat is t\u00f6r\u00f6lj\u00fck. A users szolg\u00e1ltat\u00e1sban t\u00f6rl\u00e9s eset\u00e9n \u00e9rtes\u00edti a todos -nak Redis-en kereszt\u00fcl. A todos szolg\u00e1ltat\u00e1s h\u00e1tt\u00e9rben fut\u00f3 polloz\u00e1ssal dolgozza fel a t\u00f6rl\u00e9s feladatokat. Ismerj\u00fck meg a Traefik API Gateway-t. N\u00e9zz\u00fck meg a Traefik kont\u00e9ner elind\u00edt\u00e1s\u00e1t \u00e9s a t\u00f6bbi kont\u00e9neren a label-\u00f6ket. A gatway-en kereszt\u00fcl egy porton \u00e9rhet\u0151 el a teljes alkalmaz\u00e1s a http://localhost:5080 c\u00edmen. Konfigur\u00e1ljuk be a forward authentication -t. Pr\u00f3b\u00e1ljuk ki, hogyan m\u0171k\u00f6dik a REST API, ha a users mikroszolg\u00e1ltat\u00e1s k\u00f3dj\u00e1ban a /api/auth v\u00e1lasz\u00e1t lecser\u00e9lj\u00fck.","title":"Feladat"},{"location":"Architektura/Kommunikacio/","text":"El\u0151ad\u00e1s \u00b6 Kommunik\u00e1ci\u00f3s lehet\u0151s\u00e9gek C\u00e9l \u00b6 A laborfeladatok c\u00e9lja a mikroszolg\u00e1ltat\u00e1sok fejleszt\u00e9se sor\u00e1n leggyakrabban felmer\u00fcl\u0151 megold\u00e1sok alapszint\u0171 gyakorl\u00e1sa. El\u0151k\u00f6vetelm\u00e9nyek \u00b6 Docker Desktop Visual Studio 2019 min v16.3 (gRPC miatt) ASP.NET Core 3.0 SDK (gRPC miatt) Kiindul\u00f3 projekt: https://github.com/bmeviauav42/komm-kiindulo Postman vagy Fiddler REST webszolg\u00e1ltat\u00e1sok k\u00e9sz\u00edt\u00e9se \u00b6 A labor k\u00fcl\u00f6n nem t\u00e9r ki a REST szer\u0171 webszolg\u00e1ltat\u00e1sok k\u00e9sz\u00edt\u00e9s\u00e9nek m\u00f3dszereire, arra a szakir\u00e1nyos k\u00e9pz\u00e9s \u00e9s a Szoftverfejleszt\u00e9s .NET platformra c\u00edm\u0171 v\u00e1laszthat\u00f3 t\u00e1rgy ASP.NET Core anyaga az aj\u00e1nlott irodalom. Hibat\u0171r\u0151 kommunik\u00e1ci\u00f3s m\u00f3dszerek \u00b6 Az el\u0151ad\u00e1son t\u00e1rgyalt tervez\u00e9si mint\u00e1k nem csak a kommunik\u00e1ci\u00f3 implement\u00e1ci\u00f3ja sor\u00e1n hasznos, hanem b\u00e1rmilyen olyan komponens h\u00edv\u00e1sa sor\u00e1n, ami nem v\u00e1rt tranziens hibajelens\u00e9get produk\u00e1lhat. T\u00e9ny, hogy leggyakrabban egy t\u00e1voli h\u00edv\u00e1s kommunik\u00e1ci\u00f3ja sor\u00e1n t\u00f6rt\u00e9nhet ilyen, \u00edgy ott mindenk\u00e9ppen \u00e9rdemes a hibat\u0171r\u00e9st valamilyen m\u00f3don megval\u00f3s\u00edtani. A laborfeladat sor\u00e1n k\u00e9t ASP.NET Core mikroszolg\u00e1ltat\u00e1s k\u00f6z\u00f6tti REST-es kommunik\u00e1ci\u00f3t szeretn\u00e9nk hibat\u0171r\u0151bb\u00e9 tenni. Ehhez a Polly oszt\u00e1lyk\u00f6nyvt\u00e1rat h\u00edvjuk seg\u00edts\u00e9g\u00fcl, ami a leggyakoribb mint\u00e1kat val\u00f3s\u00edtja meg, nek\u00fcnk csak felkonfigur\u00e1lnunk kell. Az egyszer\u0171s\u00e9g kedv\u00e9\u00e9rt most a Retry mint\u00e1t val\u00f3s\u00edtsuk meg. Kiindul\u00f3 projekt \u00e1ttekint\u00e9se \u00b6 Kl\u00f3nozzuk le a kiindul\u00f3 projektet, \u00e9s nyissuk meg a solutiont Visual Studio-val. \u00c9kezetes el\u00e9r\u00e9si \u00fat Fontos, hogy ne legyen az el\u00e9r\u00e9si \u00fatban speci\u00e1lis (\u00e9s \u00e9kezetes) karakter, k\u00fcl\u00f6nben nem tud a VS a docker-compose-hoz Debuggerrel csatlakozni. mkdir c:\\munka\\[neptun]\\MSA\\komm cd c:\\munka\\[neptun]\\MSA\\komm git clone https://github.com/bmeviauav42/komm-kiindulo A master branchen tal\u00e1lhat\u00f3 a kiindul\u00f3, m\u00edg a megold\u00e1sok k\u00fcl\u00f6n branchre ker\u00fcltek fel, ha valamelyik r\u00e9szfeladatn\u00e1l lemaradt\u00e1l volna. Mind a k\u00e9t projekt m\u00e1r Dockeriz\u00e1lt (Projekten jobb gomb / Add / Docker support), a teljes solutionh\u00f6z pedig tartozik egy Docker Compose le\u00edr\u00f3 (Projekteken jobb gomb / Add / Docker Orchestrator support / Docker Compose), amit egyben a futtatand\u00f3 projekt is. K\u00e9t ASP.NET Core projekt\u00fcnk van, n\u00e9zz\u00fck meg jobban \u0151ket: Catalog : REST API, t\u00f6rekedve az egyszer\u0171s\u00e9gre, hogy a labort ne ezzel bonyol\u00edtsuk el. ProductController Get() : Product list\u00e1val t\u00e9r vissza, az egyszer\u0171s\u00e9g kedv\u00e9\u00e9rt az adatok egy statikus list\u00e1ban vannak Get(int id) : Egy adott azonos\u00edt\u00f3val rendelkez\u0151 term\u00e9ket ad vissza a list\u00e1b\u00f3l A list\u00e1s Get() k\u00e9r\u00e9s v\u00e9letlenszer\u0171en hib\u00e1val (503-as hibak\u00f3ddal) t\u00e9r vissza. Ezzel szimul\u00e1ljuk a szolg\u00e1ltat\u00e1s esetleges kimarad\u00e1s\u00e1t. Order : szint\u00e9n egy egyszer\u0171 REST-es webszolg\u00e1ltat\u00e1s, most csak tesztel\u00e9s c\u00e9lj\u00e1b\u00f3l ApiClients/ICatalogApiClient Refit alap\u00fa er\u0151sen t\u00edpusos HTTP kliens szolg\u00e1ltat\u00e1s Refit k\u00f6nyvt\u00e1r az\u00e9rt is j\u00f3, mert el\u00e9g csak az interf\u00e9szt meg\u00edrni a h\u00edv\u00e1sok metaadataival felattribut\u00e1lva, \u00e9s az implement\u00e1c\u00f3t a Refit legener\u00e1lja a h\u00e1tt\u00e9rben. (Persze egy contract-first megk\u00f6zel\u00edt\u00e9st is v\u00e1laszthattunk volna, de most ne ezzel bonyol\u00edtsuk a labort) Startup A Refit klienst beregisztr\u00e1ljuk a DI kont\u00e9nerbe az AddRefitClient h\u00edv\u00e1ssal, ahol megadjuk a t\u00e1voli szolg\u00e1ltat\u00e1s base URL-j\u00e9t is. Az AddRefitClient h\u00edv\u00e1s az IHttpClientFactory mint\u00e1ra \u00e9p\u00fcl: modern .NET alkalmaz\u00e1sokban m\u00e1r ez az aj\u00e1nlott m\u00f3dszer, \u00e9s nem a HttpClient k\u00e9zi megp\u00e9ld\u00e1nyos\u00edt\u00e1sa. https://docs.microsoft.com/en-us/aspnet/core/fundamentals/http-requests?view=aspnetcore-3.0 Ez az\u00e9rt is j\u00f3, mert a Polly k\u00f6nyvt\u00e1r egyszer\u0171en tud ide integr\u00e1l\u00f3dni. TestController Csak a tesztel\u00e9s c\u00e9lj\u00e1b\u00f3l l\u00e9trehozott v\u00e9gpontokat tartalmaz, amin kereszt\u00fcl az \u00e1th\u00edv\u00e1st tudjuk majd tesztelni DI-b\u00f3l elk\u00e9rj\u00fck az ICatalogApiClient objektumot \u00e9s azon kereszt\u00fcl \u00e1th\u00edvunk a t\u00e1voli szolg\u00e1ltat\u00e1sba, majd nagyon egyszer\u0171en annak az eredm\u00e9ny\u00e9vel t\u00e9r\u00fcnk vissza. docker-compose Figyelj\u00fck meg, hogy az order kont\u00e9ner f\u00fcgg a catalog kont\u00e9nert\u0151l ( depends_on ). Ez az\u00e9rt fontos, hogy k\u00f6z\u00f6tt\u00fck fel\u00e9p\u00fclj\u00f6n a h\u00e1l\u00f3zati kapcsolat Ha visszatekint\u00fcnk az Order szolg\u00e1ltat\u00e1s Startup oszt\u00e1ly\u00e1ra, akkor l\u00e1thatjuk, hogy a szolg\u00e1ltat\u00e1s nev\u00e9vel hivatkozunk a m\u00e1sik kont\u00e9nerre. Ha ez nem tetszik, akkor a docker-compose f\u00e1jlban a hostnevet fel\u00fcl is lehet defini\u00e1lni. Azt is megfigyelhetj\u00fck, hogy nem a localhostra kiaj\u00e1nlott portot kell haszn\u00e1ljuk, hanem egym\u00e1s fel\u00e9 a t\u00e9nyleges portok vannak nyitva a kont\u00e9nereken. (80, 443). Most HTTP-t haszn\u00e1ljunk, hogy egyr\u00e9szt a tanus\u00edtv\u00e1nyokkal ne kelljen foglalkozzunk, illetve a docker-compose-on bel\u00fcl l\u00e9v\u0151 kommunik\u00e1ci\u00f3 eset\u00e9ben nem \u00f6rd\u00f6gt\u0151l val\u00f3 a sima https sem. Ha valamilyen \u00f6sszetettebb orchesztr\u00e1tort haszn\u00e1lunk, akkor \u00e9rdemes nem be\u00e9getni ezeket a hostneveket, hanem konfigur\u00e1ci\u00f3b\u00f3l v\u00e1rni azokat. (most ezt nem n\u00e9zz\u00fck meg) Pr\u00f3b\u00e1ljuk futtatni a projekteket! Vizsg\u00e1ljuk meg az el\u00e9rhet\u0151 h\u00edv\u00e1s viselked\u00e9s\u00e9t! Azt tapasztaljuk, hogy a Catalog Get() k\u00e9r\u00e9se ( api/Product ) \u00e9s \u00edgy az Order Get() k\u00e9r\u00e9se is ( api/test ) v\u00e9letlenszer\u0171en elsz\u00e1ll. Polly haszn\u00e1lata \u00b6 Vegy\u00fck fel az Order projektbe az al\u00e1bbi NuGet csomagot. Ez f\u00fcgg\u0151s\u00e9gk\u00e9nt beh\u00fazza a Polly -t is, \u00e9s t\u00e1mogat\u00e1st ad az IHttpClientFactory val t\u00f6rt\u00e9n\u0151 integr\u00e1ci\u00f3ra. <PackageReference Include= \"Microsoft.Extensions.Http.Polly\" Version= \"2.2.0\" /> Egyszer\u0171 Retry \u00b6 A Startup oszt\u00e1lyba adjuk hozz\u00e1 a Retry policy-t az IHttpClientFactory -hoz. \u00c1ll\u00edtsuk be, hogy a kapcsol\u00f3d\u00e1si hib\u00e1k \u00e9s az \u00e1ltalunk tranziens hib\u00e1knak v\u00e9lt st\u00e1tuszk\u00f3dok eset\u00e9n pr\u00f3b\u00e1lkozzon \u00fajra 5x. bool RetryableStatusCodesPredicate ( HttpStatusCode statusCode ) => statusCode == HttpStatusCode . BadGateway || statusCode == HttpStatusCode . ServiceUnavailable || statusCode == HttpStatusCode . GatewayTimeout ; services . AddRefitClient < ICatalogApiClient >() . ConfigureHttpClient ( c => c . BaseAddress = new Uri ( \"http://msa.comm.lab.services.catalog\" )) . AddPolicyHandler ( Policy . Handle < HttpRequestException >() . OrResult < HttpResponseMessage >( msg => RetryableStatusCodesPredicate ( msg . StatusCode )) . RetryAsync ( 5 ) ); Pr\u00f3b\u00e1ljuk ki! Tapasztalatunk szerint szinte megsz\u0171ntek a hib\u00e1k. Ez a Policy gyakorlatilag be\u00e9p\u00fcl a HttpClient Handler Pipeline-j\u00e1ba, \u00edgy a h\u00edv\u00f3 sz\u00e1m\u00e1ra transzparens lesz az \u00fajrapr\u00f3b\u00e1lkoz\u00e1si logika. Viszont \u00e9redemes odafigyelni a HttpClient Timeout j\u00e1ra is, mert az \u00fajrapr\u00f3b\u00e1lkoz\u00e1sok sor\u00e1n \u00edgy az nem indul \u00fajra. Polly \u00e1ltal\u00e1nosan A Polly-t nem csak HttpClient -tel lehet haszn\u00e1lni, hanem tetsz\u0151leges k\u00f3dban: \u00f6ssze lehet rakni egy Policy l\u00e1ncot, \u00e9s abba beburkolni a k\u00edv\u00e1nt h\u00edv\u00e1st. A Policy-ket ak\u00e1r karbantarthatjuk a DI kont\u00e9nerben is. Exponenci\u00e1lisan n\u00f6vekv\u0151 Retry id\u0151k\u00f6z \u00b6 Azonnali Retry helyett v\u00e1rjunk egy kicsit az \u00fajrapr\u00f3b\u00e1lkoz\u00e1sok k\u00f6z\u00f6tt, m\u00e9gpedig exponenci\u00e1lisan egyre t\u00f6bbet. //.RetryAsync(5) . WaitAndRetryAsync ( 5 , retryAttempt => TimeSpan . FromSeconds ( Math . Pow ( 2 , retryAttempt ))) Pr\u00f3b\u00e1ljuk ki! T\u00f6bb Policy haszn\u00e1lata \u00b6 Most laboron nem n\u00e9z\u00fcnk p\u00e9ld\u00e1t t\u00f6bb Policy haszn\u00e1lat\u00e1ra, de sz\u00f3baj\u00f6hetne m\u00e9g a Timeout, a Circuit breaker, Cache vagy ak\u00e1r a Fallback policy is . Az el\u0151ad\u00e1s anyagban tal\u00e1ltok egy \u00f6sszetettebb szekvencia diagrammot, az aj\u00e1nlott \u00f6sszet\u00e9telr\u0151l. Aszinkron kommunik\u00e1ci\u00f3 RabbitMQ-val \u00b6 A laboron egy esem\u00e9ny alap\u00fa aszinkron kommunik\u00e1ci\u00f3t val\u00f3s\u00edtunk meg. Az Order szolg\u00e1ltat\u00e1s fog publik\u00e1lni egy OrderCreated integr\u00e1ci\u00f3s esem\u00e9nyt, amit egy \u00fczenetsorba rak. Erre tetsz\u0151leges szolg\u00e1ltat\u00e1s feliratkozhat \u00e9s reag\u00e1lhat r\u00e1. Eset\u00fcnkben a Catalog szolg\u00e1ltat\u00e1s fogja a megrendelt term\u00e9k rakt\u00e1rk\u00e9szlet\u00e9t cs\u00f6kkenteni. Most az egyszer\u0171s\u00e9g kedv\u00e9\u00e9rt ne foglalkozzunk az idempotens megval\u00f3s\u00edt\u00e1ssal. Az aszinkron kommunik\u00e1ci\u00f3t most RabbitMQ-n kereszt\u00fcl val\u00f3s\u00edtjuk meg, \u00e9s a MassTransit oszt\u00e1lyk\u00f6nyvt\u00e1r seg\u00edts\u00e9g\u00e9vel fedj\u00fck el, hogy ne kelljen az akacsonyszint\u0171 implement\u00e1ci\u00f3val foglalkoznunk. RabbitMQ be\u00fczemel\u00e9se \u00b6 A docker-compose konfigur\u00e1ci\u00f3nkba vegy\u00fcnk fel egy RabbitMQ image alap\u00fa kont\u00e9nert. rabbitmq : image : rabbitmq:3-management ports : - \"5672:5672\" - \"15672:15672\" container_name : rabbitmq hostname : rabbitmq A RabbitMQ funkci\u00f3it a 5672 porton \u00e9rj\u00fck el, m\u00edg a 15672-n az admin fel\u00fcletet n\u00e9zhetj\u00fck meg. Alap\u00e9rtelmezett felhaszn\u00e1l\u00f3n\u00e9v: guest jelsz\u00f3: guest Adjuk meg, a m\u00e1sik k\u00e9t service eset\u00e9ben a rabbitmq-t\u00f3l val\u00f3 f\u00fcgg\u00e9st. msa.comm.lab.services.catalog : # ... depends_on : - rabbitmq msa.comm.lab.services.order : # ... depends_on : - msa.comm.lab.services.catalog - rabbitmq Integr\u00e1ci\u00f3s esem\u00e9ny \u00b6 Hozzunk l\u00e9tre egy \u00faj .NET Standard projektet Msa.Comm.Lab.Events n\u00e9ven, ami l\u00e9nyeg\u00e9ben a kommunik\u00e1ci\u00f3 contractja lesz. Ebbe vegy\u00fcnk fel egy interf\u00e9szt IOrderCreatedEvent n\u00e9ven az al\u00e1bbi tartalommal. Ez fog majd utazni az \u00fczenetsorban. public interface IOrderCreatedEvent { int ProductId { get ; } DateTimeOffset OrderPlaced { get ; } int Quantity { get ; } } Az Order projektben adjunk referenci\u00e1t az el\u0151z\u0151leg l\u00e9trehozott projektre. Hozzunk l\u00e9tre egy IntegrationEvents mapp\u00e1t az Order projektben, majd abba implement\u00e1ljuk egy oszt\u00e1lyba az IOrderCreatedEvent interf\u00e9szt. public class OrderCreatedEvent : IOrderCreatedEvent { public int ProductId { get ; set ; } public DateTimeOffset OrderPlaced { get ; set ; } public int Quantity { get ; set ; } } Ism\u00e9telj\u00fck meg ezt a Catalog szolg\u00e1ltat\u00e1sban is. MassTransit haszn\u00e1lata \u00b6 K\u00fcld\u0151 oldal \u00b6 Kezdj\u00fck a a k\u00fcld\u0151 oldallal. Vegy\u00fck fel az Order szolg\u00e1ltat\u00e1sba a k\u00f6vetkez\u0151 NuGet csomagokat. <PackageReference Include= \"MassTransit.Extensions.DependencyInjection\" Version= \"5.5.5\" /> <PackageReference Include= \"MassTransit.Extensions.Logging\" Version= \"5.5.5\" /> <PackageReference Include= \"MassTransit.RabbitMQ\" Version= \"5.5.5\" /> Konfigur\u00e1ljuk be a Startup oszt\u00e1lyban a MassTransit-ot, hogy RabbitMQ-t haszn\u00e1ljon, adjuk meg a logol\u00e1s m\u00f3dj\u00e1t, \u00e9s hogy melyik \u00fczenetsorba rakja az IOrderCreatedEvent esem\u00e9ny\u00fcnket. services . AddMassTransit ( x => { x . AddBus ( provider => Bus . Factory . CreateUsingRabbitMq ( cfg => { cfg . Host ( new Uri ( $ \"rabbitmq://rabbitmq:/\" ), hostConfig => { hostConfig . Username ( \"guest\" ); hostConfig . Password ( \"guest\" ); }); cfg . UseExtensionsLogging ( provider . GetRequiredService < ILoggerFactory >()); })); EndpointConvention . Map < IOrderCreatedEvent >( new Uri ( \"rabbitmq://rabbitmq:/integration\" )); }); Konfigur\u00e1ci\u00f3 korrektebben Itt is \u00e9rdemesebb lenne a rabbitmq hosztnevet \u00e9s a bejelentkez\u00e9si adatokat konfigur\u00e1ci\u00f3b\u00f3l nyerni. S\u00fcss\u00fck el az esem\u00e9nyt a TestController ben. K\u00e9rj\u00fck el az IBusControl objektumot, \u00e9s azon h\u00edvjuk meg a Publish met\u00f3dust. MassTransit eset\u00e9ben a Publish s\u00fcti el a broadcast szer\u0171 esem\u00e9nyeket, m\u00edg a Send ink\u00e1bb a command t\u00edpus\u00fa \u00fczenetekre van kihegyezve. private readonly ICatalogApiClient _catalogApiClient ; private readonly IBusControl _bus ; public TestController ( ICatalogApiClient catalogApiClient , IBusControl bus ) { _catalogApiClient = catalogApiClient ; _bus = bus ; } // ... [HttpPost(\"[action] \")] public async Task < ActionResult > CreateOrder () { await _bus . Publish ( new OrderCreatedEvent { ProductId = 1 , Quantity = 1 , OrderPlaced = DateTimeOffset . UtcNow }); return Ok ( new { Message = \"Megrendel\u00e9s sikeres!\" }); } Fogad\u00f3 oldal \u00b6 T\u00e9rj\u00fcnk \u00e1t a fogad\u00f3 oldalra. A Catalog szolg\u00e1ltat\u00e1s projektbe vegy\u00fck fel szint\u00e9n az al\u00e1bbi NuGet csomagokat. <PackageReference Include= \"MassTransit.Extensions.DependencyInjection\" Version= \"5.5.5\" /> <PackageReference Include= \"MassTransit.Extensions.Logging\" Version= \"5.5.5\" /> <PackageReference Include= \"MassTransit.RabbitMQ\" Version= \"5.5.5\" /> Sz\u00fcks\u00e9g\u00fcnk lesz egy az esem\u00e9nyt lekezel\u0151 oszt\u00e1lyra is, aminek MassTransit esetben az IConsumer<T> interf\u00e9szt kell megval\u00f3s\u00edtania. Vegy\u00fcnk fel a Catalog projektbe egy IntegrationEventHandlers mapp\u00e1t, majd abba hozzunk l\u00e9tre egy \u00faj oszt\u00e1lyt OrderCreatedEventHandler n\u00e9ven az al\u00e1bbi tartommal. Itt csak a kapott adatok alapj\u00e1n friss\u00edts\u00fck az adatainkat: a mi M\u00f3ricka p\u00e9ld\u00e1nkban a ProductController ben l\u00e9v\u0151 statikus list\u00e1n dolgozunk. public class OrderCreatedEventHandler : IConsumer < IOrderCreatedEvent > { public Task Consume ( ConsumeContext < IOrderCreatedEvent > context ) { var product = ProductController . Products . SingleOrDefault ( p => p . ProductId == context . Message . ProductId ); if ( product != null ) { product . Stock -= context . Message . Quantity ; } return Task . CompletedTask ; } } Konfigur\u00e1ljuk be a Startup -ban a MassTransit-ot, hogy RabbitMQ-t haszn\u00e1ljon, adjuk meg a logol\u00e1s m\u00f3dj\u00e1t, illetve hogy melyik \u00fczenetsorb\u00f3l v\u00e1rja az IOrderCreatedEvent esem\u00e9ny\u00fcnket, \u00e9s azt melyik IConsumer megval\u00f3s\u00edt\u00e1s kezelje le. services . AddMassTransit ( x => { x . AddConsumer < OrderCreatedEventHandler >(); x . AddBus ( provider => Bus . Factory . CreateUsingRabbitMq ( cfg => { var host = cfg . Host ( new Uri ( $ \"rabbitmq://rabbitmq:\" ), hostConfig => { hostConfig . Username ( \"guest\" ); hostConfig . Password ( \"guest\" ); }); cfg . UseExtensionsLogging ( provider . GetRequiredService < ILoggerFactory >()); cfg . ReceiveEndpoint ( host , \"integration\" , ep => { ep . ConfigureConsumer < OrderCreatedEventHandler >( provider ); }); })); }); Fogad\u00f3 oldalon m\u00e9g sz\u00fcks\u00e9ges elind\u00edtani egy h\u00e1tt\u00e9rfolyamatot is, ami figyeli az \u00fczenetsorokat. Ezt most tegy\u00fck meg a Startup.Configure() met\u00f3dus v\u00e9g\u00e9n. public void Configure ( IApplicationBuilder app , IHostingEnvironment env , IApplicationLifetime lifetime ) { // ... app . UseMvc (); var bus = app . ApplicationServices . GetService < IBusControl >(); var busHandle = TaskUtil . Await (() => bus . StartAsync ()); lifetime . ApplicationStopping . Register (() => busHandle . Stop ()); } Pr\u00f3b\u00e1ljuk ki! K\u00e9rj\u00fck le a term\u00e9keket S\u00fcss\u00fck el fiddlerb\u0151l vagy Postmanb\u0151l a CreateOrder Actiont N\u00e9zz\u00fck meg, hogy friss\u00fclt-e a term\u00e9k rakt\u00e1rk\u00e9szlete Hibakeres\u00e9s Ha nem friss\u00fclt, akkor a logokb\u00f3l vagy a rabbitmq menedzsment fel\u00fclet\u00e9r\u0151l lehet nyomozni. Ha azt tapasztaljuk hogy nem tud csatlakozni valamelyik szolg\u00e1ltat\u00e1s, akkor ellen\u0151rizz\u00fck a docker-compose file-t \u00e9s a connection string-eket. Ha azt tapasztaljuk, hogy skipped \u00fczenetsorba ker\u00fclnek az \u00fczenetek, akkor a k\u00fcld\u0151 oldal rendben m\u0171k\u00f6d\u00f6tt, de valami\u00e9rt a fogad\u00f3 oldal nem tudott a megadott \u00fczenett\u00edpusra egyszer sem feliratkozni helyesen. Kitekint\u00e9s A fenti p\u00e9ld\u00e1ban nem t\u00f6r\u0151dt\u00fcnk az idempotens megval\u00f3s\u00edt\u00e1ssal, ez mindig k\u00fcl\u00f6n tervez\u00e9st ig\u00e9nyel, az \u00fczleti logik\u00e1nk f\u00fcggv\u00e9ny\u00e9ben, de mindenk\u00e9ppen \u00e9rdemes a tervez\u00e9s sor\u00e1n figyelni erre. Mi most broadcast jelleg\u0171 integr\u00e1ci\u00f3s esem\u00e9nyt s\u00fct\u00f6tt\u00fcnk el. Ne feledj\u00fcnk van ennek egy m\u00e1sik vari\u00e1nsa is, amikor command szer\u0171 \u00fczenetet k\u00fcld\u00fcnk egy m\u00e1sik szolg\u00e1ltat\u00e1snak, \u00e9s ott elv\u00e1rjuk az esem\u00e9ny lefut\u00e1s\u00e1t. Integr\u00e1ci\u00f3s esem\u00e9ny sor\u00e1n a fogad\u00f3 f\u00e9lre van b\u00edzva, hogy mit kezd a kapott inform\u00e1ci\u00f3val. Contract-First API k\u00e9sz\u00edt\u00e9s - gRPC \u00b6 A feladat c\u00e9lja kipr\u00f3b\u00e1lni a contract-first megkozel\u00edt\u00e9st: teh\u00e1t el\u0151bb az interf\u00e9sz le\u00edr\u00f3t k\u00e9sz\u00edtj\u00fck el egy Domain Specific Language (DSL) seg\u00edt\u00e9s\u00e9vel, majd abb\u00f3l gener\u00e1lunk kliens \u00e9s szerver oldali k\u00f3dot. Ezt REST-es API-val is meg tudn\u00e1nk tenni a Swagger/OpenAPI le\u00edr\u00f3val, de mi most gRPC-n kereszt\u00fcl pr\u00f3v\u00e1ljuk ki. Feladatunkban a Catalog szolg\u00e1ltat\u00e1s ProductController k\u00e9t m\u0171velet\u00e9t \u00edrjuk meg gRPC protokollal. Majd h\u00edvjuk meg ezeket az Order szolg\u00e1ltat\u00e1sb\u00f3l. .NET Core 3.0+ A gRPC csak .NET Core 3.0-t\u00f3l t\u00e1mogatott, \u00edgy \u00e9rdemes a Visual Studio-t friss\u00edteni legal\u00e1bb 16.3.x verzi\u00f3ra Szerver oldal \u00b6 Lehet\u0151s\u00e9g lenne egy Projektsablonb\u00f3l is dolgoznunk (File / New Project / gRPC Serice), ami szint\u00e9n egy ASP.NET Core 3.0-\u00e1s projekt, de most a megl\u00e9v\u0151 Catalog projekt\u00fcnkbe rakjuk bele ezt a funkcionalit\u00e1st. Vegy\u00fck fel a Catalog projektbe az al\u00e1bbi NuGet csomagot. Fontos, hogy a 2.23.1 -es verzi\u00f3 legyen, mert \u00e9n a 2.23.2-es verzi\u00f3val belefutottam egy bugba. <PackageReference Include= \"Grpc.AspNetCore\" Version= \"2.23.1\" /> A Catalog projektbe vegy\u00fcnk fel egy Protos mapp\u00e1t, \u00e9s abba egy Text f\u00e1jlt catalog.proto n\u00e9ven, ami a szolg\u00e1ltat\u00e1sle\u00edr\u00f3nk lesz. K\u00e9sz\u00fcts\u00fcnk el egy saj\u00e1t szolg\u00e1ltat\u00e1sle\u00edr\u00f3t a Catalog REST API-nk mint\u00e1j\u00e1ra. Tartalom a k\u00f6vetkez\u0151: Egy szolg\u00e1ltat\u00e1sunk lesz CatalogService n\u00e9ven A szolg\u00e1ltat\u00e1sban rpc kulcssz\u00f3val tudunk m\u0171veleteket defini\u00e1lni, megadva a bemen\u0151 param\u00e9tert \u00e9s a visszat\u00e9r\u00e9si \u00e9rt\u00e9ket. Ezek k\u00fcl\u00f6n defini\u00e1lt message t\u00edpusok lehetnek. Az els\u0151 legyen a GetProducts Sajnos a proto szabv\u00e1nyban nincs void t\u00edpus\u00fa \u00fczenet, ez\u00e9rt sz\u00fcks\u00e9ges felvenn\u00fcnk egy \u00fcres \u00fczenetet Empty n\u00e9ven, ez fogja majd a void t\u00edpust reprezent\u00e1lni A visszat\u00e9r\u00e9si \u00e9rt\u00e9k t\u00edpus\u00e1t a ProductsResponse \u00fczenet \u00edrja le, amiben t\u00f6bb Product t\u00edpus\u00fa \u00fczenet lesz (nevezhetn\u00e9nk t\u00f6mbnek is, itt repeated kulcssz\u00f3) Az \u00fczenetekben meg kell adnunk a mez\u0151k sorrendj\u00e9t az egyenl\u0151s\u00e9g m\u00f6g\u00f6tt a bin\u00e1ris soros\u00edt\u00e1s miatt Product \u00fczenet az eddig megszokott propertykkel rendelkezzen a proto-s t\u00edpusokkal (sajnos itt nincs decimal ) A GetProduct m\u0171velet egy azonos\u00edt\u00f3t v\u00e1r, \u00e9s egy Product -tal t\u00e9r vissza. Az id-t is sz\u00fcks\u00e9ges becsomagolni egy \u00fczenett\u00edpusba: GetProductRequest syntax = \"proto3\" ; option csharp_namespace = \"Msa.Comm.Lab.Services.Catalog.Grpc\" ; package Catalog ; service CatalogService { rpc GetProducts ( Empty ) returns ( ProductsResponse ); rpc GetProduct ( GetProductRequest ) returns ( Product ); } message Empty { } message ProductsResponse { repeated Product Products = 1 ; } message Product { int32 ProductId = 1 ; string Name = 2 ; double UnitPrice = 3 ; int32 Stock = 4 ; } message GetProductRequest { int32 ProductId = 1 ; } A .proto f\u00e1jlb\u00f3l a modellek \u00e9s egy \u0151soszt\u00e1ly is gener\u00e1l\u00f3dik a ford\u00edt\u00e1s sor\u00e1n. Ehhez viszont a Catalog projekt f\u00e1jlban fel kell venni a k\u00f6vetkez\u0151 elemet: <ItemGroup> <Protobuf Include= \"Protos\\catalog.proto\" GrpcServices= \"Server\" /> </ItemGroup> A Catalog projektbe hozzunk l\u00e9tre egy Services mapp\u00e1t, majd abba egy CatalogService oszt\u00e1lyt, ami a Grpc.CatalogService.CatalogServiceBase -b\u0151l sz\u00e1rmazik le. Nek\u00fcnk csak fel\u00fcl kell defini\u00e1lni a k\u00edv\u00e1nt met\u00f3dusokat. Ide is vegy\u00fck fel az al\u00e1bbi statikus list\u00e1t, \u00e9s ennek a seg\u00edts\u00e9g\u00e9vel val\u00f3s\u00edtsuk meg az \u00fczleti m\u0171veleteket. Figyelj\u00fck meg, hogy a .proto-b\u00f3l gener\u00e1lt t\u00edpusokkal tudunk itt dolgozni. public class CatalogService : Grpc . CatalogService . CatalogServiceBase { private readonly ILogger < CatalogService > _logger ; internal static List < Product > _products = new List < Product > { new Product { ProductId = 1 , Name = \"S\u00f6r\" , Stock = 10 , UnitPrice = 250 }, new Product { ProductId = 2 , Name = \"Bor\" , Stock = 5 , UnitPrice = 890 }, new Product { ProductId = 3 , Name = \"Csoki\" , Stock = 15 , UnitPrice = 200 }, }; public CatalogService ( ILogger < CatalogService > logger ) { _logger = logger ; } public override Task < ProductsResponse > GetProducts ( Empty request , ServerCallContext context ) { var response = new ProductsResponse (); response . Products . Add ( _products ); return Task . FromResult ( response ); } public override Task < Product > GetProduct ( GetProductRequest request , ServerCallContext context ) { var product = _products . SingleOrDefault ( p => p . ProductId == request . ProductId ); return Task . FromResult ( product ); } } A Catalog Startup oszt\u00e1lyban regisztr\u00e1ljuk be a gRPC szolg\u00e1ltat\u00e1sainkat. services . AddGrpc (); app . UseEndpoints ( endpoints => { endpoints . MapGrpcService < Services . CatalogService >(); endpoints . MapControllers (); }); Kliens oldal \u00b6 Adjunk az Order projekthez egy szolg\u00e1ltat\u00e1s referenci\u00e1t. Az Order projekten jobb gomb / Add / Service Reference / gRPC / Add / File ahol tall\u00f3zuk ki a m\u00e1sik projektben tal\u00e1lhat\u00f3 .proto f\u00e1jlt \u00e9s Client m\u00f3dban gener\u00e1ljuk le a sz\u00fcks\u00e9ges oszt\u00e1lyokat. Ez a m\u0171velet a sz\u00fcks\u00e9ges NuGet csomagokat is hozz\u00e1adja a projekthez. A Startup oszt\u00e1lyban regisztr\u00e1ljuk be a DI kont\u00e9nerbe a gRPC kliens\u00fcnket. Mivel a gRPC-nek sz\u00fcks\u00e9ge van HTTPS-re, viszont most a kont\u00e9nereink nem b\u00edznak egym\u00e1s tanus\u00edtv\u00e1nyaiban, ez\u00e9rt most ideiglenesen kapcsoljuk ki a HTTPS hib\u00e1kat a gRPC-t alatt l\u00e9v\u0151 HttpClient -ben. services . AddGrpcClient < CatalogService . CatalogServiceClient >( o => { o . Address = new Uri ( \"https://msa.comm.lab.services.catalog\" ); }). ConfigurePrimaryHttpMessageHandler ( p => new HttpClientHandler { ServerCertificateCustomValidationCallback = HttpClientHandler . DangerousAcceptAnyServerCertificateValidator }); IHttpClientFactory hasz\u00e1lata Megfigyelhetj\u00fck, hogy a gRPC kliens is az IHttpClientFactory megold\u00e1sra \u00e9p\u00edt. A TestController ben cser\u00e9lj\u00fck le A REST kliens\u00fcnket a gRPC kliensre. //private readonly ICatalogApiClient _catalogApiClient; private readonly IBusControl _bus ; private readonly CatalogService . CatalogServiceClient _catalogServiceClient ; public TestController ( //ICatalogApiClient catalogApiClient, IBusControl bus , CatalogService . CatalogServiceClient catalogServiceClient ) { //_catalogApiClient = catalogApiClient; _bus = bus ; _catalogServiceClient = catalogServiceClient ; } [HttpGet] public async Task < ActionResult < IEnumerable < Catalog . Grpc . Product >>> Get () { return ( await _catalogServiceClient . GetProductsAsync ( new Empty ())). Products ; } Pr\u00f3b\u00e1ljuk ki! \u00d6sszefoglal\u00e1s \u00b6 A gyakorlat sor\u00e1n n\u00e9zt\u00fcnk p\u00e9ld\u00e1t REST-es kommunik\u00e1ci\u00f3 eset\u00e9ben a hibat\u0171r\u00e9s n\u00f6vel\u00e9s\u00e9re egy Retry policy-vel. Majd Aszinkron kommunik\u00e1ci\u00f3t implement\u00e1ltunk egy RabbitMQ \u00fczenetsor \u00e9s MassTransit konyvt\u00e1r seg\u00edts\u00e9g\u00e9vel a k\u00e9t szolg\u00e1ltat\u00e1sunk k\u00f6z\u00f6tt. V\u00e9g\u00fcl Contract first megk\u00f6zel\u00edt\u00e9ssel k\u00e9sz\u00edtett\u00fcnk szolg\u00e1ltat\u00e1st, most gRPC-n kipr\u00f3b\u00e1lva.","title":"Kommunik\u00e1ci\u00f3"},{"location":"Architektura/Kommunikacio/#eloadas","text":"Kommunik\u00e1ci\u00f3s lehet\u0151s\u00e9gek","title":"El\u0151ad\u00e1s"},{"location":"Architektura/Kommunikacio/#cel","text":"A laborfeladatok c\u00e9lja a mikroszolg\u00e1ltat\u00e1sok fejleszt\u00e9se sor\u00e1n leggyakrabban felmer\u00fcl\u0151 megold\u00e1sok alapszint\u0171 gyakorl\u00e1sa.","title":"C\u00e9l"},{"location":"Architektura/Kommunikacio/#elokovetelmenyek","text":"Docker Desktop Visual Studio 2019 min v16.3 (gRPC miatt) ASP.NET Core 3.0 SDK (gRPC miatt) Kiindul\u00f3 projekt: https://github.com/bmeviauav42/komm-kiindulo Postman vagy Fiddler","title":"El\u0151k\u00f6vetelm\u00e9nyek"},{"location":"Architektura/Kommunikacio/#rest-webszolgaltatasok-keszitese","text":"A labor k\u00fcl\u00f6n nem t\u00e9r ki a REST szer\u0171 webszolg\u00e1ltat\u00e1sok k\u00e9sz\u00edt\u00e9s\u00e9nek m\u00f3dszereire, arra a szakir\u00e1nyos k\u00e9pz\u00e9s \u00e9s a Szoftverfejleszt\u00e9s .NET platformra c\u00edm\u0171 v\u00e1laszthat\u00f3 t\u00e1rgy ASP.NET Core anyaga az aj\u00e1nlott irodalom.","title":"REST webszolg\u00e1ltat\u00e1sok k\u00e9sz\u00edt\u00e9se"},{"location":"Architektura/Kommunikacio/#hibaturo-kommunikacios-modszerek","text":"Az el\u0151ad\u00e1son t\u00e1rgyalt tervez\u00e9si mint\u00e1k nem csak a kommunik\u00e1ci\u00f3 implement\u00e1ci\u00f3ja sor\u00e1n hasznos, hanem b\u00e1rmilyen olyan komponens h\u00edv\u00e1sa sor\u00e1n, ami nem v\u00e1rt tranziens hibajelens\u00e9get produk\u00e1lhat. T\u00e9ny, hogy leggyakrabban egy t\u00e1voli h\u00edv\u00e1s kommunik\u00e1ci\u00f3ja sor\u00e1n t\u00f6rt\u00e9nhet ilyen, \u00edgy ott mindenk\u00e9ppen \u00e9rdemes a hibat\u0171r\u00e9st valamilyen m\u00f3don megval\u00f3s\u00edtani. A laborfeladat sor\u00e1n k\u00e9t ASP.NET Core mikroszolg\u00e1ltat\u00e1s k\u00f6z\u00f6tti REST-es kommunik\u00e1ci\u00f3t szeretn\u00e9nk hibat\u0171r\u0151bb\u00e9 tenni. Ehhez a Polly oszt\u00e1lyk\u00f6nyvt\u00e1rat h\u00edvjuk seg\u00edts\u00e9g\u00fcl, ami a leggyakoribb mint\u00e1kat val\u00f3s\u00edtja meg, nek\u00fcnk csak felkonfigur\u00e1lnunk kell. Az egyszer\u0171s\u00e9g kedv\u00e9\u00e9rt most a Retry mint\u00e1t val\u00f3s\u00edtsuk meg.","title":"Hibat\u0171r\u0151 kommunik\u00e1ci\u00f3s m\u00f3dszerek"},{"location":"Architektura/Kommunikacio/#kiindulo-projekt-attekintese","text":"Kl\u00f3nozzuk le a kiindul\u00f3 projektet, \u00e9s nyissuk meg a solutiont Visual Studio-val. \u00c9kezetes el\u00e9r\u00e9si \u00fat Fontos, hogy ne legyen az el\u00e9r\u00e9si \u00fatban speci\u00e1lis (\u00e9s \u00e9kezetes) karakter, k\u00fcl\u00f6nben nem tud a VS a docker-compose-hoz Debuggerrel csatlakozni. mkdir c:\\munka\\[neptun]\\MSA\\komm cd c:\\munka\\[neptun]\\MSA\\komm git clone https://github.com/bmeviauav42/komm-kiindulo A master branchen tal\u00e1lhat\u00f3 a kiindul\u00f3, m\u00edg a megold\u00e1sok k\u00fcl\u00f6n branchre ker\u00fcltek fel, ha valamelyik r\u00e9szfeladatn\u00e1l lemaradt\u00e1l volna. Mind a k\u00e9t projekt m\u00e1r Dockeriz\u00e1lt (Projekten jobb gomb / Add / Docker support), a teljes solutionh\u00f6z pedig tartozik egy Docker Compose le\u00edr\u00f3 (Projekteken jobb gomb / Add / Docker Orchestrator support / Docker Compose), amit egyben a futtatand\u00f3 projekt is. K\u00e9t ASP.NET Core projekt\u00fcnk van, n\u00e9zz\u00fck meg jobban \u0151ket: Catalog : REST API, t\u00f6rekedve az egyszer\u0171s\u00e9gre, hogy a labort ne ezzel bonyol\u00edtsuk el. ProductController Get() : Product list\u00e1val t\u00e9r vissza, az egyszer\u0171s\u00e9g kedv\u00e9\u00e9rt az adatok egy statikus list\u00e1ban vannak Get(int id) : Egy adott azonos\u00edt\u00f3val rendelkez\u0151 term\u00e9ket ad vissza a list\u00e1b\u00f3l A list\u00e1s Get() k\u00e9r\u00e9s v\u00e9letlenszer\u0171en hib\u00e1val (503-as hibak\u00f3ddal) t\u00e9r vissza. Ezzel szimul\u00e1ljuk a szolg\u00e1ltat\u00e1s esetleges kimarad\u00e1s\u00e1t. Order : szint\u00e9n egy egyszer\u0171 REST-es webszolg\u00e1ltat\u00e1s, most csak tesztel\u00e9s c\u00e9lj\u00e1b\u00f3l ApiClients/ICatalogApiClient Refit alap\u00fa er\u0151sen t\u00edpusos HTTP kliens szolg\u00e1ltat\u00e1s Refit k\u00f6nyvt\u00e1r az\u00e9rt is j\u00f3, mert el\u00e9g csak az interf\u00e9szt meg\u00edrni a h\u00edv\u00e1sok metaadataival felattribut\u00e1lva, \u00e9s az implement\u00e1c\u00f3t a Refit legener\u00e1lja a h\u00e1tt\u00e9rben. (Persze egy contract-first megk\u00f6zel\u00edt\u00e9st is v\u00e1laszthattunk volna, de most ne ezzel bonyol\u00edtsuk a labort) Startup A Refit klienst beregisztr\u00e1ljuk a DI kont\u00e9nerbe az AddRefitClient h\u00edv\u00e1ssal, ahol megadjuk a t\u00e1voli szolg\u00e1ltat\u00e1s base URL-j\u00e9t is. Az AddRefitClient h\u00edv\u00e1s az IHttpClientFactory mint\u00e1ra \u00e9p\u00fcl: modern .NET alkalmaz\u00e1sokban m\u00e1r ez az aj\u00e1nlott m\u00f3dszer, \u00e9s nem a HttpClient k\u00e9zi megp\u00e9ld\u00e1nyos\u00edt\u00e1sa. https://docs.microsoft.com/en-us/aspnet/core/fundamentals/http-requests?view=aspnetcore-3.0 Ez az\u00e9rt is j\u00f3, mert a Polly k\u00f6nyvt\u00e1r egyszer\u0171en tud ide integr\u00e1l\u00f3dni. TestController Csak a tesztel\u00e9s c\u00e9lj\u00e1b\u00f3l l\u00e9trehozott v\u00e9gpontokat tartalmaz, amin kereszt\u00fcl az \u00e1th\u00edv\u00e1st tudjuk majd tesztelni DI-b\u00f3l elk\u00e9rj\u00fck az ICatalogApiClient objektumot \u00e9s azon kereszt\u00fcl \u00e1th\u00edvunk a t\u00e1voli szolg\u00e1ltat\u00e1sba, majd nagyon egyszer\u0171en annak az eredm\u00e9ny\u00e9vel t\u00e9r\u00fcnk vissza. docker-compose Figyelj\u00fck meg, hogy az order kont\u00e9ner f\u00fcgg a catalog kont\u00e9nert\u0151l ( depends_on ). Ez az\u00e9rt fontos, hogy k\u00f6z\u00f6tt\u00fck fel\u00e9p\u00fclj\u00f6n a h\u00e1l\u00f3zati kapcsolat Ha visszatekint\u00fcnk az Order szolg\u00e1ltat\u00e1s Startup oszt\u00e1ly\u00e1ra, akkor l\u00e1thatjuk, hogy a szolg\u00e1ltat\u00e1s nev\u00e9vel hivatkozunk a m\u00e1sik kont\u00e9nerre. Ha ez nem tetszik, akkor a docker-compose f\u00e1jlban a hostnevet fel\u00fcl is lehet defini\u00e1lni. Azt is megfigyelhetj\u00fck, hogy nem a localhostra kiaj\u00e1nlott portot kell haszn\u00e1ljuk, hanem egym\u00e1s fel\u00e9 a t\u00e9nyleges portok vannak nyitva a kont\u00e9nereken. (80, 443). Most HTTP-t haszn\u00e1ljunk, hogy egyr\u00e9szt a tanus\u00edtv\u00e1nyokkal ne kelljen foglalkozzunk, illetve a docker-compose-on bel\u00fcl l\u00e9v\u0151 kommunik\u00e1ci\u00f3 eset\u00e9ben nem \u00f6rd\u00f6gt\u0151l val\u00f3 a sima https sem. Ha valamilyen \u00f6sszetettebb orchesztr\u00e1tort haszn\u00e1lunk, akkor \u00e9rdemes nem be\u00e9getni ezeket a hostneveket, hanem konfigur\u00e1ci\u00f3b\u00f3l v\u00e1rni azokat. (most ezt nem n\u00e9zz\u00fck meg) Pr\u00f3b\u00e1ljuk futtatni a projekteket! Vizsg\u00e1ljuk meg az el\u00e9rhet\u0151 h\u00edv\u00e1s viselked\u00e9s\u00e9t! Azt tapasztaljuk, hogy a Catalog Get() k\u00e9r\u00e9se ( api/Product ) \u00e9s \u00edgy az Order Get() k\u00e9r\u00e9se is ( api/test ) v\u00e9letlenszer\u0171en elsz\u00e1ll.","title":"Kiindul\u00f3 projekt \u00e1ttekint\u00e9se"},{"location":"Architektura/Kommunikacio/#polly-hasznalata","text":"Vegy\u00fck fel az Order projektbe az al\u00e1bbi NuGet csomagot. Ez f\u00fcgg\u0151s\u00e9gk\u00e9nt beh\u00fazza a Polly -t is, \u00e9s t\u00e1mogat\u00e1st ad az IHttpClientFactory val t\u00f6rt\u00e9n\u0151 integr\u00e1ci\u00f3ra. <PackageReference Include= \"Microsoft.Extensions.Http.Polly\" Version= \"2.2.0\" />","title":"Polly haszn\u00e1lata"},{"location":"Architektura/Kommunikacio/#egyszeru-retry","text":"A Startup oszt\u00e1lyba adjuk hozz\u00e1 a Retry policy-t az IHttpClientFactory -hoz. \u00c1ll\u00edtsuk be, hogy a kapcsol\u00f3d\u00e1si hib\u00e1k \u00e9s az \u00e1ltalunk tranziens hib\u00e1knak v\u00e9lt st\u00e1tuszk\u00f3dok eset\u00e9n pr\u00f3b\u00e1lkozzon \u00fajra 5x. bool RetryableStatusCodesPredicate ( HttpStatusCode statusCode ) => statusCode == HttpStatusCode . BadGateway || statusCode == HttpStatusCode . ServiceUnavailable || statusCode == HttpStatusCode . GatewayTimeout ; services . AddRefitClient < ICatalogApiClient >() . ConfigureHttpClient ( c => c . BaseAddress = new Uri ( \"http://msa.comm.lab.services.catalog\" )) . AddPolicyHandler ( Policy . Handle < HttpRequestException >() . OrResult < HttpResponseMessage >( msg => RetryableStatusCodesPredicate ( msg . StatusCode )) . RetryAsync ( 5 ) ); Pr\u00f3b\u00e1ljuk ki! Tapasztalatunk szerint szinte megsz\u0171ntek a hib\u00e1k. Ez a Policy gyakorlatilag be\u00e9p\u00fcl a HttpClient Handler Pipeline-j\u00e1ba, \u00edgy a h\u00edv\u00f3 sz\u00e1m\u00e1ra transzparens lesz az \u00fajrapr\u00f3b\u00e1lkoz\u00e1si logika. Viszont \u00e9redemes odafigyelni a HttpClient Timeout j\u00e1ra is, mert az \u00fajrapr\u00f3b\u00e1lkoz\u00e1sok sor\u00e1n \u00edgy az nem indul \u00fajra. Polly \u00e1ltal\u00e1nosan A Polly-t nem csak HttpClient -tel lehet haszn\u00e1lni, hanem tetsz\u0151leges k\u00f3dban: \u00f6ssze lehet rakni egy Policy l\u00e1ncot, \u00e9s abba beburkolni a k\u00edv\u00e1nt h\u00edv\u00e1st. A Policy-ket ak\u00e1r karbantarthatjuk a DI kont\u00e9nerben is.","title":"Egyszer\u0171 Retry"},{"location":"Architektura/Kommunikacio/#exponencialisan-novekvo-retry-idokoz","text":"Azonnali Retry helyett v\u00e1rjunk egy kicsit az \u00fajrapr\u00f3b\u00e1lkoz\u00e1sok k\u00f6z\u00f6tt, m\u00e9gpedig exponenci\u00e1lisan egyre t\u00f6bbet. //.RetryAsync(5) . WaitAndRetryAsync ( 5 , retryAttempt => TimeSpan . FromSeconds ( Math . Pow ( 2 , retryAttempt ))) Pr\u00f3b\u00e1ljuk ki!","title":"Exponenci\u00e1lisan n\u00f6vekv\u0151 Retry id\u0151k\u00f6z"},{"location":"Architektura/Kommunikacio/#tobb-policy-hasznalata","text":"Most laboron nem n\u00e9z\u00fcnk p\u00e9ld\u00e1t t\u00f6bb Policy haszn\u00e1lat\u00e1ra, de sz\u00f3baj\u00f6hetne m\u00e9g a Timeout, a Circuit breaker, Cache vagy ak\u00e1r a Fallback policy is . Az el\u0151ad\u00e1s anyagban tal\u00e1ltok egy \u00f6sszetettebb szekvencia diagrammot, az aj\u00e1nlott \u00f6sszet\u00e9telr\u0151l.","title":"T\u00f6bb Policy haszn\u00e1lata"},{"location":"Architektura/Kommunikacio/#aszinkron-kommunikacio-rabbitmq-val","text":"A laboron egy esem\u00e9ny alap\u00fa aszinkron kommunik\u00e1ci\u00f3t val\u00f3s\u00edtunk meg. Az Order szolg\u00e1ltat\u00e1s fog publik\u00e1lni egy OrderCreated integr\u00e1ci\u00f3s esem\u00e9nyt, amit egy \u00fczenetsorba rak. Erre tetsz\u0151leges szolg\u00e1ltat\u00e1s feliratkozhat \u00e9s reag\u00e1lhat r\u00e1. Eset\u00fcnkben a Catalog szolg\u00e1ltat\u00e1s fogja a megrendelt term\u00e9k rakt\u00e1rk\u00e9szlet\u00e9t cs\u00f6kkenteni. Most az egyszer\u0171s\u00e9g kedv\u00e9\u00e9rt ne foglalkozzunk az idempotens megval\u00f3s\u00edt\u00e1ssal. Az aszinkron kommunik\u00e1ci\u00f3t most RabbitMQ-n kereszt\u00fcl val\u00f3s\u00edtjuk meg, \u00e9s a MassTransit oszt\u00e1lyk\u00f6nyvt\u00e1r seg\u00edts\u00e9g\u00e9vel fedj\u00fck el, hogy ne kelljen az akacsonyszint\u0171 implement\u00e1ci\u00f3val foglalkoznunk.","title":"Aszinkron kommunik\u00e1ci\u00f3 RabbitMQ-val"},{"location":"Architektura/Kommunikacio/#rabbitmq-beuzemelese","text":"A docker-compose konfigur\u00e1ci\u00f3nkba vegy\u00fcnk fel egy RabbitMQ image alap\u00fa kont\u00e9nert. rabbitmq : image : rabbitmq:3-management ports : - \"5672:5672\" - \"15672:15672\" container_name : rabbitmq hostname : rabbitmq A RabbitMQ funkci\u00f3it a 5672 porton \u00e9rj\u00fck el, m\u00edg a 15672-n az admin fel\u00fcletet n\u00e9zhetj\u00fck meg. Alap\u00e9rtelmezett felhaszn\u00e1l\u00f3n\u00e9v: guest jelsz\u00f3: guest Adjuk meg, a m\u00e1sik k\u00e9t service eset\u00e9ben a rabbitmq-t\u00f3l val\u00f3 f\u00fcgg\u00e9st. msa.comm.lab.services.catalog : # ... depends_on : - rabbitmq msa.comm.lab.services.order : # ... depends_on : - msa.comm.lab.services.catalog - rabbitmq","title":"RabbitMQ be\u00fczemel\u00e9se"},{"location":"Architektura/Kommunikacio/#integracios-esemeny","text":"Hozzunk l\u00e9tre egy \u00faj .NET Standard projektet Msa.Comm.Lab.Events n\u00e9ven, ami l\u00e9nyeg\u00e9ben a kommunik\u00e1ci\u00f3 contractja lesz. Ebbe vegy\u00fcnk fel egy interf\u00e9szt IOrderCreatedEvent n\u00e9ven az al\u00e1bbi tartalommal. Ez fog majd utazni az \u00fczenetsorban. public interface IOrderCreatedEvent { int ProductId { get ; } DateTimeOffset OrderPlaced { get ; } int Quantity { get ; } } Az Order projektben adjunk referenci\u00e1t az el\u0151z\u0151leg l\u00e9trehozott projektre. Hozzunk l\u00e9tre egy IntegrationEvents mapp\u00e1t az Order projektben, majd abba implement\u00e1ljuk egy oszt\u00e1lyba az IOrderCreatedEvent interf\u00e9szt. public class OrderCreatedEvent : IOrderCreatedEvent { public int ProductId { get ; set ; } public DateTimeOffset OrderPlaced { get ; set ; } public int Quantity { get ; set ; } } Ism\u00e9telj\u00fck meg ezt a Catalog szolg\u00e1ltat\u00e1sban is.","title":"Integr\u00e1ci\u00f3s esem\u00e9ny"},{"location":"Architektura/Kommunikacio/#masstransit-hasznalata","text":"","title":"MassTransit haszn\u00e1lata"},{"location":"Architektura/Kommunikacio/#kuldo-oldal","text":"Kezdj\u00fck a a k\u00fcld\u0151 oldallal. Vegy\u00fck fel az Order szolg\u00e1ltat\u00e1sba a k\u00f6vetkez\u0151 NuGet csomagokat. <PackageReference Include= \"MassTransit.Extensions.DependencyInjection\" Version= \"5.5.5\" /> <PackageReference Include= \"MassTransit.Extensions.Logging\" Version= \"5.5.5\" /> <PackageReference Include= \"MassTransit.RabbitMQ\" Version= \"5.5.5\" /> Konfigur\u00e1ljuk be a Startup oszt\u00e1lyban a MassTransit-ot, hogy RabbitMQ-t haszn\u00e1ljon, adjuk meg a logol\u00e1s m\u00f3dj\u00e1t, \u00e9s hogy melyik \u00fczenetsorba rakja az IOrderCreatedEvent esem\u00e9ny\u00fcnket. services . AddMassTransit ( x => { x . AddBus ( provider => Bus . Factory . CreateUsingRabbitMq ( cfg => { cfg . Host ( new Uri ( $ \"rabbitmq://rabbitmq:/\" ), hostConfig => { hostConfig . Username ( \"guest\" ); hostConfig . Password ( \"guest\" ); }); cfg . UseExtensionsLogging ( provider . GetRequiredService < ILoggerFactory >()); })); EndpointConvention . Map < IOrderCreatedEvent >( new Uri ( \"rabbitmq://rabbitmq:/integration\" )); }); Konfigur\u00e1ci\u00f3 korrektebben Itt is \u00e9rdemesebb lenne a rabbitmq hosztnevet \u00e9s a bejelentkez\u00e9si adatokat konfigur\u00e1ci\u00f3b\u00f3l nyerni. S\u00fcss\u00fck el az esem\u00e9nyt a TestController ben. K\u00e9rj\u00fck el az IBusControl objektumot, \u00e9s azon h\u00edvjuk meg a Publish met\u00f3dust. MassTransit eset\u00e9ben a Publish s\u00fcti el a broadcast szer\u0171 esem\u00e9nyeket, m\u00edg a Send ink\u00e1bb a command t\u00edpus\u00fa \u00fczenetekre van kihegyezve. private readonly ICatalogApiClient _catalogApiClient ; private readonly IBusControl _bus ; public TestController ( ICatalogApiClient catalogApiClient , IBusControl bus ) { _catalogApiClient = catalogApiClient ; _bus = bus ; } // ... [HttpPost(\"[action] \")] public async Task < ActionResult > CreateOrder () { await _bus . Publish ( new OrderCreatedEvent { ProductId = 1 , Quantity = 1 , OrderPlaced = DateTimeOffset . UtcNow }); return Ok ( new { Message = \"Megrendel\u00e9s sikeres!\" }); }","title":"K\u00fcld\u0151 oldal"},{"location":"Architektura/Kommunikacio/#fogado-oldal","text":"T\u00e9rj\u00fcnk \u00e1t a fogad\u00f3 oldalra. A Catalog szolg\u00e1ltat\u00e1s projektbe vegy\u00fck fel szint\u00e9n az al\u00e1bbi NuGet csomagokat. <PackageReference Include= \"MassTransit.Extensions.DependencyInjection\" Version= \"5.5.5\" /> <PackageReference Include= \"MassTransit.Extensions.Logging\" Version= \"5.5.5\" /> <PackageReference Include= \"MassTransit.RabbitMQ\" Version= \"5.5.5\" /> Sz\u00fcks\u00e9g\u00fcnk lesz egy az esem\u00e9nyt lekezel\u0151 oszt\u00e1lyra is, aminek MassTransit esetben az IConsumer<T> interf\u00e9szt kell megval\u00f3s\u00edtania. Vegy\u00fcnk fel a Catalog projektbe egy IntegrationEventHandlers mapp\u00e1t, majd abba hozzunk l\u00e9tre egy \u00faj oszt\u00e1lyt OrderCreatedEventHandler n\u00e9ven az al\u00e1bbi tartommal. Itt csak a kapott adatok alapj\u00e1n friss\u00edts\u00fck az adatainkat: a mi M\u00f3ricka p\u00e9ld\u00e1nkban a ProductController ben l\u00e9v\u0151 statikus list\u00e1n dolgozunk. public class OrderCreatedEventHandler : IConsumer < IOrderCreatedEvent > { public Task Consume ( ConsumeContext < IOrderCreatedEvent > context ) { var product = ProductController . Products . SingleOrDefault ( p => p . ProductId == context . Message . ProductId ); if ( product != null ) { product . Stock -= context . Message . Quantity ; } return Task . CompletedTask ; } } Konfigur\u00e1ljuk be a Startup -ban a MassTransit-ot, hogy RabbitMQ-t haszn\u00e1ljon, adjuk meg a logol\u00e1s m\u00f3dj\u00e1t, illetve hogy melyik \u00fczenetsorb\u00f3l v\u00e1rja az IOrderCreatedEvent esem\u00e9ny\u00fcnket, \u00e9s azt melyik IConsumer megval\u00f3s\u00edt\u00e1s kezelje le. services . AddMassTransit ( x => { x . AddConsumer < OrderCreatedEventHandler >(); x . AddBus ( provider => Bus . Factory . CreateUsingRabbitMq ( cfg => { var host = cfg . Host ( new Uri ( $ \"rabbitmq://rabbitmq:\" ), hostConfig => { hostConfig . Username ( \"guest\" ); hostConfig . Password ( \"guest\" ); }); cfg . UseExtensionsLogging ( provider . GetRequiredService < ILoggerFactory >()); cfg . ReceiveEndpoint ( host , \"integration\" , ep => { ep . ConfigureConsumer < OrderCreatedEventHandler >( provider ); }); })); }); Fogad\u00f3 oldalon m\u00e9g sz\u00fcks\u00e9ges elind\u00edtani egy h\u00e1tt\u00e9rfolyamatot is, ami figyeli az \u00fczenetsorokat. Ezt most tegy\u00fck meg a Startup.Configure() met\u00f3dus v\u00e9g\u00e9n. public void Configure ( IApplicationBuilder app , IHostingEnvironment env , IApplicationLifetime lifetime ) { // ... app . UseMvc (); var bus = app . ApplicationServices . GetService < IBusControl >(); var busHandle = TaskUtil . Await (() => bus . StartAsync ()); lifetime . ApplicationStopping . Register (() => busHandle . Stop ()); } Pr\u00f3b\u00e1ljuk ki! K\u00e9rj\u00fck le a term\u00e9keket S\u00fcss\u00fck el fiddlerb\u0151l vagy Postmanb\u0151l a CreateOrder Actiont N\u00e9zz\u00fck meg, hogy friss\u00fclt-e a term\u00e9k rakt\u00e1rk\u00e9szlete Hibakeres\u00e9s Ha nem friss\u00fclt, akkor a logokb\u00f3l vagy a rabbitmq menedzsment fel\u00fclet\u00e9r\u0151l lehet nyomozni. Ha azt tapasztaljuk hogy nem tud csatlakozni valamelyik szolg\u00e1ltat\u00e1s, akkor ellen\u0151rizz\u00fck a docker-compose file-t \u00e9s a connection string-eket. Ha azt tapasztaljuk, hogy skipped \u00fczenetsorba ker\u00fclnek az \u00fczenetek, akkor a k\u00fcld\u0151 oldal rendben m\u0171k\u00f6d\u00f6tt, de valami\u00e9rt a fogad\u00f3 oldal nem tudott a megadott \u00fczenett\u00edpusra egyszer sem feliratkozni helyesen. Kitekint\u00e9s A fenti p\u00e9ld\u00e1ban nem t\u00f6r\u0151dt\u00fcnk az idempotens megval\u00f3s\u00edt\u00e1ssal, ez mindig k\u00fcl\u00f6n tervez\u00e9st ig\u00e9nyel, az \u00fczleti logik\u00e1nk f\u00fcggv\u00e9ny\u00e9ben, de mindenk\u00e9ppen \u00e9rdemes a tervez\u00e9s sor\u00e1n figyelni erre. Mi most broadcast jelleg\u0171 integr\u00e1ci\u00f3s esem\u00e9nyt s\u00fct\u00f6tt\u00fcnk el. Ne feledj\u00fcnk van ennek egy m\u00e1sik vari\u00e1nsa is, amikor command szer\u0171 \u00fczenetet k\u00fcld\u00fcnk egy m\u00e1sik szolg\u00e1ltat\u00e1snak, \u00e9s ott elv\u00e1rjuk az esem\u00e9ny lefut\u00e1s\u00e1t. Integr\u00e1ci\u00f3s esem\u00e9ny sor\u00e1n a fogad\u00f3 f\u00e9lre van b\u00edzva, hogy mit kezd a kapott inform\u00e1ci\u00f3val.","title":"Fogad\u00f3 oldal"},{"location":"Architektura/Kommunikacio/#contract-first-api-keszites-grpc","text":"A feladat c\u00e9lja kipr\u00f3b\u00e1lni a contract-first megkozel\u00edt\u00e9st: teh\u00e1t el\u0151bb az interf\u00e9sz le\u00edr\u00f3t k\u00e9sz\u00edtj\u00fck el egy Domain Specific Language (DSL) seg\u00edt\u00e9s\u00e9vel, majd abb\u00f3l gener\u00e1lunk kliens \u00e9s szerver oldali k\u00f3dot. Ezt REST-es API-val is meg tudn\u00e1nk tenni a Swagger/OpenAPI le\u00edr\u00f3val, de mi most gRPC-n kereszt\u00fcl pr\u00f3v\u00e1ljuk ki. Feladatunkban a Catalog szolg\u00e1ltat\u00e1s ProductController k\u00e9t m\u0171velet\u00e9t \u00edrjuk meg gRPC protokollal. Majd h\u00edvjuk meg ezeket az Order szolg\u00e1ltat\u00e1sb\u00f3l. .NET Core 3.0+ A gRPC csak .NET Core 3.0-t\u00f3l t\u00e1mogatott, \u00edgy \u00e9rdemes a Visual Studio-t friss\u00edteni legal\u00e1bb 16.3.x verzi\u00f3ra","title":"Contract-First API k\u00e9sz\u00edt\u00e9s - gRPC"},{"location":"Architektura/Kommunikacio/#szerver-oldal","text":"Lehet\u0151s\u00e9g lenne egy Projektsablonb\u00f3l is dolgoznunk (File / New Project / gRPC Serice), ami szint\u00e9n egy ASP.NET Core 3.0-\u00e1s projekt, de most a megl\u00e9v\u0151 Catalog projekt\u00fcnkbe rakjuk bele ezt a funkcionalit\u00e1st. Vegy\u00fck fel a Catalog projektbe az al\u00e1bbi NuGet csomagot. Fontos, hogy a 2.23.1 -es verzi\u00f3 legyen, mert \u00e9n a 2.23.2-es verzi\u00f3val belefutottam egy bugba. <PackageReference Include= \"Grpc.AspNetCore\" Version= \"2.23.1\" /> A Catalog projektbe vegy\u00fcnk fel egy Protos mapp\u00e1t, \u00e9s abba egy Text f\u00e1jlt catalog.proto n\u00e9ven, ami a szolg\u00e1ltat\u00e1sle\u00edr\u00f3nk lesz. K\u00e9sz\u00fcts\u00fcnk el egy saj\u00e1t szolg\u00e1ltat\u00e1sle\u00edr\u00f3t a Catalog REST API-nk mint\u00e1j\u00e1ra. Tartalom a k\u00f6vetkez\u0151: Egy szolg\u00e1ltat\u00e1sunk lesz CatalogService n\u00e9ven A szolg\u00e1ltat\u00e1sban rpc kulcssz\u00f3val tudunk m\u0171veleteket defini\u00e1lni, megadva a bemen\u0151 param\u00e9tert \u00e9s a visszat\u00e9r\u00e9si \u00e9rt\u00e9ket. Ezek k\u00fcl\u00f6n defini\u00e1lt message t\u00edpusok lehetnek. Az els\u0151 legyen a GetProducts Sajnos a proto szabv\u00e1nyban nincs void t\u00edpus\u00fa \u00fczenet, ez\u00e9rt sz\u00fcks\u00e9ges felvenn\u00fcnk egy \u00fcres \u00fczenetet Empty n\u00e9ven, ez fogja majd a void t\u00edpust reprezent\u00e1lni A visszat\u00e9r\u00e9si \u00e9rt\u00e9k t\u00edpus\u00e1t a ProductsResponse \u00fczenet \u00edrja le, amiben t\u00f6bb Product t\u00edpus\u00fa \u00fczenet lesz (nevezhetn\u00e9nk t\u00f6mbnek is, itt repeated kulcssz\u00f3) Az \u00fczenetekben meg kell adnunk a mez\u0151k sorrendj\u00e9t az egyenl\u0151s\u00e9g m\u00f6g\u00f6tt a bin\u00e1ris soros\u00edt\u00e1s miatt Product \u00fczenet az eddig megszokott propertykkel rendelkezzen a proto-s t\u00edpusokkal (sajnos itt nincs decimal ) A GetProduct m\u0171velet egy azonos\u00edt\u00f3t v\u00e1r, \u00e9s egy Product -tal t\u00e9r vissza. Az id-t is sz\u00fcks\u00e9ges becsomagolni egy \u00fczenett\u00edpusba: GetProductRequest syntax = \"proto3\" ; option csharp_namespace = \"Msa.Comm.Lab.Services.Catalog.Grpc\" ; package Catalog ; service CatalogService { rpc GetProducts ( Empty ) returns ( ProductsResponse ); rpc GetProduct ( GetProductRequest ) returns ( Product ); } message Empty { } message ProductsResponse { repeated Product Products = 1 ; } message Product { int32 ProductId = 1 ; string Name = 2 ; double UnitPrice = 3 ; int32 Stock = 4 ; } message GetProductRequest { int32 ProductId = 1 ; } A .proto f\u00e1jlb\u00f3l a modellek \u00e9s egy \u0151soszt\u00e1ly is gener\u00e1l\u00f3dik a ford\u00edt\u00e1s sor\u00e1n. Ehhez viszont a Catalog projekt f\u00e1jlban fel kell venni a k\u00f6vetkez\u0151 elemet: <ItemGroup> <Protobuf Include= \"Protos\\catalog.proto\" GrpcServices= \"Server\" /> </ItemGroup> A Catalog projektbe hozzunk l\u00e9tre egy Services mapp\u00e1t, majd abba egy CatalogService oszt\u00e1lyt, ami a Grpc.CatalogService.CatalogServiceBase -b\u0151l sz\u00e1rmazik le. Nek\u00fcnk csak fel\u00fcl kell defini\u00e1lni a k\u00edv\u00e1nt met\u00f3dusokat. Ide is vegy\u00fck fel az al\u00e1bbi statikus list\u00e1t, \u00e9s ennek a seg\u00edts\u00e9g\u00e9vel val\u00f3s\u00edtsuk meg az \u00fczleti m\u0171veleteket. Figyelj\u00fck meg, hogy a .proto-b\u00f3l gener\u00e1lt t\u00edpusokkal tudunk itt dolgozni. public class CatalogService : Grpc . CatalogService . CatalogServiceBase { private readonly ILogger < CatalogService > _logger ; internal static List < Product > _products = new List < Product > { new Product { ProductId = 1 , Name = \"S\u00f6r\" , Stock = 10 , UnitPrice = 250 }, new Product { ProductId = 2 , Name = \"Bor\" , Stock = 5 , UnitPrice = 890 }, new Product { ProductId = 3 , Name = \"Csoki\" , Stock = 15 , UnitPrice = 200 }, }; public CatalogService ( ILogger < CatalogService > logger ) { _logger = logger ; } public override Task < ProductsResponse > GetProducts ( Empty request , ServerCallContext context ) { var response = new ProductsResponse (); response . Products . Add ( _products ); return Task . FromResult ( response ); } public override Task < Product > GetProduct ( GetProductRequest request , ServerCallContext context ) { var product = _products . SingleOrDefault ( p => p . ProductId == request . ProductId ); return Task . FromResult ( product ); } } A Catalog Startup oszt\u00e1lyban regisztr\u00e1ljuk be a gRPC szolg\u00e1ltat\u00e1sainkat. services . AddGrpc (); app . UseEndpoints ( endpoints => { endpoints . MapGrpcService < Services . CatalogService >(); endpoints . MapControllers (); });","title":"Szerver oldal"},{"location":"Architektura/Kommunikacio/#kliens-oldal","text":"Adjunk az Order projekthez egy szolg\u00e1ltat\u00e1s referenci\u00e1t. Az Order projekten jobb gomb / Add / Service Reference / gRPC / Add / File ahol tall\u00f3zuk ki a m\u00e1sik projektben tal\u00e1lhat\u00f3 .proto f\u00e1jlt \u00e9s Client m\u00f3dban gener\u00e1ljuk le a sz\u00fcks\u00e9ges oszt\u00e1lyokat. Ez a m\u0171velet a sz\u00fcks\u00e9ges NuGet csomagokat is hozz\u00e1adja a projekthez. A Startup oszt\u00e1lyban regisztr\u00e1ljuk be a DI kont\u00e9nerbe a gRPC kliens\u00fcnket. Mivel a gRPC-nek sz\u00fcks\u00e9ge van HTTPS-re, viszont most a kont\u00e9nereink nem b\u00edznak egym\u00e1s tanus\u00edtv\u00e1nyaiban, ez\u00e9rt most ideiglenesen kapcsoljuk ki a HTTPS hib\u00e1kat a gRPC-t alatt l\u00e9v\u0151 HttpClient -ben. services . AddGrpcClient < CatalogService . CatalogServiceClient >( o => { o . Address = new Uri ( \"https://msa.comm.lab.services.catalog\" ); }). ConfigurePrimaryHttpMessageHandler ( p => new HttpClientHandler { ServerCertificateCustomValidationCallback = HttpClientHandler . DangerousAcceptAnyServerCertificateValidator }); IHttpClientFactory hasz\u00e1lata Megfigyelhetj\u00fck, hogy a gRPC kliens is az IHttpClientFactory megold\u00e1sra \u00e9p\u00edt. A TestController ben cser\u00e9lj\u00fck le A REST kliens\u00fcnket a gRPC kliensre. //private readonly ICatalogApiClient _catalogApiClient; private readonly IBusControl _bus ; private readonly CatalogService . CatalogServiceClient _catalogServiceClient ; public TestController ( //ICatalogApiClient catalogApiClient, IBusControl bus , CatalogService . CatalogServiceClient catalogServiceClient ) { //_catalogApiClient = catalogApiClient; _bus = bus ; _catalogServiceClient = catalogServiceClient ; } [HttpGet] public async Task < ActionResult < IEnumerable < Catalog . Grpc . Product >>> Get () { return ( await _catalogServiceClient . GetProductsAsync ( new Empty ())). Products ; } Pr\u00f3b\u00e1ljuk ki!","title":"Kliens oldal"},{"location":"Architektura/Kommunikacio/#osszefoglalas","text":"A gyakorlat sor\u00e1n n\u00e9zt\u00fcnk p\u00e9ld\u00e1t REST-es kommunik\u00e1ci\u00f3 eset\u00e9ben a hibat\u0171r\u00e9s n\u00f6vel\u00e9s\u00e9re egy Retry policy-vel. Majd Aszinkron kommunik\u00e1ci\u00f3t implement\u00e1ltunk egy RabbitMQ \u00fczenetsor \u00e9s MassTransit konyvt\u00e1r seg\u00edts\u00e9g\u00e9vel a k\u00e9t szolg\u00e1ltat\u00e1sunk k\u00f6z\u00f6tt. V\u00e9g\u00fcl Contract first megk\u00f6zel\u00edt\u00e9ssel k\u00e9sz\u00edtett\u00fcnk szolg\u00e1ltat\u00e1st, most gRPC-n kipr\u00f3b\u00e1lva.","title":"\u00d6sszefoglal\u00e1s"},{"location":"DevOps/Logging-Health-Checks/","text":"El\u0151ad\u00e1s \u00b6 Napl\u00f3z\u00e1s, Health Checks El\u0151z\u0151 alkalmak cheatsheet Docker docker rm -f $(docker ps -aq) K8S dashboard kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/alternative/kubernetes-dashboard.yaml kubectl proxy http://localhost:8001/api/v1/namespaces/kube-system/services/kubernetes-dashboard:/proxy Traefic helm version helm init helm install stable/traefik --name traefik --version \"1.78.3\" --set rbac.enabled=true --set logLevel=debug --set dashboard.enabled=true --set service.nodePorts.http=30080 --set serviceType=NodePort DB kubectl apply -f db App kubectl apply -f app $env:IMAGE_TAG=\"v1\" vagy \u00edrjuk \u00e1t a yml-ben latest-re ideiglenesen docker-compose build http://localhost:30080 Napl\u00f3z\u00e1s \u00b6 Az ASP.NET Core TODO webalkalmaz\u00e1sunkban implement\u00e1ljunk szemantikus napl\u00f3z\u00e1st. A napl\u00f3bejegyz\u00e9seket az \u00fagynevezett ELK technol\u00f3giai stackkel fogjuk feldolgozni. E : Elasticsearch A napl\u00f3bejegyz\u00e9sek t\u00e1rol\u00e1s\u00e1\u00e9rt \u00e9s indexel\u00e9s\u00e9\u00e9rt/kereshet\u0151s\u00e9g biztos\u00edt\u00e1s\u00e1\u00e9rt felel\u0151s L : Logstash Transzform\u00e1ci\u00f3s r\u00e9teg az alkalmaz\u00e1s \u00e9s a perzisztencia r\u00e9teg k\u00f6z\u00f6tt K : Kibana Adatvizualiz\u00e1ci\u00f3\u00e9rt felel\u0151s komponens A labor keret\u00e9ben most a Logstash transzform\u00e1ci\u00f3s komponenst kihagyjuk a k\u00e9pb\u0151l id\u0151 hi\u00e1ny\u00e1ban, \u00e9s k\u00f6zvetlen\u00fcl az Elasticsearch-be fog \u00edrni az alkalmaz\u00e1s. Azure Application Insights Ha az alkalmaz\u00e1sunkat Azure PaaS szolg\u00e1ltat\u00e1sokra \u00e9p\u00edtj\u00fck, akkor az aj\u00e1nlott megold\u00e1s az Azure Application Insights . ELK-t akkor \u00e9rdemes haszn\u00e1lni, ha az architekt\u00far\u00e1nkat felh\u0151 szolg\u00e1ltat\u00f3 f\u00fcggetlennek szeretn\u00e9nk tartani (ami viszonylag ritka). Az (ASP).NET Core kiv\u00e1l\u00f3 absztrakci\u00f3s r\u00e9teget ny\u00fajt nek\u00fcnk a napl\u00f3z\u00e1shoz az ILogger<T> \u00e9s kapcsol\u00f3d\u00f3 interf\u00e9szein kereszt\u00fcl. Egyszer\u0171bb implement\u00e1ci\u00f3i a keretrendszerben is megtal\u00e1lhat\u00f3ak, de ezek nem el\u00e9g\u00edtik ki a szemantikus napl\u00f3z\u00e1shoz kapcsol\u00f3d\u00f3 ig\u00e9nyeket: m\u00e9gpedig, hogy egyszer\u0171en \u00e9s egys\u00e9gesen parsolhat\u00f3ak legyenek a napl\u00f3bejegyz\u00e9sek. Haszn\u00e1ljuk a Serilog k\u00fcls\u0151 oszt\u00e1lyk\u00f6nyvt\u00e1rat a napl\u00f3z\u00e1sra, ami a a fenti absztrakci\u00f3ra \u00e9p\u00fcl r\u00e1. Seriloghoz t\u00f6bb nyel\u0151 (Sink) implement\u00e1ci\u00f3 is k\u00e9sz\u00fclt, \u00e9s van kifejezetten Elasticsearch-be napl\u00f3z\u00f3 csomag is. El\u0151k\u00e9sz\u00fclet \u00b6 Kl\u00f3nozzuk le a kiindul\u00f3 projektet, ami ASP.NET Core 3.0-ra lett felmigr\u00e1lva az el\u0151z\u0151 alkalmakhoz k\u00e9pest. git clone https://github.com/bmeviauav42/todoapp-logging-hc.git Pr\u00f3b\u00e1ljuk ki, hogy docker-compose-zal elindul-e az alkalmaz\u00e1sunk, \u00e9s tesztelj\u00fck az API GW-en kereszt\u00fcl a m\u0171k\u00f6d\u00e9st. ELK \u00b6 Vegy\u00fcnk fel a docker-compose.yml-be k\u00e9t \u00faj kont\u00e9nert az Elasticsearch-nek \u00e9s a Kibana-nak. Fontos, hogy ne az OSS verzi\u00f3t haszn\u00e1ljuk az ES-b\u0151l, mert abban nincsenek alap\u00e9rtelmezetten a Kibana-hoz sz\u00fcks\u00e9ges komponensek feltelep\u00edtve. logs : image : docker.elastic.co/elasticsearch/elasticsearch:7.4.2 container_name : logs environment : - cluster.name=logs # Settings to start Elasticsearch in a single-node development environment - node.name=logs - discovery.type=single-node - \"ES_JAVA_OPTS=-Xms256m -Xmx256m\" ports : - \"9202:9200\" volumes : - logs-elastic-data:/usr/share/elasticsearch/data networks : - todoapp-network kibana : image : docker.elastic.co/kibana/kibana:7.4.2 container_name : kibana environment : - ELASTICSEARCH_HOSTS=http://logs:9200 ports : - \"5602:5601\" depends_on : - logs networks : - todoapp-network A k\u00f6tetek k\u00f6z\u00e9 is fel kell venn\u00fcnk egy bejegyz\u00e9st. volumes : # The volumes will store the database data; kept even after the containers are deleted todoapp-mongo-data : driver : local todoapp-elastic-data : driver : local logs-elastic-data : driver : local Serilog \u00b6 Vegy\u00fck fel az al\u00e1bbi csomagokat a Todos.Api projektbe. <PackageReference Include= \"Serilog\" Version= \"2.9.0\" /> <PackageReference Include= \"Serilog.AspNetCore\" Version= \"3.2.0\" /> <PackageReference Include= \"Serilog.Exceptions\" Version= \"5.3.1\" /> <PackageReference Include= \"Serilog.Settings.Configuration\" Version= \"3.1.0\" /> <PackageReference Include= \"Serilog.Sinks.Elasticsearch\" Version= \"8.0.1\" /> A Program.cs-ben konfigur\u00e1ljuk be a Serilog-ot. public static IHostBuilder CreateWebHostBuilder ( string [] args ) => Host . CreateDefaultBuilder ( args ) . UseSerilog (( hostingContext , loggerConfiguration ) => loggerConfiguration . ReadFrom . Configuration ( hostingContext . Configuration ) . Enrich . FromLogContext () . Enrich . WithExceptionDetails () . WriteTo . Console () . WriteTo . Elasticsearch ( new ElasticsearchSinkOptions ( new Uri ( hostingContext . Configuration . GetValue < string >( \"LogsUrl\" ))) { AutoRegisterTemplate = true , })) . ConfigureWebHostDefaults ( c => { c . UseStartup < Startup >(); }); Hibakezel\u00e9s az indul\u00e1s k\u00f6zben A Serilog aj\u00e1nl\u00e1sok szerint a Main f\u00fcggv\u00e9nyben lenne \u00e9rdemes a Serilog-ot inicializ\u00e1lni, hogy az app indul\u00e1sa sor\u00e1n fell\u00e9p\u0151 kiv\u00e9teleket is le lehessen logolni. Most ett\u0151l az aspektust\u00f3l eltekint\u00fcnk az egyszer\u0171s\u00e9g kedv\u00e9\u00e9rt. A k\u00f3dr\u00e9szletb\u0151l l\u00e1thatjuk, hogy a Serilog-ot konfigur\u00e1lhatjuk az IConfiguration -b\u00f3l is, de ezt most nem fogjuk kihaszn\u00e1lni, \u00e9s itt inline adjuk meg az alap\u00e9rtelmez\u00e9seket. K\u00e9t Sink-et haszn\u00e1lunk most A konzolra \u00edrunk, mivel ez egy \u00e1ltal\u00e1nos elv\u00e1r\u00e1s a kont\u00e9neriz\u00e1lt alkalmaz\u00e1sok eset\u00e9ben Elasticsearchbe \u00edrjuk a logot. 2 Enrichert haszn\u00e1lunk Az enricher-ek olyan \u00e1ltal\u00e1nos komponensek, amik a logbejegyz\u00e9seket kontextusf\u00fcgg\u0151 inform\u00e1ci\u00f3kkal tudj\u00e1k kieg\u00e9sz\u00edteni. pl.: id\u0151b\u00e9lyeg, alkalmaz\u00e1s neve, g\u00e9p neve, sz\u00e1l azonos\u00edt\u00f3ja stb. Most egy \u00e1ltal\u00e1nos Enricher-t vesz\u00fcnk fel a FromLogContext szem\u00e9ly\u00e9ben. Exception-\u00f6k r\u00e9szletes adataival eg\u00e9sz\u00edtj\u00fck ki a logot a WithExceptionDetails -szel Fentebb is l\u00e1thatjuk, hogy az Elasticsearch URL-j\u00e9t a konfigur\u00e1ci\u00f3b\u00f3l nyerj\u00fck. Adjuk most ezt meg k\u00f6rnyezeti v\u00e1ltoz\u00f3k\u00e9nt a docker-compose.cs.debug.yml \u00e1llom\u00e1nyban. - ASPNETCORE_LogsUrl=http://logs:9200 A Serilog az appsetting.json \u00e9s az appsettings.Development.json-b\u0151l nem haszn\u00e1lja fel a Logging szekci\u00f3t. Ezeket az ig\u00e9nyess\u00e9g kedv\u00e9\u00e9rt t\u00f6r\u00f6lhetj\u00fck. Ezek csak a Microsoft-os ILogger implement\u00e1ci\u00f3 sz\u00e1m\u00e1ra kellenek. Logoljunk egy saj\u00e1t esem\u00e9nyt, most a p\u00e9lda kedv\u00e9\u00e9rt a TodosRepository-ban. K\u00e9rj\u00fcnk el egy ILogger<T> -t private readonly ILogger < TodosRepository > _logger ; public TodosRepository ( ElasticClient elasticClient , ILogger < TodosRepository > logger ) { this . elasticClient = elasticClient ; _logger = logger ; } Napl\u00f3zzuk info szinten a todo l\u00e9trehoz\u00e1s\u00e1t. public async Task < TodoItem > Insert ( CreateNewTodoRequest value ) { // This operation is ***NOT*** idempotent! var result = await elasticClient . IndexDocumentAsync ( value . ToDal ()); var todo = await FindById ( result . Id ); _logger . LogInformation ( \"Todo with data {@todoitem} for user:{userid} has been created\" , todo , todo . UserId ); return todo ; } Figyelj\u00fck meg, hogy nem haszn\u00e1ltuk a C# string interpol\u00e1ci\u00f3 funkci\u00f3j\u00e1t ( $\"{userid}\" )! Ez sz\u00e1nd\u00e9kos szemantikus napl\u00f3z\u00e1s eset\u00e9n! Ha szemantikus logol\u00e1st akarunk megval\u00f3s\u00edtani, akkor ne csak a log bejegyz\u00e9s sz\u00f6veg\u00e9ben gondolkodjunk, hanem minden kapcsol\u00f3d\u00f3 inform\u00e1ci\u00f3ban. A fenti esetben az \u00fagynevezett log message template term\u00e9szetesen ki lesz \u00e9rt\u00e9kelve \u00e9s be lesz helyettes\u00edtve a placeholderekre a param\u00e9terek, de \u00edgy a logger komponensnek lehet\u0151s\u00e9ge van ezeket a param\u00e9tereket nem csak a log \u00fczenetbe belerakni, hanem a mi eset\u00fcnkben az Elasticsearch-ben kereshet\u0151en elt\u00e1rolni. A template-ek eset\u00e9ben csak nem egyszer\u0171 ToString() h\u00edv\u00e1st lehet v\u00e9gezni, hanem a fenti p\u00e9ld\u00e1ban a {@todoitem} placeholder eset\u00e9ben a @ jelent\u00e9se az objektum soros\u00edt\u00e1s\u00e1ra vonatkozik. l\u00e1sd: https://github.com/serilog/serilog/wiki/Structured-Data Oda kell figyelni, hogy a template-ben l\u00e9v\u0151 propertyknek a JSON t\u00edpusa ( int, string, obj, array stb) els\u0151 besz\u00far\u00e1skor fix\u00e1lva lesznek az ES s\u00e9m\u00e1j\u00e1ban. Ha refaktor\u00e1ljuk a template-et \u00e9s m\u00e1st pr\u00f3b\u00e1lunk logolni, akkor szimpl\u00e1n nem fog besz\u00far\u00f3dni az ES-be. Kibana \u00b6 Futtassuk az alkalmaz\u00e1sunkat \u00e9s gener\u00e1ljunk egy bejegyz\u00e9st a fenti funkci\u00f3val. Nyissuk meg a Kibana-t. A log n\u00e9zetben m\u00e9g nem l\u00e1tunk semmit. A kiban\u00e1nak mondjuk meg, hogy milyen index-en keresse a bejegyz\u00e9seket. Eset\u00fcnkben ez alap\u00e9rtelmezetten a logstash-* mint\u00e1ra fog illeszkedni. M\u00e1sodik l\u00e9p\u00e9sk\u00e9nt v\u00e1lasszuk ki a @timestamp mez\u0151t a sz\u0171r\u00e9shez. Vizsg\u00e1ljuk meg a logbejegyz\u00e9seket, kit\u00fcntetetten a saj\u00e1t TODO l\u00e9trehoz\u00e1s\u00e1val kapcsolatos bejegyz\u00e9st. Figyelj\u00fck meg, hogy szinte minden mez\u0151 kereshet\u0151 \u00e9s sz\u00e9pen struktur\u00e1ltan ker\u00fclnek a bejegyz\u00e9s adatai lement\u00e9sre. Health Checks \u00b6 Implement\u00e1ljunk a Todos.Api projekt\u00fcnkh\u00f6z health check-et, amit majd a kubernetes fog els\u0151sorban felhaszn\u00e1lni. El\u0151k\u00e9sz\u00fclet \u00b6 Mivel Kubernetes-hez m\u00e9g nem konfigur\u00e1ltuk fel a loggol\u00e1s szolg\u00e1ltat\u00e1sait, ez\u00e9rt a jelenlegi munk\u00e1nkat commitoljuk egy k\u00fcl\u00f6n \u00e1gra, majd \u00e1lljunk vissza a kiindul\u00f3 \u00e1gra. git branch logging git commit -m \"napl\u00f3z\u00e1s k\u00e9sz\" git checkout kiindul\u00f3TODO El\u0151z\u0151 \u00f3r\u00e1k mint\u00e1j\u00e1ra \u00fczemelj\u00fck be a kubernetes verzi\u00f3j\u00e1t az alkalmaz\u00e1snak. N\u00e9zz\u00fck meg a dashboardon a rendszer \u00e1llapot\u00e1t \u00e9s pr\u00f3b\u00e1ljuk ki az alkalmaz\u00e1st. Health Check implement\u00e1ci\u00f3 \u00b6 K\u00e9sz\u00edts\u00fcnk readiness \u00e9s liveness pr\u00f3b\u00e1kat a kubernetes sz\u00e1m\u00e1ra. Ehhez haszn\u00e1ljuk fel az ASP.NET Core 2.2 \u00f3ta rendelkez\u00e9sre \u00e1ll\u00f3 be\u00e9p\u00edtett Health Check API-kat. Liveness \u00b6 Kezdj\u00fck az egyszer\u0171bbel. Akkor lesz live egy szolg\u00e1ltat\u00e1s, ha az app fel\u00e1lt \u00e9s ki tudja szolg\u00e1lni k\u00fcls\u0151 f\u00fcgg\u0151s\u00e9gek n\u00e9lk\u00fcl a liveness pr\u00f3b\u00e1t. Ehhez vegy\u00fcnk fel egy \u00fcres health checket a /health/live v\u00e9gpontra. public void ConfigureServices ( IServiceCollection services ) { // ... services . AddHealthChecks () . AddCheck ( \"liveness\" , () => HealthCheckResult . Healthy ()); } public void Configure ( IApplicationBuilder app , IWebHostEnvironment env ) { //... app . UseEndpoints ( endpoints => { endpoints . MapControllers (); endpoints . MapHealthChecks ( \"/health/live\" , new HealthCheckOptions { Predicate = r => r . Name . Contains ( \"liveness\" ), ResponseWriter = UIResponseWriter . WriteHealthCheckUIResponse }); } A health check UI-hoz az al\u00e1bbi NuGet csomagot kell felvegy\u00fck a projektbe. <PackageReference Include= \"AspNetCore.HealthChecks.UI\" Version= \"3.0.4\" /> Mi most csak a soros\u00edt\u00f3 komponens\u00e9t fogjuk haszn\u00e1lni bel\u0151le ( UIResponseWriter ), id\u0151 hi\u00e1ny\u00e1ban a UI komponenst most nem \u00fczemelj\u00fck be. Pr\u00f3b\u00e1ljuk ki az \u00faj v\u00e9gpontot (F5). Readiness \u00b6 Vegy\u00fcnk fel HC-t a k\u00fcls\u0151 szolg\u00e1ltat\u00e1sainkhoz is (Elasticsearch, Redis), majd ezt publik\u00e1ljuk ki egy k\u00fcl\u00f6n v\u00e9gponton. Vegy\u00fck fel az k\u00f6vetkez\u0151 csomagokat a Todos.Api projekthez. <PackageReference Include= \"AspNetCore.HealthChecks.Elasticsearch\" Version= \"3.0.0\" /> <PackageReference Include= \"AspNetCore.HealthChecks.Redis\" Version= \"2.2.1\" /> Verzi\u00f3 fontos AspNetCore.HealthChecks.Redis csomag direkt a 2.2.1-es mert \u00f6sszeakadna a Microsoft.Extensions.Caching.Redis csomaggal az \u00fajabb verzi\u00f3. Vegy\u00fck fel a csekkol\u00e1sokat. services . AddHealthChecks () . AddCheck ( \"liveness\" , () => HealthCheckResult . Healthy ()) . AddRedis ( Configuration . GetValue < string >( \"RedisUrl\" ) ?? \"redis:6379\" , tags : new [] { \"readiness\" }) . AddElasticsearch ( Configuration . GetValue < string >( \"ElasticsearchUrl\" ) ?? \"http://elasticsearch:9200\" , tags : new [] { \"readiness\" }); IOptions<T> haszn\u00e1lata Az ASP.NET Core-os konfigur\u00e1ci\u00f3k kezel\u00e9s\u00e9re itt is c\u00e9lszer\u0171bb lenne az IOptions<T> mint\u00e1t haszn\u00e1lni, de most az egyszer\u0171s\u00e9g kedv\u00e9\u00e9rt ett\u0151l eltekint\u00fcnk. Publik\u00e1ljuk ki egy v\u00e9gponton \u0151ket. endpoints . MapHealthChecks ( \"/health/ready\" , new HealthCheckOptions { Predicate = r => r . Tags . Contains ( \"readiness\" ), ResponseWriter = UIResponseWriter . WriteHealthCheckUIResponse }); Pr\u00f3b\u00e1ljuk ki! Kubernetes probes \u00b6 Vegy\u00fck fel a kubernetes konfigur\u00e1ci\u00f3ba a liveness \u00e9s a readiness pr\u00f3b\u00e1kat. spec : containers : - name : todos livenessProbe : httpGet : path : /health/live port : 80 scheme : HTTP initialDelaySeconds : 5 periodSeconds : 10 readinessProbe : httpGet : path : /health/ready port : 80 scheme : HTTP initialDelaySeconds : 5 periodSeconds : 10 Ha mi k\u00edv\u00fclr\u0151l is meg akarjuk h\u00edvni a /health v\u00e9gpontokat, akkor vegy\u00fck fel \u0151ket az ingress konfigur\u00e1ci\u00f3ba. spec : rules : - http : paths : - path : /health/live backend : serviceName : todos # A service neve servicePort : http # A service-ben a port neve (lehet a port szama is) - path : /health/ready backend : serviceName : todos # A service neve servicePort : http # A service-ben a port neve (lehet a port szama is) Pr\u00f3b\u00e1ljuk ki! Rontsuk el a readiness pr\u00f3b\u00e1t \u00fagy, hogy el\u00edrjuk az elasticsearch connection string-j\u00e9t a k\u00f6rnyezeti v\u00e1ltoz\u00f3ban. Azt tapasztalhatjuk, hogy az \u00faj kont\u00e9ner elindult, de mivel nem ready ez\u00e9rt a r\u00e9gi szerep\u00e9t nem tudja \u00e1tvenni addig, am\u00edg ready nem lesz. Sajnos ez a hiba nem tud kijavulni mag\u00e1t\u00f3l, \u00edgy a kijav\u00edtott config ut\u00e1n fog indulni a pod. K\u00e9sz\u00edts\u00fcnk egy egyszer\u0171 m\u00f3dszert a liveness pr\u00f3ba elront\u00e1s\u00e1ra a Startup oszt\u00e1lyban a healthcheck l\u00e9trehoz\u00e1sakor. public bool IsLive { get ; private set ; } = true ; services . AddHealthChecks () . AddCheck ( \"liveness\" , () => IsLive ? HealthCheckResult . Healthy () : HealthCheckResult . Unhealthy ()) endpoints . MapGet ( \"/health/switch\" , async r => { IsLive = ! IsLive ; await r . Response . WriteAsync ( $ \"IsLive is now {IsLive}\" ); }); Enged\u00e9lyezz\u00fck k\u00edv\u00fclr\u0151l ezt a v\u00e9gpontot is. - path : /health/switch backend : serviceName : todos # A service neve servicePort : http # A service-ben a port neve (lehet a port szama is) Telep\u00edts\u00fck ki, majd rontsuk el a m\u0171k\u00f6d\u00e9st a v\u00e9gpontunkkal. K\u00f6zben figyelj\u00fck a podok \u00e1llapot\u00e1t. Egy id\u0151 ut\u00e1n l\u00e1thatjuk, hogy a pr\u00f3ba s\u00e9r\u00fclt, \u00e9s a k8s megpr\u00f3b\u00e1lja \u00fajraind\u00edtani a pod-ot, mivel ott m\u00e1r az IsLive property \u00e9rteke igaz lesz. \u00c9rdemes minden health check defini\u00e1l\u00e1sa sor\u00e1n megtervezni azt, hogy az most melyik pr\u00f3b\u00e1ba illik bele jobban. Ha van es\u00e9ly, hogy mag\u00e1t\u00f3l megjavuljon, akkor a readiness pr\u00f3b\u00e1ba \u00e9rdemes rakni (pl. valamilyen k\u00fcls\u0151 szolg\u00e1ltat\u00e1s nem el\u00e9rhet\u0151, persze ez lehet konfigur\u00e1ci\u00f3s hiba is, ahogy l\u00e1ttuk), ha pedig \u00fajraind\u00edt\u00e1s tud seg\u00edteni akkor a liveness pr\u00f3b\u00e1ba rakjuk.","title":"Napl\u00f3z\u00e1s, health check"},{"location":"DevOps/Logging-Health-Checks/#eloadas","text":"Napl\u00f3z\u00e1s, Health Checks El\u0151z\u0151 alkalmak cheatsheet Docker docker rm -f $(docker ps -aq) K8S dashboard kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/alternative/kubernetes-dashboard.yaml kubectl proxy http://localhost:8001/api/v1/namespaces/kube-system/services/kubernetes-dashboard:/proxy Traefic helm version helm init helm install stable/traefik --name traefik --version \"1.78.3\" --set rbac.enabled=true --set logLevel=debug --set dashboard.enabled=true --set service.nodePorts.http=30080 --set serviceType=NodePort DB kubectl apply -f db App kubectl apply -f app $env:IMAGE_TAG=\"v1\" vagy \u00edrjuk \u00e1t a yml-ben latest-re ideiglenesen docker-compose build http://localhost:30080","title":"El\u0151ad\u00e1s"},{"location":"DevOps/Logging-Health-Checks/#naplozas","text":"Az ASP.NET Core TODO webalkalmaz\u00e1sunkban implement\u00e1ljunk szemantikus napl\u00f3z\u00e1st. A napl\u00f3bejegyz\u00e9seket az \u00fagynevezett ELK technol\u00f3giai stackkel fogjuk feldolgozni. E : Elasticsearch A napl\u00f3bejegyz\u00e9sek t\u00e1rol\u00e1s\u00e1\u00e9rt \u00e9s indexel\u00e9s\u00e9\u00e9rt/kereshet\u0151s\u00e9g biztos\u00edt\u00e1s\u00e1\u00e9rt felel\u0151s L : Logstash Transzform\u00e1ci\u00f3s r\u00e9teg az alkalmaz\u00e1s \u00e9s a perzisztencia r\u00e9teg k\u00f6z\u00f6tt K : Kibana Adatvizualiz\u00e1ci\u00f3\u00e9rt felel\u0151s komponens A labor keret\u00e9ben most a Logstash transzform\u00e1ci\u00f3s komponenst kihagyjuk a k\u00e9pb\u0151l id\u0151 hi\u00e1ny\u00e1ban, \u00e9s k\u00f6zvetlen\u00fcl az Elasticsearch-be fog \u00edrni az alkalmaz\u00e1s. Azure Application Insights Ha az alkalmaz\u00e1sunkat Azure PaaS szolg\u00e1ltat\u00e1sokra \u00e9p\u00edtj\u00fck, akkor az aj\u00e1nlott megold\u00e1s az Azure Application Insights . ELK-t akkor \u00e9rdemes haszn\u00e1lni, ha az architekt\u00far\u00e1nkat felh\u0151 szolg\u00e1ltat\u00f3 f\u00fcggetlennek szeretn\u00e9nk tartani (ami viszonylag ritka). Az (ASP).NET Core kiv\u00e1l\u00f3 absztrakci\u00f3s r\u00e9teget ny\u00fajt nek\u00fcnk a napl\u00f3z\u00e1shoz az ILogger<T> \u00e9s kapcsol\u00f3d\u00f3 interf\u00e9szein kereszt\u00fcl. Egyszer\u0171bb implement\u00e1ci\u00f3i a keretrendszerben is megtal\u00e1lhat\u00f3ak, de ezek nem el\u00e9g\u00edtik ki a szemantikus napl\u00f3z\u00e1shoz kapcsol\u00f3d\u00f3 ig\u00e9nyeket: m\u00e9gpedig, hogy egyszer\u0171en \u00e9s egys\u00e9gesen parsolhat\u00f3ak legyenek a napl\u00f3bejegyz\u00e9sek. Haszn\u00e1ljuk a Serilog k\u00fcls\u0151 oszt\u00e1lyk\u00f6nyvt\u00e1rat a napl\u00f3z\u00e1sra, ami a a fenti absztrakci\u00f3ra \u00e9p\u00fcl r\u00e1. Seriloghoz t\u00f6bb nyel\u0151 (Sink) implement\u00e1ci\u00f3 is k\u00e9sz\u00fclt, \u00e9s van kifejezetten Elasticsearch-be napl\u00f3z\u00f3 csomag is.","title":"Napl\u00f3z\u00e1s"},{"location":"DevOps/Logging-Health-Checks/#elokeszulet","text":"Kl\u00f3nozzuk le a kiindul\u00f3 projektet, ami ASP.NET Core 3.0-ra lett felmigr\u00e1lva az el\u0151z\u0151 alkalmakhoz k\u00e9pest. git clone https://github.com/bmeviauav42/todoapp-logging-hc.git Pr\u00f3b\u00e1ljuk ki, hogy docker-compose-zal elindul-e az alkalmaz\u00e1sunk, \u00e9s tesztelj\u00fck az API GW-en kereszt\u00fcl a m\u0171k\u00f6d\u00e9st.","title":"El\u0151k\u00e9sz\u00fclet"},{"location":"DevOps/Logging-Health-Checks/#elk","text":"Vegy\u00fcnk fel a docker-compose.yml-be k\u00e9t \u00faj kont\u00e9nert az Elasticsearch-nek \u00e9s a Kibana-nak. Fontos, hogy ne az OSS verzi\u00f3t haszn\u00e1ljuk az ES-b\u0151l, mert abban nincsenek alap\u00e9rtelmezetten a Kibana-hoz sz\u00fcks\u00e9ges komponensek feltelep\u00edtve. logs : image : docker.elastic.co/elasticsearch/elasticsearch:7.4.2 container_name : logs environment : - cluster.name=logs # Settings to start Elasticsearch in a single-node development environment - node.name=logs - discovery.type=single-node - \"ES_JAVA_OPTS=-Xms256m -Xmx256m\" ports : - \"9202:9200\" volumes : - logs-elastic-data:/usr/share/elasticsearch/data networks : - todoapp-network kibana : image : docker.elastic.co/kibana/kibana:7.4.2 container_name : kibana environment : - ELASTICSEARCH_HOSTS=http://logs:9200 ports : - \"5602:5601\" depends_on : - logs networks : - todoapp-network A k\u00f6tetek k\u00f6z\u00e9 is fel kell venn\u00fcnk egy bejegyz\u00e9st. volumes : # The volumes will store the database data; kept even after the containers are deleted todoapp-mongo-data : driver : local todoapp-elastic-data : driver : local logs-elastic-data : driver : local","title":"ELK"},{"location":"DevOps/Logging-Health-Checks/#serilog","text":"Vegy\u00fck fel az al\u00e1bbi csomagokat a Todos.Api projektbe. <PackageReference Include= \"Serilog\" Version= \"2.9.0\" /> <PackageReference Include= \"Serilog.AspNetCore\" Version= \"3.2.0\" /> <PackageReference Include= \"Serilog.Exceptions\" Version= \"5.3.1\" /> <PackageReference Include= \"Serilog.Settings.Configuration\" Version= \"3.1.0\" /> <PackageReference Include= \"Serilog.Sinks.Elasticsearch\" Version= \"8.0.1\" /> A Program.cs-ben konfigur\u00e1ljuk be a Serilog-ot. public static IHostBuilder CreateWebHostBuilder ( string [] args ) => Host . CreateDefaultBuilder ( args ) . UseSerilog (( hostingContext , loggerConfiguration ) => loggerConfiguration . ReadFrom . Configuration ( hostingContext . Configuration ) . Enrich . FromLogContext () . Enrich . WithExceptionDetails () . WriteTo . Console () . WriteTo . Elasticsearch ( new ElasticsearchSinkOptions ( new Uri ( hostingContext . Configuration . GetValue < string >( \"LogsUrl\" ))) { AutoRegisterTemplate = true , })) . ConfigureWebHostDefaults ( c => { c . UseStartup < Startup >(); }); Hibakezel\u00e9s az indul\u00e1s k\u00f6zben A Serilog aj\u00e1nl\u00e1sok szerint a Main f\u00fcggv\u00e9nyben lenne \u00e9rdemes a Serilog-ot inicializ\u00e1lni, hogy az app indul\u00e1sa sor\u00e1n fell\u00e9p\u0151 kiv\u00e9teleket is le lehessen logolni. Most ett\u0151l az aspektust\u00f3l eltekint\u00fcnk az egyszer\u0171s\u00e9g kedv\u00e9\u00e9rt. A k\u00f3dr\u00e9szletb\u0151l l\u00e1thatjuk, hogy a Serilog-ot konfigur\u00e1lhatjuk az IConfiguration -b\u00f3l is, de ezt most nem fogjuk kihaszn\u00e1lni, \u00e9s itt inline adjuk meg az alap\u00e9rtelmez\u00e9seket. K\u00e9t Sink-et haszn\u00e1lunk most A konzolra \u00edrunk, mivel ez egy \u00e1ltal\u00e1nos elv\u00e1r\u00e1s a kont\u00e9neriz\u00e1lt alkalmaz\u00e1sok eset\u00e9ben Elasticsearchbe \u00edrjuk a logot. 2 Enrichert haszn\u00e1lunk Az enricher-ek olyan \u00e1ltal\u00e1nos komponensek, amik a logbejegyz\u00e9seket kontextusf\u00fcgg\u0151 inform\u00e1ci\u00f3kkal tudj\u00e1k kieg\u00e9sz\u00edteni. pl.: id\u0151b\u00e9lyeg, alkalmaz\u00e1s neve, g\u00e9p neve, sz\u00e1l azonos\u00edt\u00f3ja stb. Most egy \u00e1ltal\u00e1nos Enricher-t vesz\u00fcnk fel a FromLogContext szem\u00e9ly\u00e9ben. Exception-\u00f6k r\u00e9szletes adataival eg\u00e9sz\u00edtj\u00fck ki a logot a WithExceptionDetails -szel Fentebb is l\u00e1thatjuk, hogy az Elasticsearch URL-j\u00e9t a konfigur\u00e1ci\u00f3b\u00f3l nyerj\u00fck. Adjuk most ezt meg k\u00f6rnyezeti v\u00e1ltoz\u00f3k\u00e9nt a docker-compose.cs.debug.yml \u00e1llom\u00e1nyban. - ASPNETCORE_LogsUrl=http://logs:9200 A Serilog az appsetting.json \u00e9s az appsettings.Development.json-b\u0151l nem haszn\u00e1lja fel a Logging szekci\u00f3t. Ezeket az ig\u00e9nyess\u00e9g kedv\u00e9\u00e9rt t\u00f6r\u00f6lhetj\u00fck. Ezek csak a Microsoft-os ILogger implement\u00e1ci\u00f3 sz\u00e1m\u00e1ra kellenek. Logoljunk egy saj\u00e1t esem\u00e9nyt, most a p\u00e9lda kedv\u00e9\u00e9rt a TodosRepository-ban. K\u00e9rj\u00fcnk el egy ILogger<T> -t private readonly ILogger < TodosRepository > _logger ; public TodosRepository ( ElasticClient elasticClient , ILogger < TodosRepository > logger ) { this . elasticClient = elasticClient ; _logger = logger ; } Napl\u00f3zzuk info szinten a todo l\u00e9trehoz\u00e1s\u00e1t. public async Task < TodoItem > Insert ( CreateNewTodoRequest value ) { // This operation is ***NOT*** idempotent! var result = await elasticClient . IndexDocumentAsync ( value . ToDal ()); var todo = await FindById ( result . Id ); _logger . LogInformation ( \"Todo with data {@todoitem} for user:{userid} has been created\" , todo , todo . UserId ); return todo ; } Figyelj\u00fck meg, hogy nem haszn\u00e1ltuk a C# string interpol\u00e1ci\u00f3 funkci\u00f3j\u00e1t ( $\"{userid}\" )! Ez sz\u00e1nd\u00e9kos szemantikus napl\u00f3z\u00e1s eset\u00e9n! Ha szemantikus logol\u00e1st akarunk megval\u00f3s\u00edtani, akkor ne csak a log bejegyz\u00e9s sz\u00f6veg\u00e9ben gondolkodjunk, hanem minden kapcsol\u00f3d\u00f3 inform\u00e1ci\u00f3ban. A fenti esetben az \u00fagynevezett log message template term\u00e9szetesen ki lesz \u00e9rt\u00e9kelve \u00e9s be lesz helyettes\u00edtve a placeholderekre a param\u00e9terek, de \u00edgy a logger komponensnek lehet\u0151s\u00e9ge van ezeket a param\u00e9tereket nem csak a log \u00fczenetbe belerakni, hanem a mi eset\u00fcnkben az Elasticsearch-ben kereshet\u0151en elt\u00e1rolni. A template-ek eset\u00e9ben csak nem egyszer\u0171 ToString() h\u00edv\u00e1st lehet v\u00e9gezni, hanem a fenti p\u00e9ld\u00e1ban a {@todoitem} placeholder eset\u00e9ben a @ jelent\u00e9se az objektum soros\u00edt\u00e1s\u00e1ra vonatkozik. l\u00e1sd: https://github.com/serilog/serilog/wiki/Structured-Data Oda kell figyelni, hogy a template-ben l\u00e9v\u0151 propertyknek a JSON t\u00edpusa ( int, string, obj, array stb) els\u0151 besz\u00far\u00e1skor fix\u00e1lva lesznek az ES s\u00e9m\u00e1j\u00e1ban. Ha refaktor\u00e1ljuk a template-et \u00e9s m\u00e1st pr\u00f3b\u00e1lunk logolni, akkor szimpl\u00e1n nem fog besz\u00far\u00f3dni az ES-be.","title":"Serilog"},{"location":"DevOps/Logging-Health-Checks/#kibana","text":"Futtassuk az alkalmaz\u00e1sunkat \u00e9s gener\u00e1ljunk egy bejegyz\u00e9st a fenti funkci\u00f3val. Nyissuk meg a Kibana-t. A log n\u00e9zetben m\u00e9g nem l\u00e1tunk semmit. A kiban\u00e1nak mondjuk meg, hogy milyen index-en keresse a bejegyz\u00e9seket. Eset\u00fcnkben ez alap\u00e9rtelmezetten a logstash-* mint\u00e1ra fog illeszkedni. M\u00e1sodik l\u00e9p\u00e9sk\u00e9nt v\u00e1lasszuk ki a @timestamp mez\u0151t a sz\u0171r\u00e9shez. Vizsg\u00e1ljuk meg a logbejegyz\u00e9seket, kit\u00fcntetetten a saj\u00e1t TODO l\u00e9trehoz\u00e1s\u00e1val kapcsolatos bejegyz\u00e9st. Figyelj\u00fck meg, hogy szinte minden mez\u0151 kereshet\u0151 \u00e9s sz\u00e9pen struktur\u00e1ltan ker\u00fclnek a bejegyz\u00e9s adatai lement\u00e9sre.","title":"Kibana"},{"location":"DevOps/Logging-Health-Checks/#health-checks","text":"Implement\u00e1ljunk a Todos.Api projekt\u00fcnkh\u00f6z health check-et, amit majd a kubernetes fog els\u0151sorban felhaszn\u00e1lni.","title":"Health Checks"},{"location":"DevOps/Logging-Health-Checks/#elokeszulet_1","text":"Mivel Kubernetes-hez m\u00e9g nem konfigur\u00e1ltuk fel a loggol\u00e1s szolg\u00e1ltat\u00e1sait, ez\u00e9rt a jelenlegi munk\u00e1nkat commitoljuk egy k\u00fcl\u00f6n \u00e1gra, majd \u00e1lljunk vissza a kiindul\u00f3 \u00e1gra. git branch logging git commit -m \"napl\u00f3z\u00e1s k\u00e9sz\" git checkout kiindul\u00f3TODO El\u0151z\u0151 \u00f3r\u00e1k mint\u00e1j\u00e1ra \u00fczemelj\u00fck be a kubernetes verzi\u00f3j\u00e1t az alkalmaz\u00e1snak. N\u00e9zz\u00fck meg a dashboardon a rendszer \u00e1llapot\u00e1t \u00e9s pr\u00f3b\u00e1ljuk ki az alkalmaz\u00e1st.","title":"El\u0151k\u00e9sz\u00fclet"},{"location":"DevOps/Logging-Health-Checks/#health-check-implementacio","text":"K\u00e9sz\u00edts\u00fcnk readiness \u00e9s liveness pr\u00f3b\u00e1kat a kubernetes sz\u00e1m\u00e1ra. Ehhez haszn\u00e1ljuk fel az ASP.NET Core 2.2 \u00f3ta rendelkez\u00e9sre \u00e1ll\u00f3 be\u00e9p\u00edtett Health Check API-kat.","title":"Health Check implement\u00e1ci\u00f3"},{"location":"DevOps/Logging-Health-Checks/#liveness","text":"Kezdj\u00fck az egyszer\u0171bbel. Akkor lesz live egy szolg\u00e1ltat\u00e1s, ha az app fel\u00e1lt \u00e9s ki tudja szolg\u00e1lni k\u00fcls\u0151 f\u00fcgg\u0151s\u00e9gek n\u00e9lk\u00fcl a liveness pr\u00f3b\u00e1t. Ehhez vegy\u00fcnk fel egy \u00fcres health checket a /health/live v\u00e9gpontra. public void ConfigureServices ( IServiceCollection services ) { // ... services . AddHealthChecks () . AddCheck ( \"liveness\" , () => HealthCheckResult . Healthy ()); } public void Configure ( IApplicationBuilder app , IWebHostEnvironment env ) { //... app . UseEndpoints ( endpoints => { endpoints . MapControllers (); endpoints . MapHealthChecks ( \"/health/live\" , new HealthCheckOptions { Predicate = r => r . Name . Contains ( \"liveness\" ), ResponseWriter = UIResponseWriter . WriteHealthCheckUIResponse }); } A health check UI-hoz az al\u00e1bbi NuGet csomagot kell felvegy\u00fck a projektbe. <PackageReference Include= \"AspNetCore.HealthChecks.UI\" Version= \"3.0.4\" /> Mi most csak a soros\u00edt\u00f3 komponens\u00e9t fogjuk haszn\u00e1lni bel\u0151le ( UIResponseWriter ), id\u0151 hi\u00e1ny\u00e1ban a UI komponenst most nem \u00fczemelj\u00fck be. Pr\u00f3b\u00e1ljuk ki az \u00faj v\u00e9gpontot (F5).","title":"Liveness"},{"location":"DevOps/Logging-Health-Checks/#readiness","text":"Vegy\u00fcnk fel HC-t a k\u00fcls\u0151 szolg\u00e1ltat\u00e1sainkhoz is (Elasticsearch, Redis), majd ezt publik\u00e1ljuk ki egy k\u00fcl\u00f6n v\u00e9gponton. Vegy\u00fck fel az k\u00f6vetkez\u0151 csomagokat a Todos.Api projekthez. <PackageReference Include= \"AspNetCore.HealthChecks.Elasticsearch\" Version= \"3.0.0\" /> <PackageReference Include= \"AspNetCore.HealthChecks.Redis\" Version= \"2.2.1\" /> Verzi\u00f3 fontos AspNetCore.HealthChecks.Redis csomag direkt a 2.2.1-es mert \u00f6sszeakadna a Microsoft.Extensions.Caching.Redis csomaggal az \u00fajabb verzi\u00f3. Vegy\u00fck fel a csekkol\u00e1sokat. services . AddHealthChecks () . AddCheck ( \"liveness\" , () => HealthCheckResult . Healthy ()) . AddRedis ( Configuration . GetValue < string >( \"RedisUrl\" ) ?? \"redis:6379\" , tags : new [] { \"readiness\" }) . AddElasticsearch ( Configuration . GetValue < string >( \"ElasticsearchUrl\" ) ?? \"http://elasticsearch:9200\" , tags : new [] { \"readiness\" }); IOptions<T> haszn\u00e1lata Az ASP.NET Core-os konfigur\u00e1ci\u00f3k kezel\u00e9s\u00e9re itt is c\u00e9lszer\u0171bb lenne az IOptions<T> mint\u00e1t haszn\u00e1lni, de most az egyszer\u0171s\u00e9g kedv\u00e9\u00e9rt ett\u0151l eltekint\u00fcnk. Publik\u00e1ljuk ki egy v\u00e9gponton \u0151ket. endpoints . MapHealthChecks ( \"/health/ready\" , new HealthCheckOptions { Predicate = r => r . Tags . Contains ( \"readiness\" ), ResponseWriter = UIResponseWriter . WriteHealthCheckUIResponse }); Pr\u00f3b\u00e1ljuk ki!","title":"Readiness"},{"location":"DevOps/Logging-Health-Checks/#kubernetes-probes","text":"Vegy\u00fck fel a kubernetes konfigur\u00e1ci\u00f3ba a liveness \u00e9s a readiness pr\u00f3b\u00e1kat. spec : containers : - name : todos livenessProbe : httpGet : path : /health/live port : 80 scheme : HTTP initialDelaySeconds : 5 periodSeconds : 10 readinessProbe : httpGet : path : /health/ready port : 80 scheme : HTTP initialDelaySeconds : 5 periodSeconds : 10 Ha mi k\u00edv\u00fclr\u0151l is meg akarjuk h\u00edvni a /health v\u00e9gpontokat, akkor vegy\u00fck fel \u0151ket az ingress konfigur\u00e1ci\u00f3ba. spec : rules : - http : paths : - path : /health/live backend : serviceName : todos # A service neve servicePort : http # A service-ben a port neve (lehet a port szama is) - path : /health/ready backend : serviceName : todos # A service neve servicePort : http # A service-ben a port neve (lehet a port szama is) Pr\u00f3b\u00e1ljuk ki! Rontsuk el a readiness pr\u00f3b\u00e1t \u00fagy, hogy el\u00edrjuk az elasticsearch connection string-j\u00e9t a k\u00f6rnyezeti v\u00e1ltoz\u00f3ban. Azt tapasztalhatjuk, hogy az \u00faj kont\u00e9ner elindult, de mivel nem ready ez\u00e9rt a r\u00e9gi szerep\u00e9t nem tudja \u00e1tvenni addig, am\u00edg ready nem lesz. Sajnos ez a hiba nem tud kijavulni mag\u00e1t\u00f3l, \u00edgy a kijav\u00edtott config ut\u00e1n fog indulni a pod. K\u00e9sz\u00edts\u00fcnk egy egyszer\u0171 m\u00f3dszert a liveness pr\u00f3ba elront\u00e1s\u00e1ra a Startup oszt\u00e1lyban a healthcheck l\u00e9trehoz\u00e1sakor. public bool IsLive { get ; private set ; } = true ; services . AddHealthChecks () . AddCheck ( \"liveness\" , () => IsLive ? HealthCheckResult . Healthy () : HealthCheckResult . Unhealthy ()) endpoints . MapGet ( \"/health/switch\" , async r => { IsLive = ! IsLive ; await r . Response . WriteAsync ( $ \"IsLive is now {IsLive}\" ); }); Enged\u00e9lyezz\u00fck k\u00edv\u00fclr\u0151l ezt a v\u00e9gpontot is. - path : /health/switch backend : serviceName : todos # A service neve servicePort : http # A service-ben a port neve (lehet a port szama is) Telep\u00edts\u00fck ki, majd rontsuk el a m\u0171k\u00f6d\u00e9st a v\u00e9gpontunkkal. K\u00f6zben figyelj\u00fck a podok \u00e1llapot\u00e1t. Egy id\u0151 ut\u00e1n l\u00e1thatjuk, hogy a pr\u00f3ba s\u00e9r\u00fclt, \u00e9s a k8s megpr\u00f3b\u00e1lja \u00fajraind\u00edtani a pod-ot, mivel ott m\u00e1r az IsLive property \u00e9rteke igaz lesz. \u00c9rdemes minden health check defini\u00e1l\u00e1sa sor\u00e1n megtervezni azt, hogy az most melyik pr\u00f3b\u00e1ba illik bele jobban. Ha van es\u00e9ly, hogy mag\u00e1t\u00f3l megjavuljon, akkor a readiness pr\u00f3b\u00e1ba \u00e9rdemes rakni (pl. valamilyen k\u00fcls\u0151 szolg\u00e1ltat\u00e1s nem el\u00e9rhet\u0151, persze ez lehet konfigur\u00e1ci\u00f3s hiba is, ahogy l\u00e1ttuk), ha pedig \u00fajraind\u00edt\u00e1s tud seg\u00edteni akkor a liveness pr\u00f3b\u00e1ba rakjuk.","title":"Kubernetes probes"},{"location":"DevOps/Nyomkovetes/","text":"El\u0151ad\u00e1s \u00b6 Elosztott nyomk\u00f6vet\u00e9s C\u00e9l \u00b6 OpenTracing \u00e9s Jaeger alap\u00fa elosztott nyomk\u00f6vet\u00e9s eszk\u00f6zt\u00e1r\u00e1nak megismer\u00e9se OpenTracing \u00e9s Jaeger alap\u00fa nyomk\u00f6vet\u00e9s alapok .NET Core alap\u00fa mikroszolg\u00e1ltat\u00e1sok eset\u00e9n El\u0151k\u00f6vetelm\u00e9nyek \u00b6 Docker Desktop Visual Studio 2019 min v16.3 ASP.NET Core 3.0 SDK 1. Feladat - OpenTracing alap\u00fa nyomk\u00f6vet\u00e9s Jaeger k\u00f6rnyezetben \u00b6 Bevezet\u0151 \u00b6 A feladat keret\u00e9ben egy olyan mikroszolg\u00e1ltat\u00e1s alap\u00fa mintaalkalmaz\u00e1st vizsg\u00e1lunk meg, mely az OpenTracing haszn\u00e1lat\u00e1t illusztr\u00e1lja, Jaeger alap\u00fa backenddel \u00e9s megjelen\u00edt\u00e9ssel. A feladat a Take OpenTracing for a HotROD ride blogbejegyz\u00e9sen alapul, de n\u00e9h\u00e1ny olyan Jaeger szolg\u00e1ltat\u00e1st is megvizsg\u00e1lunk, melyet a cikk nem taglal. A mintaalkalmaz\u00e1s neve \"Hot R.O.D\". Go nyelven \u00edr\u00f3dott, k\u00f3dja OpenTracing instrument\u00e1lt. A forr\u00e1sk\u00f3dja a Jaeger GitHub repository p\u00e9ld\u00e1k k\u00f6z\u00f6tt tal\u00e1lhat\u00f3 meg: https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod . Itt tal\u00e1lunk le\u00edr\u00e1st arr\u00f3l, hogyan lehet a HotROD \u00e9s Jaeger alkalmaz\u00e1sokat docker alapokon futtatni: A Jeager futatt\u00e1s\u00e1ra a jaegertracing/all-in-one docker image-et haszn\u00e1ljuk: ez valamennyi Jaeger backend komponenst tartalmaz (agent, collector, ingester, stb.), bele\u00e9rtve az adatmegjelen\u00edt\u0151 frontendet is. Ez az image ismerked\u00e9shez, helyi k\u00f6rnyezetben tesztel\u00e9shez aj\u00e1nlott, production k\u00f6rnyezethez nem aj\u00e1nlj\u00e1k. B\u0151vebb le\u00edr\u00e1st az image-r\u0151l t\u00f6bbek k\u00f6z\u00f6tt itt tal\u00e1lhatunk: https://www.jaegertracing.io/docs/getting-started . A sz\u00e1mtalan portb\u00f3l sz\u00e1munkra kett\u0151 \u00e9rdekes, a 6831 (Jaeger agent, span-eket itt fogadja UDP-n a kliens k\u00f6nyvt\u00e1rakt\u00f3l) \u00e9s a 16686 (Jaeger UI frontend). Megjegyz\u00e9sek: A \"jaegertracing/all-in-one\" image alap\u00e9rtelmez\u00e9sben egy in-memory t\u00e1rol\u00f3t haszn\u00e1l, \u00edgy \u00fajraind\u00edt\u00e1st k\u00f6vet\u0151en elvesznek a kor\u00e1bban r\u00f6gz\u00edtett trace/span-ek. Le\u00edr\u00e1s a tov\u00e1bbi Jaeger image-ekr\u0151l \u00e9s konfigur\u00e1ci\u00f3s lehet\u0151s\u00e9gekr\u0151l itt tal\u00e1lgat\u00f3: https://www.jaegertracing.io/docs/deployment A HotROD futtat\u00e1s\u00e1ra a jaegertracing/example-hotrod docker image haszn\u00e1lhat\u00f3. Ez az image egyben tartalmazza valamennyi HotROD szolg\u00e1ltat\u00e1s k\u00f3dj\u00e1t, futatt\u00e1sakor minden sz\u00fcks\u00e9ges szolg\u00e1ltat\u00e1s elindul ugyanabban a kont\u00e9nerben, csak k\u00fcl\u00f6nb\u00f6z\u0151 portokon. A HotRod \u00e9s a Jaeger kont\u00e9nerek egyszerre t\u00f6rt\u00e9n\u0151 ind\u00edt\u00e1s\u00e1ra docker-compose-t haszn\u00e1lunk: T\u00f6lts\u00fck le a docker-compose.yml -t innen https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/docker-compose.yml , ide ments\u00fck: c:\\work\\<saj\u00e1tn\u00e9v> Nyissuk meg a f\u00e1jlt VS Code-ban, tekints\u00fck \u00e1t a tartalm\u00e1t A jaeger szolg\u00e1ltat\u00e1s a 6831 (Jaeger agent spanfelt\u00f6lt\u0151) \u00e9s a 16686 (Jaeger UI frontend) portokat mappeli. A hotrod szolg\u00e1ltat\u00e1s sz\u00e1m\u00e1ra k\u00f6rnyezeti v\u00e1ltoz\u00f3kban mondjuk meg, milyen hostn\u00e9ven \u00e9s milyen porton \u00e9ri az a Jaeger agentet (span-ek fel\u00f6lt\u00e9s\u00e9hez sz\u00fcks\u00e9ges). Ind\u00edtsunk \u00faj command promptot, navig\u00e1ljunk az els\u0151 l\u00e9p\u00e9sben let\u00f6lt\u00f6tt docker-compose.yml (c:\\work\\ ) mapp\u00e1ba, majd ind\u00edtsuk a kont\u00e9nereket: docker-compose up Ha nem indul a HotRod a 8080-as port \u00fctk\u00f6z\u00e9s miatt, akkor: docker-compose down -nal \u00e1ll\u00edtsuk le a kont\u00e9nereket A docker-compose.yml -ben mappelj\u00fcnk m\u00e1s k\u00fcls\u0151 portra, pl.: 8090:8080 Ind\u00edtsuk \u00fajra docker-compose up A HotRod frontend a http://localhost:8080 , a Jaeger frontend a http://localhost:16686 c\u00edmen \u00e9rhet\u0151 el. K\u00e9s\u0151bb a szolg\u00e1ltat\u00e1sok le\u00e1ll\u00edt\u00e1sa a k\u00f6vetkez\u0151 paranccsal lesz lehets\u00e9ges (most m\u00e9g ne futtassuk): docker-compose down Ismerked\u00e9s a HotROD alkalmaz\u00e1ssal \u00b6 A HotRod egy \"Ride Sharing\" alkalmaz\u00e1s. Uber-hez hasonl\u00f3, szem\u00e9lyek vagy egy\u00e9b k\u00e9zbes\u00edtend\u0151 t\u00e1rgyak j\u00e1rm\u0171vekkel t\u00f6rt\u00e9n\u0151 c\u00e9lba juttat\u00e1s\u00e1hoz. Egy Go alkalmaz\u00e1s, t\u00f6bb szolg\u00e1ltat\u00e1sb\u00f3l \u00e1ll. A docker image az \"all\" param\u00e9terrel futtatja az alkalmaz\u00e1st, mely valamennyi szolg\u00e1ltat\u00e1s\u00e1t elind\u00edtja. N\u00e9zz\u00fcnk r\u00e1 a command promptban a docker-compose up \u00e1ltal megjelen\u00edtett logokra, l\u00e1tszik, hogy n\u00e9gy szolg\u00e1ltat\u00e1sb\u00f3l \u00e1ll, melyek a 8080-8083 portokon \u00e9rhet\u0151k el: frontend customer driver route Kapcsol\u00f3djuk a HotROD frontendhez b\u00f6ng\u00e9sz\u0151b\u0151l, ha m\u00e9g nem tett\u00fck meg ( http://localhost:8080 ). Ismerkedj\u00fcnk meg a fel\u00fclet m\u0171k\u00f6d\u00e9s\u00e9vel: Egy gomb megnyom\u00e1s\u00e1val az alkalmaz\u00e1s egy adott \u00fcgyf\u00e9lhez (aki egy adott helyen tart\u00f3zkodik) keres egy a k\u00f6zel\u00e9ben tart\u00f3zkod\u00f3 j\u00e1rm\u0171vet, pl. egy csomag felv\u00e9tel\u00e9hez vagy egy szem\u00e9ly felv\u00e9tel\u00e9hez. Bal fels\u0151 sarokban egy a JavaScript frontend \u00e1ltal gener\u00e1lt v\u00e9letlen session azonos\u00edt\u00f3t l\u00e1tunk (F5-re m\u00e1s lesz). Gombnyom\u00e1sra \u00faj keres\u00e9st ind\u00edt, mely kapcs\u00e1n egy \u00faj sorban a k\u00f6vetkez\u0151 adatokat napl\u00f3zza a fel\u00fclet: A keres\u00e9s sor\u00e1n a k\u00e9r\u00e9shez rendelt j\u00e1rm\u0171 rendsz\u00e1ma \u00e9s v\u00e1rhat\u00f3 \u00e9rkez\u00e9si ideje K\u00e9r\u00e9s azonos\u00edt\u00f3 (session id + sorsz\u00e1m) Kiszolg\u00e1l\u00e1si id\u0151, a JavaScript k\u00f6nyvt\u00e1r m\u00e9ri Architekt\u00fara, szolg\u00e1ltat\u00e1sf\u00fcgg\u0151s\u00e9gek felder\u00edt\u00e9se \u00b6 L\u00e9p\u00e9sek: Kattintsunk valamelyik gombon a HotROD frontenden (ha m\u00e9g nem tett\u00fck meg) A Jaeger frontenden Dependencies men\u00fc, majd DAG tab kiv\u00e1laszt\u00e1sa A Jeager a h\u00edv\u00e1sok sor\u00e1n begy\u0171jt\u00f6tt trace/span adatok alapj\u00e1n a k\u00f6vetkez\u0151ket jelen\u00edti meg: a szolg\u00e1ltat\u00e1sokat a szolg\u00e1ltatat\u00e1sok k\u00f6z\u00f6tti f\u00fcgg\u0151s\u00e9geket a szolg\u00e1ltat\u00e1sok k\u00f6z\u00f6tti h\u00edv\u00e1sok sz\u00e1m\u00e1t Az \u00e1bra az eg\u00e9rrel pan-elhet\u0151, illetve az eg\u00e9rg\u00f6rg\u0151vel nagy\u00edthat\u00f3 (azon pontra vonatkoz\u00f3an, ahol az eg\u00e9r \u00e9ppen \u00e1ll). Force Directed Graph tab: hasonl\u00f3 az el\u0151z\u0151h\u00f6z, az eg\u00e9rrel az adott csom\u00f3ponra \u00e1llva az adott csom\u00f3pont f\u00fcgg\u0151s\u00e9geit emeli ki. L\u00e1tjuk, hogy b\u00e1r a HotROD egyetlen kont\u00e9nerben fut, hat szolg\u00e1ltat\u00e1sb\u00f3l \u00e1ll. Megjegyz\u00e9s: a Mysql \u00e9s a Redis nem val\u00f3s szolg\u00e1ltat\u00e1sok, a HotROD ezeket csak \"szimul\u00e1lja\", ezek nem val\u00f3di adatkezel\u0151k. Trace-ek \u00e9s spanek megjelen\u00edt\u00e9se \u00b6 A Jaeger frontenden v\u00e1laszuk ki a \"Search\" men\u00fct, itt van lehet\u0151s\u00e9g\u00fcnk a Jaeger \u00e1ltal r\u00f6gz\u00edtett trace-ek list\u00e1z\u00e1s\u00e1ra a megadott sz\u0171r\u00e9si felt\u00e9teleknek megfelel\u0151en. L\u00e9p\u00e9sek: A HotRod frontenden kattintsunk m\u00e9g egyszer-k\u00e9tszer a gombokon, hogy t\u00f6bb, mint egy trace-\u00fcnk legyen a rendszerben. A Jaeger frontend sz\u0171r\u0151panelj\u00e9n a \"Service\" dropdown-ban v\u00e1lasszuk ki a \"Frontend\" elemet. Ha nincs a list\u00e1ban, akkor az F5 gombban friss\u00edts\u00fck a b\u00f6ng\u00e9sz\u0151 oldal\u00e1t. Kattintsunk a \"Find traces\" gombbon A jobb oldali panel fels\u0151 r\u00e9sz\u00e9n az x id\u0151tengely ment\u00e9n a trace-ek jelennek meg (az y tengely a v\u00e9grehajt\u00e1si id\u0151). Minden kliens oldali k\u00e9r\u00e9shez k\u00fcl\u00f6n trace sz\u00fcletik. Itt is lehet\u0151s\u00e9g\u00fcnk van az adott k\u00f6rre kattintva a trace r\u00e9szleteinek megjelen\u00edt\u00e9s\u00e9re. A diagram alatt a trace-ek list\u00e1s n\u00e9zete jelenik meg, minden trace egy k\u00fcl\u00f6n sor , sz\u00e1mos hasznos inform\u00e1ci\u00f3val (t\u00f6bbek k\u00f6z\u00f6tt a trace-ben lev\u0151 span-ek sz\u00e1ma, ugyenez szolg\u00e1ltat\u00e1sonk\u00e9nti lebont\u00e1sban, hibaesem\u00e9nyek sz\u00e1ma, stb.) Lehet\u0151s\u00e9g van a t\u00e9telek sorrendez\u00e9s\u00e9re (pl. v\u00e9grehajt\u00e1si id\u0151 szerint is hasznos lehet). Kattintsunk az egyik trace-re a r\u00e9szletes n\u00e9zet\u00e9nek megjelen\u00edt\u00e9s\u00e9hez. Trace r\u00e9szletes n\u00e9zet: A trace spanjeinek egym\u00e1sba \u00e1gyaz\u00e1si hierarchi\u00e1j\u00e1t l\u00e1tjuk a v\u00edzszintes id\u0151tengely ment\u00e9n. Egy span hossza ar\u00e1nyos a v\u00e9grehajt\u00e1si idej\u00e9vel. Minden span k\u00fcl\u00f6n sor, a legfels\u0151 sor a root span: a b\u00f6ng\u00e9sz\u0151b\u0151l kiadott JavaScript k\u00e9r\u00e9st fogad\u00f3 frontend szolg\u00e1ltat\u00e1shoz tartozik, a m\u0171velet neve HTTP GET/dispatch , el\u0151sz\u00f6r a customer szolg\u00e1ltat\u00e1st h\u00edvja, mely fogadja a k\u00e9r\u00e9st \u00e9s a mysql szolg\u00e1ltat\u00e1sba h\u00edv tov\u00e1bb. Sokat seg\u00edt az \u00e1tl\u00e1that\u00f3s\u00e1gban a sz\u00ednk\u00f3dol\u00e1s : minden szolg\u00e1ltat\u00e1s adott sz\u00ednet kap, az \u00f6sszes spanje ugyanolyan sz\u00ednnel jelenik meg! A spanek alapj\u00e1n l\u00e1tjuk, hogy az alkalmaz\u00e1s hogyan szolg\u00e1l ki egy k\u00e9r\u00e9st: A frontend szolg\u00e1ltat\u00e1s a k\u00fcls\u0151 HTTP GET k\u00e9r\u00e9st a /dispatch v\u00e9gpontj\u00e1n\u00e1l kapja meg. A frontend szolg\u00e1ltat\u00e1s HTTP GET k\u00e9r\u00e9st k\u00fcld az customer szolg\u00e1ltat\u00e1s customer v\u00e9gpontj\u00e1nak (ha \u00f6sszecsukjuk a sz\u00fcl\u0151 span sor\u00e1t, akkor ezt sor elej\u00e9n a frontend -> customer ny\u00edl is mutatja). Az customer v\u00e9grehajt egy SQL SELECT utas\u00edt\u00e1st a MySQL h\u00edv\u00e1s\u00e1val. Az eredm\u00e9nyek visszaker\u00fclnek a frontend szolg\u00e1ltat\u00e1shoz. Ezut\u00e1n a frontend szolg\u00e1ltat\u00e1s RPC k\u00e9r\u00e9st ind\u00edt ( Driver :: findNearest m\u0171velet) a driver szolg\u00e1ltat\u00e1shoz. An\u00e9lk\u00fcl, hogy alaposabban belem\u00e9lyedn\u00e9nk a nyomk\u00f6vet\u00e9si r\u00e9szletekbe, nem tudjuk megmondani, hogy mely RPC keretrendszert haszn\u00e1lj\u00e1k a k\u00e9r\u00e9sek, de feltehetj\u00fck, hogy nem HTTP (val\u00f3j\u00e1ban TChannel \u00fatj\u00e1n k\u00e9sz\u00fcl). A driver szolg\u00e1ltat\u00e1s sz\u00e1mos h\u00edv\u00e1st kezdem\u00e9nyez a Redis fel\u00e9. A h\u00edv\u00e1sok k\u00f6z\u00fcl n\u00e9h\u00e1ny piros felki\u00e1lt\u00f3jel ikonnal van megjel\u00f6lve, jelezve a hib\u00e1kat. Ezut\u00e1n a frontend szolg\u00e1ltat\u00e1s egy sor HTTP GET k\u00e9r\u00e9st int\u00e9z a route szolg\u00e1ltat\u00e1s route v\u00e9gpontj\u00e1hoz. V\u00e9g\u00fcl, a frontend szolg\u00e1ltat\u00e1s visszaadja az eredm\u00e9nyt a k\u00fcls\u0151 h\u00edv\u00f3nak. Megjegyz\u00e9s: az oldal tetej\u00e9n lev\u0151 id\u0151ablakban eg\u00e9rrel egy id\u0151szeletet kijel\u00f6lve az adott id\u0151szeletre \"nagy\u00edthatunk\" r\u00e1. Span r\u00e9szletes n\u00e9zet: Egy sorra kattintva az adott span, \"kiny\u00edlik\", magass\u00e1ga megn\u0151, tov\u00e1bbi adatokat mutatva a spanr\u0151l. Nyissunk le az els\u0151 spant: l\u00e1tjuk a hozz\u00e1rendelt Tag -eket. Ezek kulcs-\u00e9rt\u00e9k p\u00e1rok a spanhez r\u00f6gz\u00edtveve (b\u00e1rmilyen kulcs-\u00e9rt\u00e9k p\u00e1r lehet). Pl. k\u00fcl\u00f6n\u00f6sen informat\u00edv a http.url , http.method , \u00e9s a http.status_code . span.kind : ha a \"h\u00edvott\" oldalon vagyunk, akkor a \"server\" \u00e9rt\u00e9kkel ker\u00fcl r\u00f6gz\u00edt\u00e9sre. A h\u00edv\u00f3 oldalon \"client\" \u00e9rt\u00e9kkel r\u00f6gz\u00edtett. sampler.type : \"const\" - jelzi, hogy minden m\u0171velet/span r\u00f6gz\u00edtend\u0151, a mintav\u00e9telez\u00e9s sor\u00e1n nem doband\u00f3 el semmi. A mysql lek\u00e9rdez\u00e9shez tartoz\u00f3 span (5. sor) tag-ben tartalmazza az SQL parancsot. Nyissunk le egy piros felki\u00e1llt\u00f3jellel dekor\u00e1lt spant. A tag-ek k\u00f6z\u00f6tt szerepel az error = true : ez a tag haszn\u00e1land\u00f3 hib\u00e1k jelz\u00e9s\u00e9re. B\u00e1rmilyen egy\u00e9ni adatot hozz\u00e1f\u0171zhet\u00fcnk tag-k\u00e9nt a span-ekhez. Erre p\u00e9lda a redis FindDriverIDs eset\u00e9n a param.location = 728,326 , vagy Redis GetDriver eset\u00e9n a param.driverID=T707027C . A \"szabv\u00e1nyos\" span tag-ekr\u0151l, illetve a r\u00f6videsen t\u00e1rgyal\u00e1sra ker\u00fcl\u0151 log \"szabv\u00e1nyos\" mez\u0151ir\u0151l itt tal\u00e1lunk le\u00edr\u00e1st: https://github.com/opentracing/specification/blob/master/semantic_conventions.md#span-tags-table (nyissuk meg az oldalt \u00e9s n\u00e9zz\u00fcnk r\u00e1 a t\u00e1bl\u00e1zatra). Az OpenTracing nem k\u00f6ti ki a tag neveket, de mindenk\u00e9ppen c\u00e9lszer\u0171 a t\u00e1bl\u00e1zat aj\u00e1nl\u00e1sait k\u00f6vetni: ha nem tessz\u00fck, a trace/span megjelen\u00edt\u0151 eszk\u00f6z\u00f6k nem tudnak szemantik\u00e1t t\u00e1rs\u00edtani hozz\u00e1 (pl. error eset\u00e9n speci\u00e1lis megjelent\u00e9s). Megjegyz\u00e9s: ha megn\u00e9zz\u00fck, a mysql \u00e9s redis spanek kliens odaliak (span.kind = client), ezek nem az adatt\u00e1rol\u00f3b\u00f3l erednek. A dem\u00f3alkalmaz\u00e1sban az adatkezel\u0151k szimul\u00e1ltak, de jellemz\u0151en a val\u00f3s adatt\u00e1rol\u00f3k sem instrument\u00e1ltak: ez esetben t\u00e1rol\u00f3hoz val\u00f3 hozz\u00e1f\u00e9r\u00e9st kliens oldalon vegy\u00fck k\u00f6rbe egy \u00faj spannel, \u00e9s ehhez csapjuk hozz\u00e1 az informat\u00edv tag-eket (pl. SQL parancs, Redis m\u0171velet \u00e9s param\u00e9terek, stb.). A spanekhez Log bejegyz\u00e9sek is tartozhatnak. Keress\u00fcnk meg p\u00e1rat: A span r\u00e9szletes n\u00e9zet\u00e9ben a Logs szekci\u00f3 alatt tal\u00e1lhat\u00f3k. A span cs\u00edkj\u00e1n is megjelennek a logbejegyz\u00e9sek v\u00e9kony f\u00fcgg\u0151leges vonallal, az egeret f\u00f6l\u00e9 h\u00fazva tooltipben r\u00e9szletes inform\u00e1ci\u00f3t kapunk az adott logbejegyz\u00e9sr\u0151l. A Log bejegyz\u00e9seknek van id\u0151b\u00e9lyege, plusz tetsz\u0151leges, debugol\u00e1st seg\u00edt\u0151 kulcs-\u00e9rt\u00e9k p\u00e1rok tartozhatnak hozz\u00e1. A \"szabv\u00e1nyosakat\" itt tekinthetj\u00fck meg: https://github.com/opentracing/specification/blob/master/semantic_conventions.md#log-fields-table (n\u00e9zz\u00fcnk r\u00e1 a t\u00e1bl\u00e1zatra). A legfontosabb az event kulcs. N\u00e9zz\u00fck meg az egyik hib\u00e1s redis GetDriver m\u0171velet logj\u00e1t. L\u00e1tjuk, hogy redis timeout t\u00f6rt\u00e9nt, a driver_id is napl\u00f3z\u00e1sra ker\u00fclt. Kontextusba helyezett logok \u00b6 Eg\u00e9sz j\u00f3l l\u00e1tjuk, hogyan \u00e9p\u00fcl fel az alkalmaz\u00e1s. Tov\u00e1bbi k\u00e9rd\u00e9sekre keress\u00fck a v\u00e1laszt, pl.: mi\u00e9rt h\u00edvja a frontend a customer szolg\u00e1ltat\u00e1s /customer v\u00e9gpontj\u00e1t? Pr\u00f3b\u00e1lhatn\u00e1nk a szolg\u00e1ltat\u00e1sok logjaib\u00f3l megtudni: Ez sokszor nagyon neh\u00e9z Ha sok felhaszn\u00e1l\u00f3i k\u00e9r\u00e9s kiszolg\u00e1l\u00e1sa t\u00f6rt\u00e9nik egyszerre p\u00e1rhuzamosan, szinte lehetetlen kih\u00e1mozni, mi tartozik egy adott felhaszn\u00e1l\u00f3i k\u00e9r\u00e9shez. Helyette n\u00e9zz\u00fck a trace renszer \u00e1ltal begy\u0171jt\u00f6tt (span-ekhez kapcsol\u00f3d\u00f3) logbejegyz\u00e9seket. N\u00e9zz\u00fck meg a HTTP GET /dispatch spanhez kapcsol\u00f3d\u00f3 logokat (18 bejegyz\u00e9s lesz). A t\u00f6bbi k\u00e9r\u00e9st\u0151l izol\u00e1ltan, j\u00f3l \u00e1ttekinthet\u0151 m\u00f3don, az adott trace \u00e9s span kontextus\u00e1ba helyezve l\u00e1tjuk a logbejegyz\u00e9seket! Megjegyz\u00e9s: Valami furcsa okb\u00f3l kifoly\u00f3an a mintaalkalmaz\u00e1s t\u00f6bb helyen is el\u00e9g szeg\u00e9nyesen napl\u00f3zza az inform\u00e1ci\u00f3kat. Pl. a Found customer nem \u00edrja ki, milyen adatok ker\u00fcltek lek\u00e9rdez\u00e9sre (customer koordin\u00e1t\u00e1k): pedig itt pont seg\u00edten\u00e9 a meg\u00e9rt\u00e9st, mert ezen koordin\u00e1t\u00e1k \u00e1ltal meghat\u00e1rozott ponthoz k\u00e9pest keresi a k\u00f6zelben lev\u0151 vezet\u0151ket a FindNearestDriver m\u0171veletben. Span Tasgs vs. Logs \u00b6 Mikor haszn\u00e1ljunk tageket, \u00e9s mikor logokat? Alapelv: A tag olyan inform\u00e1ci\u00f3 r\u00f6gz\u00edt\u00e9s\u00e9re val\u00f3, mely a span eg\u00e9sz\u00e9hez tartozik. A log id\u0151b\u00e9lyeggel rendelkez\u0151 esem\u00e9nyek r\u00f6gz\u00edt\u00e9s\u00e9re val\u00f3. K\u00e9sleltet\u00e9sek okainak felder\u00edt\u00e9se \u00b6 Vizsg\u00e1ljuk meg az alkalmaz\u00e1s teljes\u00edtm\u00e9ny karakterisztik\u00e1it. Trace-ek alap\u00e1n ezt l\u00e1tjuk: Az \u00fcgyf\u00e9ladatok lek\u00e9rdez\u00e9se ( customer szolg\u00e1ltat\u00e1s) a kritikus \u00fatvonalon van, mert ez adja vissza az \u00fcgyf\u00e9l koordin\u00e1t\u00e1it. A driver szolg\u00e1ltat\u00e1s lek\u00e9rdezi az \u00fcgyf\u00e9l k\u00f6zel\u00e9ben lev\u0151 10 j\u00e1rm\u0171vezet\u0151t, majd egyes\u00e9vel SORBAN EGYM\u00c1S UT\u00c1N lek\u00e9rdezi ezek adatait a Redist\u0151l: ezt mutatja a redis GetDriver l\u00e9pcs\u0151zetes mint\u00e1ja. Ezt k\u00f6vet\u0151en a 10 \u00fatvonalsz\u00e1m\u00edt\u00e1s ( route szolg\u00e1ltat\u00e1s) m\u0171velete nem is szekvenci\u00e1lis \u00e9s nem is teljesen p\u00e1rhuzamos. Azt l\u00e1tjuk, hogy maximum h\u00e1rom k\u00e9r\u00e9s tud p\u00e1rhuzamosan futni, \u00e9s amint ezekb\u0151l egy v\u00e9get \u00e9r, akkor indul a k\u00f6vetkez\u0151. Ez arra utal, hogy itt egy h\u00e1rmas v\u00e9grehajt\u00f3 pool futtatja a m\u0171veleteket. Vizsg\u00e1ljuk meg, mi t\u00f6rt\u00e9nik, ha sz\u00e1mos p\u00e1rhuzamos k\u00e9r\u00e9st futtatunk: A HotROD alkalmaz\u00e1sban gyorsan kattintsunk kb. 20-szor valamelyik gombon. A HotROD alkalmaz\u00e1s fel\u00fclet\u00e9n is j\u00f3l l\u00e1that\u00f3, hogy a k\u00e9r\u00e9sek kiszolg\u00e1l\u00e1sa belassul, a latency 800 ms helyett 2000 ms k\u00f6rny\u00e9ke lesz. A Jaeger alkalmaz\u00e1sban navig\u00e1ljunk vissza a nyit\u00f3oldalra, friss\u00edts\u00fck a trace list\u00e1t (Find traces gomb), keress\u00fck ki \u00e9s nyissuk meg az egyik leghosszabb lefut\u00e1s\u00fa trace-t: a trace list\u00e1ban ehhez tartozik a leghosszabb ci\u00e1nk\u00e9k sz\u00edn\u0171 s\u00e1v. A trace-t m\u00e1sk\u00e9nt is kikereshetj\u00fck: a HotROD oldal logj\u00e1ban l\u00e1tszik, hogy mely j\u00e1rm\u0171vezet\u0151t rendelte a rendszer a legnagyobb duration param\u00e9ter\u0171 sorn\u00e1l a k\u00e9r\u00e9shez (T795664C vagy egy hasonl\u00f3 form\u00e1tum\u00fa sztring). A Jaeger nyit\u00f3oldal\u00e1n a sz\u0171r\u0151paneleben a \"Tags\" mez\u0151be \u00edrjuk be ezt: \"driver:T716217C\" \u00e9s aktiv\u00e1ljuk a sz\u0171r\u00e9st. Az\u00e9rt tal\u00e1lja meg a trace-t, mert az egyik span egyik logj\u00e1ban szerepel a driver:T716217C kulcs-\u00e9rt\u00e9k p\u00e1r. (Ha meg akarjuk n\u00e9zni: a legels\u0151 spant lenyitva a spanhez tartoz\u00f3 utols\u00f3 log tartalmazza). Vizsg\u00e1ljuk meg a trace-t: a legszembe\u00f6tl\u0151bb k\u00fcl\u00f6nbs\u00e9g a kor\u00e1bbi, gyors trace-ekhez k\u00e9pest, a mysql k\u00e9r\u00e9s lass\u00fa kiszolg\u00e1l\u00e1sa. Pr\u00f3b\u00e1ljuk meg kital\u00e1lni, mi\u00e9rt: Nyissuk le a mysql SQL SELECT spant. K\u00e9t logbejegyz\u00e9s tartozik hozz\u00e1. Ezekb\u0151l egy\u00e9rtelm\u0171en kider\u00fcl, hogy az id\u0151 nagy r\u00e9sz\u00e9t lock-ra v\u00e1rakoz\u00e1ssal t\u00f6lt\u00f6te a k\u00e9r\u00e9s. Az els\u0151 log-b\u00f3l m\u00e9g az is kider\u00fcl, mely k\u00e9r\u00e9sek tartott\u00e1k fel (Waiting for lock behind 4 transactions\"), ezeket meg is neves\u00edti, pl. : [8221-10 8221-11 8221-12 8221-13] . Ezek a k\u00e9r\u00e9s azonos\u00edt\u00f3k a HotROD frontendr\u0151l j\u00f6nnek, n\u00e9zz\u00fck meg \u0151ket a fel\u00fcleten! Ami itt igaz\u00e1n izgalmas: honnan tudja a mysql \"kliens\", hogy mik a k\u00e9r\u00e9s azonos\u00edt\u00f3k, hogy jut el hozz\u00e1? Param\u00e9terben NEM passzolja v\u00e9gig a rendszer a h\u00edv\u00e1si l\u00e1ncon (pl. a HTTP GET customer k\u00e9r\u00e9sek param\u00e9ter\u00e9ben sem szerepel, megn\u00e9zhetj\u00fck a span-eket). A \"var\u00e1zslat\" m\u00f6g\u00f6tt az OpenTracing baggage koncepci\u00f3ja \u00e9s mechanizmusa \u00e1ll. Maga az elosztott trace az\u00e9rt tud megval\u00f3sulni, mert a OpenTracing instrument\u00e1l\u00e1s gondoskodik arr\u00f3l, hogy a k\u00e9r\u00e9sekhez kapcsol\u00f3d\u00f3 bizonyos metaadatok sz\u00e1lak, folyamatok \u00e9s sz\u00e1m\u00edt\u00f3g\u00e9pek k\u00f6z\u00f6tt propag\u00e1l\u00f3djanak \u00e9s minden a k\u00e9r\u00e9s kiszolg\u00e1l\u00e1s\u00e1ban r\u00e9szt vev\u0151 szerepl\u0151h\u00f6z eljussanak. Ilyen a trace id \u00e9s a span id is. Egy m\u00e1sik ilyen metainform\u00e1ci\u00f3 a baggage. Ez egy \u00e1ltal\u00e1nos kulcs-\u00e9rt\u00e9k p\u00e1r t\u00e1rol\u00f3. A p\u00e9ld\u00e1nkban a JavaScript UI beteszi a k\u00e9r\u00e9s azonos\u00edt\u00f3t a baggagebe, \u00e9s ez a k\u00e9r\u00e9s kiszolg\u00e1l\u00e1sa sor\u00e1n minden szerepl\u0151h\u00f6z eljut, an\u00e9lk\u00fcl, hogy explicit param\u00e9terekben kellene kezelni. Az OpenTracing instrument\u00e1ci\u00f3 gondoskodik r\u00f3la, pl. HTTP fejl\u00e9cbe teszi a HTTP k\u00e9r\u00e9sek kiszolg\u00e1l\u00e1sa sor\u00e1n). Nagyon hat\u00e9kony \u00e9s hasznos eszk\u00f6z! A p\u00e9ld\u00e1nkban lehet\u0151v\u00e9 teszi, hogy a k\u00e9r\u00e9sazonos\u00edt\u00f3 alapj\u00e1n kikeress\u00fck \u00e9s analiz\u00e1ljuk azokat a trace-eket, melyek feltartj\u00e1k a k\u00e9r\u00e9s\u00fcnk kiszolg\u00e1l\u00e1s\u00e1t. A val\u00f3 \u00e9letben is gyakori probl\u00e9ma: egy \u00fcgyf\u00e9l k\u00e9r\u00e9s feltart/belass\u00edt sz\u00e1mos m\u00e1sikat. Lehet\u0151s\u00e9g\u00fcnk van ezen k\u00e9r\u00e9sek megtal\u00e1l\u00e1s\u00e1ra \u00e9s analiz\u00e1l\u00e1s\u00e1ra. A HotRod p\u00e9ld\u00e1ban nincs igazi Mysql adatb\u00e1zis, csak szimul\u00e1lja a rendszer. Itt l\u00e1that\u00f3 a mesters\u00e9gesen szigor\u00edtott z\u00e1rol\u00e1s (pl. egy rosszul konfigur\u00e1lt DB connection poolt szimul\u00e1lva): https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/services/customer/database.go , ezen bel\u00fcl az if !config.MySQLMutexDisabled sor k\u00f6rny\u00e9k\u00e9t n\u00e9zz\u00fck. Szimul\u00e1ljuk a detekt\u00e1lt probl\u00e9ma jav\u00edt\u00e1s\u00e1t. A forr\u00e1sk\u00f3dhoz nem fogunk ny\u00falni, a HotROD alkalmaz\u00e1st kell speci\u00e1lis command line argumentummal futtatni ahhoz, hogy kikapcsoljuk a mesters\u00e9gesen induk\u00e1lt szigor\u00fa z\u00e1rol\u00e1st, illetve ezen fel\u00fcl a mesters\u00e9ges k\u00e9sleltet\u00e9s m\u00e9rt\u00e9k\u00e9t is cs\u00f6kkenteni fogjuk: A docker-compose.yml-ben a hotrod szolg\u00e1ltat\u00e1s command line param\u00e9tereit b\u0151v\u00edts\u00fck a \"-M\" kapcsol\u00f3val, ehhez egy sort kell m\u00f3dos\u00edtani, a command: ut\u00e1n [] k\u00f6z\u00f6tt ez kell \u00e1lljon: \"-M\", \"-D\", \"100ms\", \"all\" Ments\u00fck el a f\u00e1jlt. Tegy\u00fck fel, hogy production k\u00f6rnyezetben vagyuk, csak a m\u00f3dosult szolg\u00e1ltat\u00e1st (eset\u00fcnkben HotROD) k\u00edv\u00e1njuk \u00fajraind\u00edtani, a t\u00f6bbit (eset\u00fcnkben Jaeger) nem. \u00cdgy nem adjuk ki a docker compose down parancsot! Egy \u00faj command line ablakban navig\u00e1ljunk el a docker-compose.yml-t tartalmaz\u00f3 mapp\u00e1ba ( c:\\work\\<saj\u00e1tn\u00e9v> ). Futtassuk a k\u00f6vetkez\u0151 parancsot: docker-compose up -d . A '-d h\u00e1tt\u00e9rben futtat, az up parancs pedig csak a m\u00f3dosult kont\u00e9nereket \u00e1ll\u00edtja le \u00e9s ind\u00edtja \u00fajra. A kimeneten ez j\u00f3l k\u00f6vethet\u0151. K\u00f6zben a kor\u00e1bbi, a kont\u00e9nerekre m\u00e9g mindig r\u00e1csatolt command promptunkban l\u00e1tszik, hogy \u00fajraindult a HotROD szolg\u00e1ltat\u00e1s, 'fix: disabling db connection mutex' \u00fczemm\u00f3dban. Tesztelj\u00fck a jav\u00edtott megold\u00e1st: gener\u00e1ljunk kb. 20 p\u00e1rhuzamos k\u00e9r\u00e9st, ellen\u0151rizz\u00fck a duration-t: egyik sem megy 2000ms k\u00f6rny\u00e9k\u00e9re (kb. 900 ms k\u00f6rny\u00e9k\u00e9re k\u00faszik csak fel), \u00e9s a trace-ekben l\u00e1tsz\u00f3dik, hogy a mysql span-ek hossza 100 ms k\u00f6r\u00fcl marad. Tov\u00e1bbi teljes\u00edtm\u00e9ny optimaliz\u00e1ci\u00f3 \u00b6 Azt l\u00e1tjuk, hogy m\u00edg kor\u00e1bban a route szolg\u00e1ltat\u00e1s h\u00edv\u00e1sok k\u00f6z\u00fcl h\u00e1rom futott p\u00e1rhuzamosan, most m\u00e1r \u00e1ltal\u00e1ban csak egy fut egyszerre. S\u0151t, olyan id\u0151szakok is vannak, amikor egy sem fut. Ebb\u0151l arra k\u00f6vetkeztet\u00fcnk, hogy a v\u00e9grehajt\u00f3 goroutine m\u00e1s goroutine-okkal verseng k\u00f6z\u00f6s er\u0151forr\u00e1s\u00e9rt (vagyis k\u00f6z\u00f6s a v\u00e9grehajt\u00f3 pool). A probl\u00e9ma helye a frontend szolg\u00e1ltat\u00e1sban val\u00f3sz\u00edn\u0171, mivel a route spanek a frontend spanek gyerekei, \u00edgy azt val\u00f3sz\u00edn\u0171s\u00edtj\u00fck, hogy a route m\u0171veleteket a frontend m\u0171velete h\u00edvja. Ha van id\u0151nk, n\u00e9zz\u00fcnk r\u00e1 a frontend k\u00f3dj\u00e1ra: https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/services/frontend/best_eta.go , itt a getRoutes m\u0171velet az \u00e9rdekes. Azt l\u00e1tjuk, hogy egy poolt haszn\u00e1l az \u00fatvonalak megtervez\u00e9s\u00e9hez (a legkor\u00e1bbi v\u00e1rhat\u00f3 \u00e9rkez\u00e9si idej\u0171 \u00fatvonalat keress\u00fck). Azt is l\u00e1tjuk, hogy ha a pool m\u00e9rete kell\u0151en nagy, ak\u00e1r minden FindRoute m\u0171velet futhatna p\u00e1rhuzamosan. Arra k\u00f6vetkeztet\u00fcnk, hogy a pool m\u00e9rete nem kell\u0151en nagy (a kor\u00e1bbi tesztjeink alapj\u00e1n a pool m\u00e9ret\u00e9t 3-asnak v\u00e9lj\u00fck). N\u00f6velj\u00fck meg a pool m\u00e9ret\u00e9t 100-ra: A docker-compose.yml f\u00e1jlban v\u00e1ltoztassunk a command line param\u00e9tereken: \"-M\", \"-D\", \"100ms\", \"-W\", \"100\", \"all\" legyen az \u00faj halmaz, ments\u00fck el a v\u00e1ltoztat\u00e1st Ind\u00edtsuk \u00fajra a hotrod szolg\u00e1ltat\u00e1st: docker-compose up -d Tesztelj\u00fck a jav\u00edtott megold\u00e1st: gener\u00e1ljunk kb. 20 p\u00e1rhuzamos k\u00e9r\u00e9st \u00e9s ellen\u0151rizz\u00fck az egyik friss trace-t, a route spanek p\u00e1rhuzamosan kell fussanak. A driver szolg\u00e1ltat\u00e1son is lehetne optimaliz\u00e1lni, a Redis GetDriver lek\u00e9rdez\u00e9sek szekvenci\u00e1lisak, ezzel most nem foglalkozunk. Er\u0151forr\u00e1shaszn\u00e1lat attribut\u00e1lt m\u00e9r\u00e9se baggage seg\u00edts\u00e9g\u00e9vel \u00b6 Gyakran mer\u00fcl fel arra \u00fczleti ig\u00e9ny, hogy az er\u0151forr\u00e1s (pl. CPU) haszn\u00e1latot valamilyen magasabb szint\u0171 param\u00e9ter alapj\u00e1n m\u00e9rj\u00fck \u00e9s attribut\u00e1ljuk . A p\u00e9ld\u00e1nkban a route szolg\u00e1ltat\u00e1sban az \u00fatvonalkeres\u00e9s relat\u00edve CPU intenz\u00edv m\u0171velet, j\u00f3 lenne m\u00e9rni, hogy \u00fcgyfelenk\u00e9nt (customer) mennyi CPU id\u0151t haszn\u00e1l. Ugyanakkor a route szolg\u00e1ltat\u00e1s a f\u00fcgg\u0151s\u00e9gi gr\u00e1fban m\u00e9lyen van, itt m\u00e1r nincs inform\u00e1ci\u00f3 az \u00fcgyf\u00e9lr\u0151l, akinek kapcs\u00e1n az \u00fatvonalkeres\u00e9s t\u00f6rt\u00e9nik. Csak a m\u00e9r\u00e9s \u00e9rdek\u00e9ben egy explicit customer id param\u00e9tert bevezetni a route szolg\u00e1ltat\u00e1s API-j\u00e1n rossz tervez\u0151i d\u00f6nt\u00e9s lenne. Itt l\u00e9p k\u00e9pbe a tracing, illetve annak baggage mechanizmusa. Egy adott trace kontextus\u00e1ban m\u00e1r tudjuk, mely \u00fcgyf\u00e9l sz\u00e1m\u00e1ra t\u00f6rt\u00e9nik az \u00fatvonalkeres\u00e9s. A baggage pedig lehet\u0151s\u00e9get ny\u00fajt arra, hogy a szolg\u00e1ltat\u00e1sok k\u00f3dj\u00e1nak m\u00f3dos\u00edt\u00e1sa n\u00e9lk\u00fcl transzparens m\u00f3don tov\u00e1bb\u00edtsunk a trace-hez kapcsol\u00f3d\u00f3 metaadatokat a szolg\u00e1ltat\u00e1sok k\u00f6z\u00f6tt. Eset\u00fcnkban ez a customer id lesz, de ilyen lehet egy k\u00e9r\u00e9s/session/felhaszn\u00e1l\u00f3 azonos\u00edt\u00f3 is. A fenti koncepci\u00f3 demonstr\u00e1l\u00e1s\u00e1ra a route szolg\u00e1ltat\u00e1s olyan k\u00f3dot tartalmaz, mely m\u00e9ri az \u00fatvonalsz\u00e1m\u00edt\u00e1s idej\u00e9t \u00e9s a Go expvar metrika \"kezel\u0151\" standard library package seg\u00edts\u00e9g\u00e9vel \u00f6sszegy\u0171jti, nyilv\u00e1ntartja \u00e9s lek\u00e9rdezhet\u0151v\u00e9 teszi azt. A k\u00f3d itt tal\u00e1lhat\u00f3: https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/services/route/stats.go . Alapelve: A routeCalcByCustomer \u00e9s routeCalcBySession v\u00e1ltoz\u00f3k egy-egy map t\u00edpus\u00fa metrik\u00e1t defini\u00e1lnak, adott kulcsokkal. A stats egy strukt\u00fara t\u00f6mb, k\u00e9t elem\u0171. A strukt\u00fara objektumokban egy expvar metrika \u00e9s egy baggage kulcs tal\u00e1lhat\u00f3. Az updateCalcStats lek\u00e9rdezi az aktu\u00e1lis spant, egy cikulsban v\u00e9gigmegy a stats -ban t\u00e1rolt strukt\u00fara elemeken: minden elemre lek\u00e9rdezi a baggage-b\u0151l a stukt\u00farban t\u00e1rolt kulcs alapj\u00e1n a baggage elemet (\"customer\" \u00e9s \"sesion\"), majd ez \u00e9s a fut\u00e1si id\u0151 alapj\u00e1n friss\u00edti a strukt\u00far\u00e1ban t\u00e1rolt metrik\u00e1t. Az expvar metrik\u00e1k lek\u00e9rdezhet\u0151k a fut\u00f3 szolg\u00e1ltat\u00e1st\u00f3l a 8083-as porton: A docker-compose.yml f\u00e1jlban a hotrod szolg\u00e1ltat\u00e1sn\u00e1l publik\u00e1ljuk a 8083-os portot (vegy\u00fck fel a \"8083:8083\"-at a \"ports:\" al\u00e1), ments\u00fck el, a docker-compose up -d -vel friss\u00edts\u00fck a fut\u00f3 kont\u00e9nert. A HotROD frontendden gener\u00e1ljunk j\u00f3p\u00e1r k\u00e9r\u00e9st: az els\u0151 gombon kattintsunk sokat, a m\u00e1sodikon p\u00e1rat, a t\u00f6bbin ne kattintsunk. B\u00f6ng\u00e9sz\u0151b\u0151l kapcsol\u00f3djunk a hotrod kont\u00e9ner expvar szolg\u00e1ltat\u00e1s\u00e1hoz \u00e9s k\u00e9rdezz\u00fck le a metrik\u00e1kat: http://localhost:8083/debug/vars Keress\u00fcnk r\u00e1 sz\u00f6veg szerint a route.calc.by.customer.sec \u00e9s az alatta lev\u0151 route.calc.by.session.sec kulcsokra \u00e9s tekints\u00fck meg a metrik\u00e1k \u00e9rt\u00e9k\u00e9t. Az al\u00e1bbihoz hasonl\u00f3 kimenetet kapunk: \"route.calc.by.customer.sec\": {\"Amazing Coffee Roasters\": 1.0790000000000002, \"Japanese Desserts\": 1.5280000000000002, \"Rachel's Floral Designs\": 19.54500000000001, \"Trom Chocolatier\": 0.46199999999999997}, \"route.calc.by.session.sec\": {\"8221\": 22.61399999999999}, K\u00f3dinstrument\u00e1l\u00e1s \u00b6 Felmer\u00fcl a k\u00e9rd\u00e9s, mennyit kellett a k\u00f3d instrument\u00e1l\u00e1s\u00e1n dolgozni a HotROD alkalmaz\u00e1s fejleszt\u00e9se sor\u00e1n. Meglep\u0151en keveset: Be kell konfigur\u00e1lni, \u00f6ssze kell k\u00f6tni Jaegerrel. Az intrument\u00e1l\u00e1s a kommunik\u00e1ci\u00f3t v\u00e9gz\u0151 librarykben van: a Go hoz rendelkez\u00e9sre \u00e1ll\u00f3 REST \u00e9s TChannel kommunik\u00e1ci\u00f3t t\u00e1mogat\u00f3 open source k\u00f6nyvt\u00e1rak instrument\u00e1ltak, ezzel nek\u00fcnk nem kell foglalkozni. A k\u00f3dban a Mysql \u00e9s Redis szimul\u00e1ci\u00f3 eset\u00e9n tal\u00e1lhat\u00f3 explicit instrument\u00e1l\u00e1s. Feladat 2 - OpenTracing \u00e9s Jaeger alap\u00fa nyomk\u00f6vet\u00e9s .NET Core alap\u00fa alkalmaz\u00e1s eset\u00e9n \u00b6 Kiindul\u00f3 projekt \u00e9s megold\u00e1s: https://github.com/bmeviauav42/nyomkovetes A feladat keret\u00e9ben .NET Core 3.0 k\u00f6rnyezetben dolgozunk, de az OpenTracing \u00e9s Jaeger .NET kliens k\u00f6nyvt\u00e1rak kor\u00e1bbi .NET Core verzi\u00f3kkal is haszn\u00e1lhat\u00f3k, nincs benn\u00fck semmi .NET Core 3.0 specifikus. Vannak kliens k\u00f6nyvt\u00e1rak, melyek azt v\u00e1rj\u00e1k, hogy a jaeger-agent a helyi g\u00e9pen (pl. ugyanabban a docker containerben) fut, az agenttel UDP protokollon t\u00f6rt\u00e9nik a kommunik\u00e1ci\u00f3. A tapasztalatok szerint sz\u00e1mos kliens k\u00f6nyvt\u00e1r tud m\u00e1s g\u00e9pen/containerben fut\u00f3 agenttel kommunik\u00e1lni, konfigur\u00e1lhat\u00f3 a kliensben az agent c\u00edme (host \u00e9s a port). Ez lehet\u0151v\u00e9 teszi a jaegertracing/all-in-one k\u00e9nyelmes haszn\u00e1lat\u00e1t, a szolg\u00e1ltat\u00e1sokat futtat\u00f3 kont\u00e9netben nem kell jaeger agentet telep\u00edteni. \u00c9les k\u00f6rnyezetben ez a megold\u00e1s korl\u00e1tozottan aj\u00e1nlott: az UDP megb\u00edzhat\u00f3an m\u0171k\u00f6dik localhost eset\u00e9n, de egy val\u00f3di h\u00e1l\u00f3zaton veszhetnek el csomagok. A legt\u00f6bb kliens k\u00f6nyvt\u00e1r azt is t\u00e1mogatja, hogy az agent kihagy\u00e1s\u00e1val, k\u00f6zvetlen a collectornak t\u00f6lt\u00e9njen az adatk\u00fcld\u00e9s, ak\u00e1r HTTP protokollon. A kommunik\u00e1ci\u00f3r\u00f3l sz\u00f3l\u00f3 gyakorlat n\u00e9mik\u00e9ppen tov\u00e1bbfejlesztett Order+Customer kiindul\u00e1si p\u00e9ld\u00e1j\u00e1t instrument\u00e1ljuk k\u00f6tj\u00fck \u00f6ssze Jaegerrel \u00e9s instrument\u00e1ljuk OpenTracing alapokon \u00e9s. Miben m\u00e1s a kiindul\u00f3 k\u00f3d, mint a kor\u00e1bbi kommunik\u00e1ci\u00f3s \u00f3r\u00e1n szerepl\u0151 kiindul\u00f3 gyakorlat? Repository mint\u00e1t haszn\u00e1l. A Catalog szolg\u00e1ltat\u00e1s Sqlite adatb\u00e1zisban t\u00e1rolja az adatokat (in-memory \u00fczemm\u00f3dra konfigur\u00e1ltan), indul\u00e1skori seed-et, vagyis tesztadat gener\u00e1l\u00e1st k\u00f6vet\u0151en. Ez \u00e9les k\u00f6rnyezetben nem haszn\u00e1lhat\u00f3, a c\u00e9lja mind\u00f6ssze a nyomk\u00f6vet\u00e9s demonstr\u00e1l\u00e1sa. A v\u00e1laszt\u00e1s az\u00e9rt esett az Sqlite-ra az Entity Framework In Memory Database-zel szemben, mert r\u00e9szletesebb nyomk\u00f6vet\u00e9si inform\u00e1ci\u00f3t szolg\u00e1ltat. El\u0151k\u00e9sz\u00edt\u00e9s, konfigur\u00e1ci\u00f3 \u00b6 Kiindul\u00f3 l\u00e9p\u00e9sek: GitHub-r\u00f3l kl\u00f3nozzuk ki a kiindul\u00f3 solution-t: Hozzunk l\u00e9tre egy tracing mapp\u00e1t a c:\\work\\<saj\u00e1tn\u00e9v> munkak\u00f6nyv\u00e1trunkban \u00e9s ind\u00edtsunk egy command prompotot innen, futtasuk az al\u00e1bbi parancsot: git clone https://github.com/bmeviauav42/nyomkovetes Ind\u00edtsuk el VS alatt a szolg\u00e1ltat\u00e1sokat A b\u00f6ng\u00e9sz\u0151ben hibaoldal jelenik meg. Friss\u00edts\u00fck (ha kell t\u00f6bbsz\u00f6r is), a hiba elt\u0171nik. Az oka: az OrderService h\u00edvja a CatalogService-t, de az el\u0151sz\u00f6r m\u00e9g nem \u00e1llt fel teljesen, az Sqlite adatb\u00e1zis seed-el\u00e9se id\u0151t ig\u00e9nyel. Ha kev\u00e9s az id\u0151, n\u00e9zhetj\u00fck a feladat megold\u00e1s\u00e1t: ez a megoldas/1-konfig-es-beepitett-instrumentalas \u00e1gon tal\u00e1lhat\u00f3, az \u00e1g checkout l\u00e9p\u00e9sei: git fetch git checkout megoldas/1-konfig-es-beepitett-instrumentalas VS alatt startup projektnek \u00e1ll\u00edtsuk be a docker-compose projektet. Projekt referenci\u00e1k felv\u00e9tele \u00b6 A solution\u00fcnk t\u00f6bb projektb\u0151l/szolg\u00e1ltat\u00e1sb\u00f3l \u00e1ll. Mindegyiket hasonl\u00f3 m\u00f3don kell OpenTracing \u00e9s Jaeger vonatkoz\u00e1s\u00e1ban inicializ\u00e1lni, illetve ugyanarra a Jaeger \"kiszolg\u00e1l\u00f3ra\" kell r\u00e1k\u00f6tni. Ezt a k\u00f3dot ne copy-paste-tel szapor\u00edtsuk, hanem tegy\u00fck ki egy k\u00fcl\u00f6n megosztott k\u00f6nyvt\u00e1rba. Ez a kiindul\u00f3 megold\u00e1sban el\u0151 van k\u00e9sz\u00edtve, m\u00e1r van egy ilyen projekt (Msa.Comm.Lab.Shared), csak a projekt referenci\u00e1k nincsenek felv\u00e9ve. Tegy\u00fck ezt meg minden projektn\u00e9l, ahol a nyomk\u00f6vet\u00e9st haszn\u00e1lni akarjuk: Msa.Comm.Lab.Services.Catalog -> Msa.Comm.Lab.Shared projekt referencia felv\u00e9tele Msa.Comm.Lab.Services.Order -> Msa.Comm.Lab.Shared projekt referencia felv\u00e9tele OpenTracing \u00e9s Jaeger kliens k\u00f6nyvt\u00e1rak \u00b6 .NET Core k\u00f6rnyezetben az al\u00e1bbi NuGet csomagok haszn\u00e1lat\u00e1ra van sz\u00fcks\u00e9g: \"OpenTracing\" OpenTracing API a k\u00f3d instrument\u00e1l\u00e1s\u00e1hoz. https://github.com/opentracing/opentracing-csharp \"OpenTracing.Contrib.NetCore\" https://github.com/opentracing-contrib/csharp-netcore Nem k\u00f6telez\u0151, de .NET Core k\u00f6rnyezetben javasolt: an\u00e9lk\u00fcl tudunk a seg\u00edts\u00e9g\u00e9vel pl. REST h\u00edv\u00e1sokat trace-elni, hogy a k\u00f3dunkat instument\u00e1lni kellene. Be\u00e9p\u00fcl az ASP.NET-be, Entity Framework-be, bizonyos .NET Core BCL t\u00edpusokba,melyek k\u00f6z\u00fcl a legfontosabb a HttpClient. Minden .NET k\u00f6nyvt\u00e1rat, keretrendszert, stb.-t t\u00e1mogat, ami a .NET DiagnosticSource-t haszn\u00e1lja (minden Activity-hez spant k\u00e9sz\u00edt \u00e9s span log-ot az egy\u00e9b esem\u00e9nyekhez). Beregisztr\u00e1lja mag\u00e1t a Microsoft.Extensions.Logging rendszerbe, \u00e9s minden logba \u00edr\u00e1s eset\u00e9n span log-ot k\u00e9sz\u00edt, de csak akkor, ha van akt\u00edv span. F\u00fcgg\u0151s\u00e9gk\u00e9nt felteszi az OpenTracing package-et is! \"Jaeger\" https://github.com/jaegertracing/jaeger-client-csharp Jaeger .NET kliens k\u00f6nyvt\u00e1r Az Msa.Comm.Lab.Shared k\u00f6nyvt\u00e1rba m\u00e1r fel vannak v\u00e9ve ezek a NuGet f\u00fcgg\u0151s\u00e9gek (az OpenTracing csak k\u00f6zvetve, az OpenTracing.Contrib.NetCor e alatt), \u00edgy nek\u00fcnk nem kell megtenn\u00fcnk. A szolg\u00e1ltat\u00e1s projektekben nem fogunk els\u0151 k\u00f6rben k\u00f6zvetlen OpenTracing instrument\u00e1l\u00e1st v\u00e9gezni, csak haszn\u00e1ljuk a Msa.Comm.Lab.Shared k\u00f6nyvt\u00e1r egyetelen oszt\u00e1ly\u00e1t/m\u0171velet\u00e9t a szolg\u00e1ltat\u00e1sok konfigur\u00e1l\u00e1sakor: vegy\u00fck fel a Msa.Comm.Lab.Services.Catalog projekt Startup.ConfigureServices m\u0171velet\u00e9nek elej\u00e9re: // Registers and starts Jaeger (see Shared.JaegerServiceCollectionExtensions) // Also registers OpenTracing services . AddJaeger ( currentEnvironment ); A szolg\u00e1ltat\u00e1s projektekbe \u00edgy (egyel\u0151re legal\u00e1bbis) egyetlen OpenTracing/NuGet package-et sem vett\u00fcnk/vesz\u00fcnk fel, csak k\u00f6zvetett f\u00fcgg\u00e9s van! Tekints\u00fck \u00e1t a JaegerServiceCollectionExtensions.AddJaeger m\u0171veletet, a legfontosabbak: A DI kont\u00e9nerbe egy ITrace r implement\u00e1ci\u00f3t regisztr\u00e1lunk be singletonk\u00e9nt. Ez eset\u00fcnkben egy Jaeger tracer lesz, amit a p\u00e9ld\u00e1ban k\u00f6rnyezeti v\u00e1ltoz\u00f3k alapj\u00e1n inicializ\u00e1lunk (docker k\u00f6rnyezetben praktikus megk\u00f6zel\u00edt\u00e9s): Jaeger.Configuration.FromEnv(loggerFactory) A k\u00f6rnyezeti v\u00e1ltoz\u00f3k k\u00f6z\u00fcl a JAEGER_SERVICE_NAME , JAEGER_AGENT_HOST , JAEGER_AGENT_PORT \u00e9s JAEGER_SAMPLER_TYPE a fontosabbak. A GlobalTracer egy klasszikus sigleton hozz\u00e1f\u00e9r\u00e9st biztos\u00edt b\u00e1rhol a k\u00f3dban a tracer-hez, ezt is \u00e1ll\u00edtsuk be: GlobalTracer.Register(tracer) . Ez azon k\u00f3d sz\u00e1m\u00e1ra fontos, mely nem tud DI alapokon m\u0171k\u00f6dni. A v\u00e9g\u00e9n fontos az OpenTracing szolg\u00e1ltat\u00e1sok regisztr\u00e1l\u00e1sa is, eddig csak Jaegerrel foglalkoztunk: services.AddOpenTracing(); Jaeger szolg\u00e1ltat\u00e1s be\u00e9p\u00edt\u00e9se docker-compose.yml-be \u00b6 Eg\u00e9sz\u00edts\u00fck ki a docker-compose.yml f\u00e1jlt (id\u0151hi\u00e1ny eset\u00e9n m\u00e1soljuk be az eg\u00e9szet): jaeger szolg\u00e1ltat\u00e1s felv\u00e9tele a m\u00e1r megl\u00e9v\u0151 k\u00e9t szolg\u00e1ltat\u00e1sn\u00e1l depends_on alatt jaeger megad\u00e1sa k\u00f6rnyezeti v\u00e1ltoz\u00f3k felv\u00e9tele (ez eet\u00fcnkben nem k\u00f6telez\u0151, mert ha nem adjuk meg, az Msa.Comm.Lab.Shared \u00e1ltal be\u00e1ll\u00edtott alap\u00e9rt\u00e9kek megfelel\u0151k) version : '3.7' services : msa.comm.lab.services.catalog : image : ${DOCKER_REGISTRY-}msacommlabservicescatalog build : context : . dockerfile : Msa.Comm.Lab.Services.Catalog/Dockerfile environment : - JAEGER_AGENT_HOST=jaeger - JAEGER_AGENT_PORT=6831 - JAEGER_SAMPLER_TYPE=const depends_on : - jaeger msa.comm.lab.services.order : image : ${DOCKER_REGISTRY-}msacommlabservicesorder build : context : . dockerfile : Msa.Comm.Lab.Services.Order/Dockerfile environment : - JAEGER_AGENT_HOST=jaeger - JAEGER_AGENT_PORT=6831 - JAEGER_SAMPLER_TYPE=const depends_on : - msa.comm.lab.services.catalog - jaeger jaeger : image : jaegertracing/all-in-one:latest ports : - \"16686:16686\" # For Jaeger web UI Tesztelj\u00fck a m\u0171k\u00f6d\u00e9st: Ind\u00edtsuk el a szolg\u00e1ltat\u00e1sokat VS alatt, friss\u00edts\u00fck p\u00e1rszor a b\u00f6ng\u00e9sz\u0151ablakot Jelen\u00edts\u00fck meg Jaeger UI-t: http://localhost:16686/ A sz\u0171r\u0151panelen v\u00e1lasszuk ki a Msa.Comm.Lab.Services.Order szolg\u00e1ltat\u00e1st, friss\u00edts\u00fck a megjelen\u00edtett trace.eket (Find traces gomb), v\u00e1lasszunk ki jobboldalt egy hib\u00e1val nem rendelkez\u0151 trace-t \u00e9s nyissuk meg. A r\u00e9szteles trace megjelen\u00edt\u0151ben l\u00e1tjuk span hierarchi\u00e1t: Http h\u00edv\u00e1sok \u00e9s DB ExecuteReader a m\u00e9ly\u00e9n. Az Action Msa.Comm.Lab.Services.Catalog.Controllers.ProductController/Get spanben sz\u00e1mos log van, t\u00f6bbek k\u00f6z\u00f6tt EF-h\u00f6z kapcsol\u00f3d\u00f3k is. Mindezt \u00fagy \u00e9rt\u00fck el, hogy semmif\u00e9le OpenTracing instrument\u00e1l\u00e1st nem v\u00e9gezt\u00fcnk a saj\u00e1t k\u00f3dunkban . K\u00f3d instrument\u00e1l\u00e1s \u00b6 Checkoutoljuk ki Git-ben k\u00e9sz megold\u00e1st, a megoldas/1-konfig-es-beepitett-instrumentalas \u00e1gon tal\u00e1lhat\u00f3: git checkout megoldas/2-kod-instrumentalas Egyszer\u0171 napl\u00f3z\u00e1s (##Instr_Log) \u00b6 Itt m\u00e9g nem haszn\u00e1lunk OpenTracing specifikus instrument\u00e1l\u00e1st, az Microsoft.Extensions.Logging ILogger seg\u00edts\u00e9g\u00e9vel napl\u00f3zunk. Az Order szolg\u00e1ltat\u00e1s TestController oszt\u00e1lyt n\u00e9zz\u00fck DI-vel kap egy ILogger<TestController> objektumot. Ezt a Microsoft.Extensions.Logging be\u00e9p\u00edtve t\u00e1mogatja. A Get() m\u0171veletben a log.LogInformation h\u00edv\u00e1ssal napl\u00f3zunk. Struktur\u00e1lt napl\u00f3z\u00e1st haszn\u00e1lunk, az els\u0151 param\u00e9terben a {ProdCount} defini\u00e1l egy kulcsot, az \u00e9rt\u00e9ke a count param\u00e9ter lesz. Ez nemcsak mint string, hanem egy kulcs-\u00e9rt\u00e9k p\u00e1rk\u00e9nt is megjelenik a napl\u00f3z\u00e1s sor\u00e1n: a spanhez f\u0171z\u00f6tt log-ban lesz egy ilyen kulcs-\u00e9rt\u00e9k p\u00e1r. Futtassuk az alkalmaz\u00e1st, gener\u00e1ljunk p\u00e1r nem hib\u00e1s h\u00edv\u00e1st. Jager UI frontenden nyissunk meg egy nem hib\u00e1s trace-t. A fels\u0151 Find keres\u0151be \u00edrjuk be: ProdCount \u00e9s keress\u00fcnk r\u00e1. Azon spanek, ahol van tal\u00e1lat, s\u00e1rga sz\u00ednnel jelennek meg. Egy tal\u00e1latunk van. Nyissuk le a spant, \u00e9s keress\u00fck ki szemmel a sz\u00e1munkra \u00e9rdekes logbejegyz\u00e9st. Itt l\u00e1tjuk, hogy a logon megjelenik a ProdCount = 3 kulcs-\u00e9rt\u00e9k p\u00e1r, \u00edgy lehet(ne) erre is \u00e9rtelmesen keresni (a nyit\u00f3oldalon a trace keres\u0151ben igen, a trace r\u00e9szletes oldalon a span keres\u0151ben nem). Tipp: A span keres\u0151 egyel\u0151re el\u00e9g b\u00e9na: ha valamit nem tal\u00e1lunk, a jobb fels\u0151 sarokban lev\u0151 gombbal v\u00e1ltsunk JSON n\u00e9zetre \u00e9s keress\u00fcnk abban sz\u00f6veg szerint. Saj\u00e1t span k\u00e9sz\u00edt\u00e9se, taggel\u00e9s (##Instr_CreateSpan) \u00b6 Saj\u00e1t spant k\u00e9sz\u00edt\u00fcnk. Itt m\u00e1r explicit OpenTracing API instument\u00e1l\u00e1st v\u00e9gz\u00fcnk. Ehhez \"logikailag\" fel kellene vegy\u00fck az \u00e9rintett projektben az OpenTracing NuGet package hivatkoz\u00e1st ( Jager \u00e9s OpenTracing.Contrib.NetCore nem kell, hiszen mi csak az API-t haszn\u00e1ljuk). Eset\u00fcnkben nem kell megtenni, mert az Msa.Comm.Lab.Shared projektre van referencia, aminek m\u00e1r van (k\u00f6zvetett) OpenTracing f\u00fcgg\u00e9se, a .NET Core 3.0 eset\u00e9ben ez m\u00e1r el\u00e9g a megfelel\u0151 m\u0171k\u00f6d\u00e9shez. Feladat : a Catalog szolg\u00e1ltat\u00e1s ProductController oszt\u00e1lyban a repository-hoz val\u00f3 hozz\u00e1r\u00e9s el\u0151tt cache-ben val\u00f3 keres\u00e9st szimul\u00e1lunk, ezt egy \u00faj span hat\u00f3k\u00f6r\u00e9ben trace-elj\u00fck. A ProductController f\u00fcgg\u0151s\u00e9ginjekt\u00e1l\u00e1ssal kap egy ITracer objektumot A Get(int id) -ban lev\u0151 k\u00f3dot \u00e9rtelmezz\u00fck \u00daj span l\u00e9trehoz\u00e1sa, param\u00e9terben m\u0171veletn\u00e9v Tag hozz\u00e1f\u0171z\u00e9se akt\u00edv spanhez Log esem\u00e9ny felv\u00e9tele akt\u00edv spanhez Futtasuk az alkalmaz\u00e1st, b\u00f6ng\u00e9sz\u0151ben egym\u00e1s ut\u00e1n k\u00e9rj\u00fck le az egyes term\u00e9kek adatait a Test szolg\u00e1ltat\u00e1s seg\u00edts\u00e9g\u00e9vel, a k\u00fcl\u00f6nb\u00f6z\u0151 k\u00f3d \u00e1gak tesztel\u00e9s\u00e9hez: https://localhost:44385/api/test/1 , cache hib\u00e1t gener\u00e1l https://localhost:44385/api/test/1 , nincs cache tal\u00e1lat https://localhost:44385/api/test/1 , van cache tal\u00e1lat A Jaeger UI seg\u00edts\u00e9g\u00e9vel vizsg\u00e1ljuk meg a h\u00e1rom trace-t Baggage haszn\u00e1lata (##Instr_Baggage) \u00b6 A Catalog szolg\u00e1ltat\u00e1s ProductRepositor y oszt\u00e1lyban \u00edrjuk ki a felhaszn\u00e1l\u00f3nevet \u00e9s k\u00e9r\u00e9st azonos\u00edt\u00f3t Log-ba, melyet a p\u00e9ld\u00e1nkban az Order szolg\u00e1ltat\u00e1s gener\u00e1l. A megold\u00e1s elve: Nem szennyezz\u00fck az API-t, nem vessz\u00fck fel explicit param\u00e9terk\u00e9nt Helyette baggage -ben tov\u00e1bb\u00edtjuk. Pontosabban r\u00e1b\u00edzzuk az OpenTracing instrument\u00e1lt HttpClient-re (az bepakolja http headerbe a baggage tartalm\u00e1t). L\u00e9p\u00e9sek OrderService.TestController -t n\u00e9zz\u00fck meg, itt rt\u00f6rt\u00e9nik az akt\u00edv span baggage-\u00e9be a kulcs-\u00e9rt\u00e9k p\u00e1rok felv\u00e9tele (string-string). CatalogService.ProductRepository -t n\u00e9zz\u00fck meg, itt olvassuk ki az akt\u00edv span Bbaggage-\u00e9b\u0151l az \u00e9rt\u00e9keket. Ezeket logban hozz\u00e1\u00edrjuk az akt\u00edv span-hez. (Nagyobb \u00e9rtelme lenne pl. valamilyen countert/merik\u00e1t nyilv\u00e1ntartani ez alapj\u00e1n). Futtassuk (b\u00f6ng\u00e9sz\u0151ben https://localhost:44385/api/test ) Jaeger UI-n a trace r\u00e9szletes n\u00e9zetben az ablak tetej\u00e9n a keres\u0151be \u00edrjuk be: Msa.Comm.Lab.Services.Catalog.Controllers.ProductController/Get . A s\u00e1rg\u00e1val kiement spant nyissuk le, kb. a 10. logbejegyz\u00e9s a ProductRepository.GetProducts is executed , l\u00e1tjuk a username \u00e9s requestid kulcsokat \u00e9s azok \u00e9rt\u00e9k\u00e9t.","title":"Elosztott nyomk\u00f6vet\u00e9s"},{"location":"DevOps/Nyomkovetes/#eloadas","text":"Elosztott nyomk\u00f6vet\u00e9s","title":"El\u0151ad\u00e1s"},{"location":"DevOps/Nyomkovetes/#cel","text":"OpenTracing \u00e9s Jaeger alap\u00fa elosztott nyomk\u00f6vet\u00e9s eszk\u00f6zt\u00e1r\u00e1nak megismer\u00e9se OpenTracing \u00e9s Jaeger alap\u00fa nyomk\u00f6vet\u00e9s alapok .NET Core alap\u00fa mikroszolg\u00e1ltat\u00e1sok eset\u00e9n","title":"C\u00e9l"},{"location":"DevOps/Nyomkovetes/#elokovetelmenyek","text":"Docker Desktop Visual Studio 2019 min v16.3 ASP.NET Core 3.0 SDK","title":"El\u0151k\u00f6vetelm\u00e9nyek"},{"location":"DevOps/Nyomkovetes/#1-feladat-opentracing-alapu-nyomkovetes-jaeger-kornyezetben","text":"","title":"1. Feladat - OpenTracing alap\u00fa nyomk\u00f6vet\u00e9s Jaeger k\u00f6rnyezetben"},{"location":"DevOps/Nyomkovetes/#bevezeto","text":"A feladat keret\u00e9ben egy olyan mikroszolg\u00e1ltat\u00e1s alap\u00fa mintaalkalmaz\u00e1st vizsg\u00e1lunk meg, mely az OpenTracing haszn\u00e1lat\u00e1t illusztr\u00e1lja, Jaeger alap\u00fa backenddel \u00e9s megjelen\u00edt\u00e9ssel. A feladat a Take OpenTracing for a HotROD ride blogbejegyz\u00e9sen alapul, de n\u00e9h\u00e1ny olyan Jaeger szolg\u00e1ltat\u00e1st is megvizsg\u00e1lunk, melyet a cikk nem taglal. A mintaalkalmaz\u00e1s neve \"Hot R.O.D\". Go nyelven \u00edr\u00f3dott, k\u00f3dja OpenTracing instrument\u00e1lt. A forr\u00e1sk\u00f3dja a Jaeger GitHub repository p\u00e9ld\u00e1k k\u00f6z\u00f6tt tal\u00e1lhat\u00f3 meg: https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod . Itt tal\u00e1lunk le\u00edr\u00e1st arr\u00f3l, hogyan lehet a HotROD \u00e9s Jaeger alkalmaz\u00e1sokat docker alapokon futtatni: A Jeager futatt\u00e1s\u00e1ra a jaegertracing/all-in-one docker image-et haszn\u00e1ljuk: ez valamennyi Jaeger backend komponenst tartalmaz (agent, collector, ingester, stb.), bele\u00e9rtve az adatmegjelen\u00edt\u0151 frontendet is. Ez az image ismerked\u00e9shez, helyi k\u00f6rnyezetben tesztel\u00e9shez aj\u00e1nlott, production k\u00f6rnyezethez nem aj\u00e1nlj\u00e1k. B\u0151vebb le\u00edr\u00e1st az image-r\u0151l t\u00f6bbek k\u00f6z\u00f6tt itt tal\u00e1lhatunk: https://www.jaegertracing.io/docs/getting-started . A sz\u00e1mtalan portb\u00f3l sz\u00e1munkra kett\u0151 \u00e9rdekes, a 6831 (Jaeger agent, span-eket itt fogadja UDP-n a kliens k\u00f6nyvt\u00e1rakt\u00f3l) \u00e9s a 16686 (Jaeger UI frontend). Megjegyz\u00e9sek: A \"jaegertracing/all-in-one\" image alap\u00e9rtelmez\u00e9sben egy in-memory t\u00e1rol\u00f3t haszn\u00e1l, \u00edgy \u00fajraind\u00edt\u00e1st k\u00f6vet\u0151en elvesznek a kor\u00e1bban r\u00f6gz\u00edtett trace/span-ek. Le\u00edr\u00e1s a tov\u00e1bbi Jaeger image-ekr\u0151l \u00e9s konfigur\u00e1ci\u00f3s lehet\u0151s\u00e9gekr\u0151l itt tal\u00e1lgat\u00f3: https://www.jaegertracing.io/docs/deployment A HotROD futtat\u00e1s\u00e1ra a jaegertracing/example-hotrod docker image haszn\u00e1lhat\u00f3. Ez az image egyben tartalmazza valamennyi HotROD szolg\u00e1ltat\u00e1s k\u00f3dj\u00e1t, futatt\u00e1sakor minden sz\u00fcks\u00e9ges szolg\u00e1ltat\u00e1s elindul ugyanabban a kont\u00e9nerben, csak k\u00fcl\u00f6nb\u00f6z\u0151 portokon. A HotRod \u00e9s a Jaeger kont\u00e9nerek egyszerre t\u00f6rt\u00e9n\u0151 ind\u00edt\u00e1s\u00e1ra docker-compose-t haszn\u00e1lunk: T\u00f6lts\u00fck le a docker-compose.yml -t innen https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/docker-compose.yml , ide ments\u00fck: c:\\work\\<saj\u00e1tn\u00e9v> Nyissuk meg a f\u00e1jlt VS Code-ban, tekints\u00fck \u00e1t a tartalm\u00e1t A jaeger szolg\u00e1ltat\u00e1s a 6831 (Jaeger agent spanfelt\u00f6lt\u0151) \u00e9s a 16686 (Jaeger UI frontend) portokat mappeli. A hotrod szolg\u00e1ltat\u00e1s sz\u00e1m\u00e1ra k\u00f6rnyezeti v\u00e1ltoz\u00f3kban mondjuk meg, milyen hostn\u00e9ven \u00e9s milyen porton \u00e9ri az a Jaeger agentet (span-ek fel\u00f6lt\u00e9s\u00e9hez sz\u00fcks\u00e9ges). Ind\u00edtsunk \u00faj command promptot, navig\u00e1ljunk az els\u0151 l\u00e9p\u00e9sben let\u00f6lt\u00f6tt docker-compose.yml (c:\\work\\ ) mapp\u00e1ba, majd ind\u00edtsuk a kont\u00e9nereket: docker-compose up Ha nem indul a HotRod a 8080-as port \u00fctk\u00f6z\u00e9s miatt, akkor: docker-compose down -nal \u00e1ll\u00edtsuk le a kont\u00e9nereket A docker-compose.yml -ben mappelj\u00fcnk m\u00e1s k\u00fcls\u0151 portra, pl.: 8090:8080 Ind\u00edtsuk \u00fajra docker-compose up A HotRod frontend a http://localhost:8080 , a Jaeger frontend a http://localhost:16686 c\u00edmen \u00e9rhet\u0151 el. K\u00e9s\u0151bb a szolg\u00e1ltat\u00e1sok le\u00e1ll\u00edt\u00e1sa a k\u00f6vetkez\u0151 paranccsal lesz lehets\u00e9ges (most m\u00e9g ne futtassuk): docker-compose down","title":"Bevezet\u0151"},{"location":"DevOps/Nyomkovetes/#ismerkedes-a-hotrod-alkalmazassal","text":"A HotRod egy \"Ride Sharing\" alkalmaz\u00e1s. Uber-hez hasonl\u00f3, szem\u00e9lyek vagy egy\u00e9b k\u00e9zbes\u00edtend\u0151 t\u00e1rgyak j\u00e1rm\u0171vekkel t\u00f6rt\u00e9n\u0151 c\u00e9lba juttat\u00e1s\u00e1hoz. Egy Go alkalmaz\u00e1s, t\u00f6bb szolg\u00e1ltat\u00e1sb\u00f3l \u00e1ll. A docker image az \"all\" param\u00e9terrel futtatja az alkalmaz\u00e1st, mely valamennyi szolg\u00e1ltat\u00e1s\u00e1t elind\u00edtja. N\u00e9zz\u00fcnk r\u00e1 a command promptban a docker-compose up \u00e1ltal megjelen\u00edtett logokra, l\u00e1tszik, hogy n\u00e9gy szolg\u00e1ltat\u00e1sb\u00f3l \u00e1ll, melyek a 8080-8083 portokon \u00e9rhet\u0151k el: frontend customer driver route Kapcsol\u00f3djuk a HotROD frontendhez b\u00f6ng\u00e9sz\u0151b\u0151l, ha m\u00e9g nem tett\u00fck meg ( http://localhost:8080 ). Ismerkedj\u00fcnk meg a fel\u00fclet m\u0171k\u00f6d\u00e9s\u00e9vel: Egy gomb megnyom\u00e1s\u00e1val az alkalmaz\u00e1s egy adott \u00fcgyf\u00e9lhez (aki egy adott helyen tart\u00f3zkodik) keres egy a k\u00f6zel\u00e9ben tart\u00f3zkod\u00f3 j\u00e1rm\u0171vet, pl. egy csomag felv\u00e9tel\u00e9hez vagy egy szem\u00e9ly felv\u00e9tel\u00e9hez. Bal fels\u0151 sarokban egy a JavaScript frontend \u00e1ltal gener\u00e1lt v\u00e9letlen session azonos\u00edt\u00f3t l\u00e1tunk (F5-re m\u00e1s lesz). Gombnyom\u00e1sra \u00faj keres\u00e9st ind\u00edt, mely kapcs\u00e1n egy \u00faj sorban a k\u00f6vetkez\u0151 adatokat napl\u00f3zza a fel\u00fclet: A keres\u00e9s sor\u00e1n a k\u00e9r\u00e9shez rendelt j\u00e1rm\u0171 rendsz\u00e1ma \u00e9s v\u00e1rhat\u00f3 \u00e9rkez\u00e9si ideje K\u00e9r\u00e9s azonos\u00edt\u00f3 (session id + sorsz\u00e1m) Kiszolg\u00e1l\u00e1si id\u0151, a JavaScript k\u00f6nyvt\u00e1r m\u00e9ri","title":"Ismerked\u00e9s a HotROD alkalmaz\u00e1ssal"},{"location":"DevOps/Nyomkovetes/#architektura-szolgaltatasfuggosegek-felderitese","text":"L\u00e9p\u00e9sek: Kattintsunk valamelyik gombon a HotROD frontenden (ha m\u00e9g nem tett\u00fck meg) A Jaeger frontenden Dependencies men\u00fc, majd DAG tab kiv\u00e1laszt\u00e1sa A Jeager a h\u00edv\u00e1sok sor\u00e1n begy\u0171jt\u00f6tt trace/span adatok alapj\u00e1n a k\u00f6vetkez\u0151ket jelen\u00edti meg: a szolg\u00e1ltat\u00e1sokat a szolg\u00e1ltatat\u00e1sok k\u00f6z\u00f6tti f\u00fcgg\u0151s\u00e9geket a szolg\u00e1ltat\u00e1sok k\u00f6z\u00f6tti h\u00edv\u00e1sok sz\u00e1m\u00e1t Az \u00e1bra az eg\u00e9rrel pan-elhet\u0151, illetve az eg\u00e9rg\u00f6rg\u0151vel nagy\u00edthat\u00f3 (azon pontra vonatkoz\u00f3an, ahol az eg\u00e9r \u00e9ppen \u00e1ll). Force Directed Graph tab: hasonl\u00f3 az el\u0151z\u0151h\u00f6z, az eg\u00e9rrel az adott csom\u00f3ponra \u00e1llva az adott csom\u00f3pont f\u00fcgg\u0151s\u00e9geit emeli ki. L\u00e1tjuk, hogy b\u00e1r a HotROD egyetlen kont\u00e9nerben fut, hat szolg\u00e1ltat\u00e1sb\u00f3l \u00e1ll. Megjegyz\u00e9s: a Mysql \u00e9s a Redis nem val\u00f3s szolg\u00e1ltat\u00e1sok, a HotROD ezeket csak \"szimul\u00e1lja\", ezek nem val\u00f3di adatkezel\u0151k.","title":"Architekt\u00fara, szolg\u00e1ltat\u00e1sf\u00fcgg\u0151s\u00e9gek felder\u00edt\u00e9se"},{"location":"DevOps/Nyomkovetes/#trace-ek-es-spanek-megjelenitese","text":"A Jaeger frontenden v\u00e1laszuk ki a \"Search\" men\u00fct, itt van lehet\u0151s\u00e9g\u00fcnk a Jaeger \u00e1ltal r\u00f6gz\u00edtett trace-ek list\u00e1z\u00e1s\u00e1ra a megadott sz\u0171r\u00e9si felt\u00e9teleknek megfelel\u0151en. L\u00e9p\u00e9sek: A HotRod frontenden kattintsunk m\u00e9g egyszer-k\u00e9tszer a gombokon, hogy t\u00f6bb, mint egy trace-\u00fcnk legyen a rendszerben. A Jaeger frontend sz\u0171r\u0151panelj\u00e9n a \"Service\" dropdown-ban v\u00e1lasszuk ki a \"Frontend\" elemet. Ha nincs a list\u00e1ban, akkor az F5 gombban friss\u00edts\u00fck a b\u00f6ng\u00e9sz\u0151 oldal\u00e1t. Kattintsunk a \"Find traces\" gombbon A jobb oldali panel fels\u0151 r\u00e9sz\u00e9n az x id\u0151tengely ment\u00e9n a trace-ek jelennek meg (az y tengely a v\u00e9grehajt\u00e1si id\u0151). Minden kliens oldali k\u00e9r\u00e9shez k\u00fcl\u00f6n trace sz\u00fcletik. Itt is lehet\u0151s\u00e9g\u00fcnk van az adott k\u00f6rre kattintva a trace r\u00e9szleteinek megjelen\u00edt\u00e9s\u00e9re. A diagram alatt a trace-ek list\u00e1s n\u00e9zete jelenik meg, minden trace egy k\u00fcl\u00f6n sor , sz\u00e1mos hasznos inform\u00e1ci\u00f3val (t\u00f6bbek k\u00f6z\u00f6tt a trace-ben lev\u0151 span-ek sz\u00e1ma, ugyenez szolg\u00e1ltat\u00e1sonk\u00e9nti lebont\u00e1sban, hibaesem\u00e9nyek sz\u00e1ma, stb.) Lehet\u0151s\u00e9g van a t\u00e9telek sorrendez\u00e9s\u00e9re (pl. v\u00e9grehajt\u00e1si id\u0151 szerint is hasznos lehet). Kattintsunk az egyik trace-re a r\u00e9szletes n\u00e9zet\u00e9nek megjelen\u00edt\u00e9s\u00e9hez. Trace r\u00e9szletes n\u00e9zet: A trace spanjeinek egym\u00e1sba \u00e1gyaz\u00e1si hierarchi\u00e1j\u00e1t l\u00e1tjuk a v\u00edzszintes id\u0151tengely ment\u00e9n. Egy span hossza ar\u00e1nyos a v\u00e9grehajt\u00e1si idej\u00e9vel. Minden span k\u00fcl\u00f6n sor, a legfels\u0151 sor a root span: a b\u00f6ng\u00e9sz\u0151b\u0151l kiadott JavaScript k\u00e9r\u00e9st fogad\u00f3 frontend szolg\u00e1ltat\u00e1shoz tartozik, a m\u0171velet neve HTTP GET/dispatch , el\u0151sz\u00f6r a customer szolg\u00e1ltat\u00e1st h\u00edvja, mely fogadja a k\u00e9r\u00e9st \u00e9s a mysql szolg\u00e1ltat\u00e1sba h\u00edv tov\u00e1bb. Sokat seg\u00edt az \u00e1tl\u00e1that\u00f3s\u00e1gban a sz\u00ednk\u00f3dol\u00e1s : minden szolg\u00e1ltat\u00e1s adott sz\u00ednet kap, az \u00f6sszes spanje ugyanolyan sz\u00ednnel jelenik meg! A spanek alapj\u00e1n l\u00e1tjuk, hogy az alkalmaz\u00e1s hogyan szolg\u00e1l ki egy k\u00e9r\u00e9st: A frontend szolg\u00e1ltat\u00e1s a k\u00fcls\u0151 HTTP GET k\u00e9r\u00e9st a /dispatch v\u00e9gpontj\u00e1n\u00e1l kapja meg. A frontend szolg\u00e1ltat\u00e1s HTTP GET k\u00e9r\u00e9st k\u00fcld az customer szolg\u00e1ltat\u00e1s customer v\u00e9gpontj\u00e1nak (ha \u00f6sszecsukjuk a sz\u00fcl\u0151 span sor\u00e1t, akkor ezt sor elej\u00e9n a frontend -> customer ny\u00edl is mutatja). Az customer v\u00e9grehajt egy SQL SELECT utas\u00edt\u00e1st a MySQL h\u00edv\u00e1s\u00e1val. Az eredm\u00e9nyek visszaker\u00fclnek a frontend szolg\u00e1ltat\u00e1shoz. Ezut\u00e1n a frontend szolg\u00e1ltat\u00e1s RPC k\u00e9r\u00e9st ind\u00edt ( Driver :: findNearest m\u0171velet) a driver szolg\u00e1ltat\u00e1shoz. An\u00e9lk\u00fcl, hogy alaposabban belem\u00e9lyedn\u00e9nk a nyomk\u00f6vet\u00e9si r\u00e9szletekbe, nem tudjuk megmondani, hogy mely RPC keretrendszert haszn\u00e1lj\u00e1k a k\u00e9r\u00e9sek, de feltehetj\u00fck, hogy nem HTTP (val\u00f3j\u00e1ban TChannel \u00fatj\u00e1n k\u00e9sz\u00fcl). A driver szolg\u00e1ltat\u00e1s sz\u00e1mos h\u00edv\u00e1st kezdem\u00e9nyez a Redis fel\u00e9. A h\u00edv\u00e1sok k\u00f6z\u00fcl n\u00e9h\u00e1ny piros felki\u00e1lt\u00f3jel ikonnal van megjel\u00f6lve, jelezve a hib\u00e1kat. Ezut\u00e1n a frontend szolg\u00e1ltat\u00e1s egy sor HTTP GET k\u00e9r\u00e9st int\u00e9z a route szolg\u00e1ltat\u00e1s route v\u00e9gpontj\u00e1hoz. V\u00e9g\u00fcl, a frontend szolg\u00e1ltat\u00e1s visszaadja az eredm\u00e9nyt a k\u00fcls\u0151 h\u00edv\u00f3nak. Megjegyz\u00e9s: az oldal tetej\u00e9n lev\u0151 id\u0151ablakban eg\u00e9rrel egy id\u0151szeletet kijel\u00f6lve az adott id\u0151szeletre \"nagy\u00edthatunk\" r\u00e1. Span r\u00e9szletes n\u00e9zet: Egy sorra kattintva az adott span, \"kiny\u00edlik\", magass\u00e1ga megn\u0151, tov\u00e1bbi adatokat mutatva a spanr\u0151l. Nyissunk le az els\u0151 spant: l\u00e1tjuk a hozz\u00e1rendelt Tag -eket. Ezek kulcs-\u00e9rt\u00e9k p\u00e1rok a spanhez r\u00f6gz\u00edtveve (b\u00e1rmilyen kulcs-\u00e9rt\u00e9k p\u00e1r lehet). Pl. k\u00fcl\u00f6n\u00f6sen informat\u00edv a http.url , http.method , \u00e9s a http.status_code . span.kind : ha a \"h\u00edvott\" oldalon vagyunk, akkor a \"server\" \u00e9rt\u00e9kkel ker\u00fcl r\u00f6gz\u00edt\u00e9sre. A h\u00edv\u00f3 oldalon \"client\" \u00e9rt\u00e9kkel r\u00f6gz\u00edtett. sampler.type : \"const\" - jelzi, hogy minden m\u0171velet/span r\u00f6gz\u00edtend\u0151, a mintav\u00e9telez\u00e9s sor\u00e1n nem doband\u00f3 el semmi. A mysql lek\u00e9rdez\u00e9shez tartoz\u00f3 span (5. sor) tag-ben tartalmazza az SQL parancsot. Nyissunk le egy piros felki\u00e1llt\u00f3jellel dekor\u00e1lt spant. A tag-ek k\u00f6z\u00f6tt szerepel az error = true : ez a tag haszn\u00e1land\u00f3 hib\u00e1k jelz\u00e9s\u00e9re. B\u00e1rmilyen egy\u00e9ni adatot hozz\u00e1f\u0171zhet\u00fcnk tag-k\u00e9nt a span-ekhez. Erre p\u00e9lda a redis FindDriverIDs eset\u00e9n a param.location = 728,326 , vagy Redis GetDriver eset\u00e9n a param.driverID=T707027C . A \"szabv\u00e1nyos\" span tag-ekr\u0151l, illetve a r\u00f6videsen t\u00e1rgyal\u00e1sra ker\u00fcl\u0151 log \"szabv\u00e1nyos\" mez\u0151ir\u0151l itt tal\u00e1lunk le\u00edr\u00e1st: https://github.com/opentracing/specification/blob/master/semantic_conventions.md#span-tags-table (nyissuk meg az oldalt \u00e9s n\u00e9zz\u00fcnk r\u00e1 a t\u00e1bl\u00e1zatra). Az OpenTracing nem k\u00f6ti ki a tag neveket, de mindenk\u00e9ppen c\u00e9lszer\u0171 a t\u00e1bl\u00e1zat aj\u00e1nl\u00e1sait k\u00f6vetni: ha nem tessz\u00fck, a trace/span megjelen\u00edt\u0151 eszk\u00f6z\u00f6k nem tudnak szemantik\u00e1t t\u00e1rs\u00edtani hozz\u00e1 (pl. error eset\u00e9n speci\u00e1lis megjelent\u00e9s). Megjegyz\u00e9s: ha megn\u00e9zz\u00fck, a mysql \u00e9s redis spanek kliens odaliak (span.kind = client), ezek nem az adatt\u00e1rol\u00f3b\u00f3l erednek. A dem\u00f3alkalmaz\u00e1sban az adatkezel\u0151k szimul\u00e1ltak, de jellemz\u0151en a val\u00f3s adatt\u00e1rol\u00f3k sem instrument\u00e1ltak: ez esetben t\u00e1rol\u00f3hoz val\u00f3 hozz\u00e1f\u00e9r\u00e9st kliens oldalon vegy\u00fck k\u00f6rbe egy \u00faj spannel, \u00e9s ehhez csapjuk hozz\u00e1 az informat\u00edv tag-eket (pl. SQL parancs, Redis m\u0171velet \u00e9s param\u00e9terek, stb.). A spanekhez Log bejegyz\u00e9sek is tartozhatnak. Keress\u00fcnk meg p\u00e1rat: A span r\u00e9szletes n\u00e9zet\u00e9ben a Logs szekci\u00f3 alatt tal\u00e1lhat\u00f3k. A span cs\u00edkj\u00e1n is megjelennek a logbejegyz\u00e9sek v\u00e9kony f\u00fcgg\u0151leges vonallal, az egeret f\u00f6l\u00e9 h\u00fazva tooltipben r\u00e9szletes inform\u00e1ci\u00f3t kapunk az adott logbejegyz\u00e9sr\u0151l. A Log bejegyz\u00e9seknek van id\u0151b\u00e9lyege, plusz tetsz\u0151leges, debugol\u00e1st seg\u00edt\u0151 kulcs-\u00e9rt\u00e9k p\u00e1rok tartozhatnak hozz\u00e1. A \"szabv\u00e1nyosakat\" itt tekinthetj\u00fck meg: https://github.com/opentracing/specification/blob/master/semantic_conventions.md#log-fields-table (n\u00e9zz\u00fcnk r\u00e1 a t\u00e1bl\u00e1zatra). A legfontosabb az event kulcs. N\u00e9zz\u00fck meg az egyik hib\u00e1s redis GetDriver m\u0171velet logj\u00e1t. L\u00e1tjuk, hogy redis timeout t\u00f6rt\u00e9nt, a driver_id is napl\u00f3z\u00e1sra ker\u00fclt.","title":"Trace-ek \u00e9s spanek megjelen\u00edt\u00e9se"},{"location":"DevOps/Nyomkovetes/#kontextusba-helyezett-logok","text":"Eg\u00e9sz j\u00f3l l\u00e1tjuk, hogyan \u00e9p\u00fcl fel az alkalmaz\u00e1s. Tov\u00e1bbi k\u00e9rd\u00e9sekre keress\u00fck a v\u00e1laszt, pl.: mi\u00e9rt h\u00edvja a frontend a customer szolg\u00e1ltat\u00e1s /customer v\u00e9gpontj\u00e1t? Pr\u00f3b\u00e1lhatn\u00e1nk a szolg\u00e1ltat\u00e1sok logjaib\u00f3l megtudni: Ez sokszor nagyon neh\u00e9z Ha sok felhaszn\u00e1l\u00f3i k\u00e9r\u00e9s kiszolg\u00e1l\u00e1sa t\u00f6rt\u00e9nik egyszerre p\u00e1rhuzamosan, szinte lehetetlen kih\u00e1mozni, mi tartozik egy adott felhaszn\u00e1l\u00f3i k\u00e9r\u00e9shez. Helyette n\u00e9zz\u00fck a trace renszer \u00e1ltal begy\u0171jt\u00f6tt (span-ekhez kapcsol\u00f3d\u00f3) logbejegyz\u00e9seket. N\u00e9zz\u00fck meg a HTTP GET /dispatch spanhez kapcsol\u00f3d\u00f3 logokat (18 bejegyz\u00e9s lesz). A t\u00f6bbi k\u00e9r\u00e9st\u0151l izol\u00e1ltan, j\u00f3l \u00e1ttekinthet\u0151 m\u00f3don, az adott trace \u00e9s span kontextus\u00e1ba helyezve l\u00e1tjuk a logbejegyz\u00e9seket! Megjegyz\u00e9s: Valami furcsa okb\u00f3l kifoly\u00f3an a mintaalkalmaz\u00e1s t\u00f6bb helyen is el\u00e9g szeg\u00e9nyesen napl\u00f3zza az inform\u00e1ci\u00f3kat. Pl. a Found customer nem \u00edrja ki, milyen adatok ker\u00fcltek lek\u00e9rdez\u00e9sre (customer koordin\u00e1t\u00e1k): pedig itt pont seg\u00edten\u00e9 a meg\u00e9rt\u00e9st, mert ezen koordin\u00e1t\u00e1k \u00e1ltal meghat\u00e1rozott ponthoz k\u00e9pest keresi a k\u00f6zelben lev\u0151 vezet\u0151ket a FindNearestDriver m\u0171veletben.","title":"Kontextusba helyezett logok"},{"location":"DevOps/Nyomkovetes/#span-tasgs-vs-logs","text":"Mikor haszn\u00e1ljunk tageket, \u00e9s mikor logokat? Alapelv: A tag olyan inform\u00e1ci\u00f3 r\u00f6gz\u00edt\u00e9s\u00e9re val\u00f3, mely a span eg\u00e9sz\u00e9hez tartozik. A log id\u0151b\u00e9lyeggel rendelkez\u0151 esem\u00e9nyek r\u00f6gz\u00edt\u00e9s\u00e9re val\u00f3.","title":"Span Tasgs vs. Logs"},{"location":"DevOps/Nyomkovetes/#kesleltetesek-okainak-felderitese","text":"Vizsg\u00e1ljuk meg az alkalmaz\u00e1s teljes\u00edtm\u00e9ny karakterisztik\u00e1it. Trace-ek alap\u00e1n ezt l\u00e1tjuk: Az \u00fcgyf\u00e9ladatok lek\u00e9rdez\u00e9se ( customer szolg\u00e1ltat\u00e1s) a kritikus \u00fatvonalon van, mert ez adja vissza az \u00fcgyf\u00e9l koordin\u00e1t\u00e1it. A driver szolg\u00e1ltat\u00e1s lek\u00e9rdezi az \u00fcgyf\u00e9l k\u00f6zel\u00e9ben lev\u0151 10 j\u00e1rm\u0171vezet\u0151t, majd egyes\u00e9vel SORBAN EGYM\u00c1S UT\u00c1N lek\u00e9rdezi ezek adatait a Redist\u0151l: ezt mutatja a redis GetDriver l\u00e9pcs\u0151zetes mint\u00e1ja. Ezt k\u00f6vet\u0151en a 10 \u00fatvonalsz\u00e1m\u00edt\u00e1s ( route szolg\u00e1ltat\u00e1s) m\u0171velete nem is szekvenci\u00e1lis \u00e9s nem is teljesen p\u00e1rhuzamos. Azt l\u00e1tjuk, hogy maximum h\u00e1rom k\u00e9r\u00e9s tud p\u00e1rhuzamosan futni, \u00e9s amint ezekb\u0151l egy v\u00e9get \u00e9r, akkor indul a k\u00f6vetkez\u0151. Ez arra utal, hogy itt egy h\u00e1rmas v\u00e9grehajt\u00f3 pool futtatja a m\u0171veleteket. Vizsg\u00e1ljuk meg, mi t\u00f6rt\u00e9nik, ha sz\u00e1mos p\u00e1rhuzamos k\u00e9r\u00e9st futtatunk: A HotROD alkalmaz\u00e1sban gyorsan kattintsunk kb. 20-szor valamelyik gombon. A HotROD alkalmaz\u00e1s fel\u00fclet\u00e9n is j\u00f3l l\u00e1that\u00f3, hogy a k\u00e9r\u00e9sek kiszolg\u00e1l\u00e1sa belassul, a latency 800 ms helyett 2000 ms k\u00f6rny\u00e9ke lesz. A Jaeger alkalmaz\u00e1sban navig\u00e1ljunk vissza a nyit\u00f3oldalra, friss\u00edts\u00fck a trace list\u00e1t (Find traces gomb), keress\u00fck ki \u00e9s nyissuk meg az egyik leghosszabb lefut\u00e1s\u00fa trace-t: a trace list\u00e1ban ehhez tartozik a leghosszabb ci\u00e1nk\u00e9k sz\u00edn\u0171 s\u00e1v. A trace-t m\u00e1sk\u00e9nt is kikereshetj\u00fck: a HotROD oldal logj\u00e1ban l\u00e1tszik, hogy mely j\u00e1rm\u0171vezet\u0151t rendelte a rendszer a legnagyobb duration param\u00e9ter\u0171 sorn\u00e1l a k\u00e9r\u00e9shez (T795664C vagy egy hasonl\u00f3 form\u00e1tum\u00fa sztring). A Jaeger nyit\u00f3oldal\u00e1n a sz\u0171r\u0151paneleben a \"Tags\" mez\u0151be \u00edrjuk be ezt: \"driver:T716217C\" \u00e9s aktiv\u00e1ljuk a sz\u0171r\u00e9st. Az\u00e9rt tal\u00e1lja meg a trace-t, mert az egyik span egyik logj\u00e1ban szerepel a driver:T716217C kulcs-\u00e9rt\u00e9k p\u00e1r. (Ha meg akarjuk n\u00e9zni: a legels\u0151 spant lenyitva a spanhez tartoz\u00f3 utols\u00f3 log tartalmazza). Vizsg\u00e1ljuk meg a trace-t: a legszembe\u00f6tl\u0151bb k\u00fcl\u00f6nbs\u00e9g a kor\u00e1bbi, gyors trace-ekhez k\u00e9pest, a mysql k\u00e9r\u00e9s lass\u00fa kiszolg\u00e1l\u00e1sa. Pr\u00f3b\u00e1ljuk meg kital\u00e1lni, mi\u00e9rt: Nyissuk le a mysql SQL SELECT spant. K\u00e9t logbejegyz\u00e9s tartozik hozz\u00e1. Ezekb\u0151l egy\u00e9rtelm\u0171en kider\u00fcl, hogy az id\u0151 nagy r\u00e9sz\u00e9t lock-ra v\u00e1rakoz\u00e1ssal t\u00f6lt\u00f6te a k\u00e9r\u00e9s. Az els\u0151 log-b\u00f3l m\u00e9g az is kider\u00fcl, mely k\u00e9r\u00e9sek tartott\u00e1k fel (Waiting for lock behind 4 transactions\"), ezeket meg is neves\u00edti, pl. : [8221-10 8221-11 8221-12 8221-13] . Ezek a k\u00e9r\u00e9s azonos\u00edt\u00f3k a HotROD frontendr\u0151l j\u00f6nnek, n\u00e9zz\u00fck meg \u0151ket a fel\u00fcleten! Ami itt igaz\u00e1n izgalmas: honnan tudja a mysql \"kliens\", hogy mik a k\u00e9r\u00e9s azonos\u00edt\u00f3k, hogy jut el hozz\u00e1? Param\u00e9terben NEM passzolja v\u00e9gig a rendszer a h\u00edv\u00e1si l\u00e1ncon (pl. a HTTP GET customer k\u00e9r\u00e9sek param\u00e9ter\u00e9ben sem szerepel, megn\u00e9zhetj\u00fck a span-eket). A \"var\u00e1zslat\" m\u00f6g\u00f6tt az OpenTracing baggage koncepci\u00f3ja \u00e9s mechanizmusa \u00e1ll. Maga az elosztott trace az\u00e9rt tud megval\u00f3sulni, mert a OpenTracing instrument\u00e1l\u00e1s gondoskodik arr\u00f3l, hogy a k\u00e9r\u00e9sekhez kapcsol\u00f3d\u00f3 bizonyos metaadatok sz\u00e1lak, folyamatok \u00e9s sz\u00e1m\u00edt\u00f3g\u00e9pek k\u00f6z\u00f6tt propag\u00e1l\u00f3djanak \u00e9s minden a k\u00e9r\u00e9s kiszolg\u00e1l\u00e1s\u00e1ban r\u00e9szt vev\u0151 szerepl\u0151h\u00f6z eljussanak. Ilyen a trace id \u00e9s a span id is. Egy m\u00e1sik ilyen metainform\u00e1ci\u00f3 a baggage. Ez egy \u00e1ltal\u00e1nos kulcs-\u00e9rt\u00e9k p\u00e1r t\u00e1rol\u00f3. A p\u00e9ld\u00e1nkban a JavaScript UI beteszi a k\u00e9r\u00e9s azonos\u00edt\u00f3t a baggagebe, \u00e9s ez a k\u00e9r\u00e9s kiszolg\u00e1l\u00e1sa sor\u00e1n minden szerepl\u0151h\u00f6z eljut, an\u00e9lk\u00fcl, hogy explicit param\u00e9terekben kellene kezelni. Az OpenTracing instrument\u00e1ci\u00f3 gondoskodik r\u00f3la, pl. HTTP fejl\u00e9cbe teszi a HTTP k\u00e9r\u00e9sek kiszolg\u00e1l\u00e1sa sor\u00e1n). Nagyon hat\u00e9kony \u00e9s hasznos eszk\u00f6z! A p\u00e9ld\u00e1nkban lehet\u0151v\u00e9 teszi, hogy a k\u00e9r\u00e9sazonos\u00edt\u00f3 alapj\u00e1n kikeress\u00fck \u00e9s analiz\u00e1ljuk azokat a trace-eket, melyek feltartj\u00e1k a k\u00e9r\u00e9s\u00fcnk kiszolg\u00e1l\u00e1s\u00e1t. A val\u00f3 \u00e9letben is gyakori probl\u00e9ma: egy \u00fcgyf\u00e9l k\u00e9r\u00e9s feltart/belass\u00edt sz\u00e1mos m\u00e1sikat. Lehet\u0151s\u00e9g\u00fcnk van ezen k\u00e9r\u00e9sek megtal\u00e1l\u00e1s\u00e1ra \u00e9s analiz\u00e1l\u00e1s\u00e1ra. A HotRod p\u00e9ld\u00e1ban nincs igazi Mysql adatb\u00e1zis, csak szimul\u00e1lja a rendszer. Itt l\u00e1that\u00f3 a mesters\u00e9gesen szigor\u00edtott z\u00e1rol\u00e1s (pl. egy rosszul konfigur\u00e1lt DB connection poolt szimul\u00e1lva): https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/services/customer/database.go , ezen bel\u00fcl az if !config.MySQLMutexDisabled sor k\u00f6rny\u00e9k\u00e9t n\u00e9zz\u00fck. Szimul\u00e1ljuk a detekt\u00e1lt probl\u00e9ma jav\u00edt\u00e1s\u00e1t. A forr\u00e1sk\u00f3dhoz nem fogunk ny\u00falni, a HotROD alkalmaz\u00e1st kell speci\u00e1lis command line argumentummal futtatni ahhoz, hogy kikapcsoljuk a mesters\u00e9gesen induk\u00e1lt szigor\u00fa z\u00e1rol\u00e1st, illetve ezen fel\u00fcl a mesters\u00e9ges k\u00e9sleltet\u00e9s m\u00e9rt\u00e9k\u00e9t is cs\u00f6kkenteni fogjuk: A docker-compose.yml-ben a hotrod szolg\u00e1ltat\u00e1s command line param\u00e9tereit b\u0151v\u00edts\u00fck a \"-M\" kapcsol\u00f3val, ehhez egy sort kell m\u00f3dos\u00edtani, a command: ut\u00e1n [] k\u00f6z\u00f6tt ez kell \u00e1lljon: \"-M\", \"-D\", \"100ms\", \"all\" Ments\u00fck el a f\u00e1jlt. Tegy\u00fck fel, hogy production k\u00f6rnyezetben vagyuk, csak a m\u00f3dosult szolg\u00e1ltat\u00e1st (eset\u00fcnkben HotROD) k\u00edv\u00e1njuk \u00fajraind\u00edtani, a t\u00f6bbit (eset\u00fcnkben Jaeger) nem. \u00cdgy nem adjuk ki a docker compose down parancsot! Egy \u00faj command line ablakban navig\u00e1ljunk el a docker-compose.yml-t tartalmaz\u00f3 mapp\u00e1ba ( c:\\work\\<saj\u00e1tn\u00e9v> ). Futtassuk a k\u00f6vetkez\u0151 parancsot: docker-compose up -d . A '-d h\u00e1tt\u00e9rben futtat, az up parancs pedig csak a m\u00f3dosult kont\u00e9nereket \u00e1ll\u00edtja le \u00e9s ind\u00edtja \u00fajra. A kimeneten ez j\u00f3l k\u00f6vethet\u0151. K\u00f6zben a kor\u00e1bbi, a kont\u00e9nerekre m\u00e9g mindig r\u00e1csatolt command promptunkban l\u00e1tszik, hogy \u00fajraindult a HotROD szolg\u00e1ltat\u00e1s, 'fix: disabling db connection mutex' \u00fczemm\u00f3dban. Tesztelj\u00fck a jav\u00edtott megold\u00e1st: gener\u00e1ljunk kb. 20 p\u00e1rhuzamos k\u00e9r\u00e9st, ellen\u0151rizz\u00fck a duration-t: egyik sem megy 2000ms k\u00f6rny\u00e9k\u00e9re (kb. 900 ms k\u00f6rny\u00e9k\u00e9re k\u00faszik csak fel), \u00e9s a trace-ekben l\u00e1tsz\u00f3dik, hogy a mysql span-ek hossza 100 ms k\u00f6r\u00fcl marad.","title":"K\u00e9sleltet\u00e9sek okainak felder\u00edt\u00e9se"},{"location":"DevOps/Nyomkovetes/#tovabbi-teljesitmeny-optimalizacio","text":"Azt l\u00e1tjuk, hogy m\u00edg kor\u00e1bban a route szolg\u00e1ltat\u00e1s h\u00edv\u00e1sok k\u00f6z\u00fcl h\u00e1rom futott p\u00e1rhuzamosan, most m\u00e1r \u00e1ltal\u00e1ban csak egy fut egyszerre. S\u0151t, olyan id\u0151szakok is vannak, amikor egy sem fut. Ebb\u0151l arra k\u00f6vetkeztet\u00fcnk, hogy a v\u00e9grehajt\u00f3 goroutine m\u00e1s goroutine-okkal verseng k\u00f6z\u00f6s er\u0151forr\u00e1s\u00e9rt (vagyis k\u00f6z\u00f6s a v\u00e9grehajt\u00f3 pool). A probl\u00e9ma helye a frontend szolg\u00e1ltat\u00e1sban val\u00f3sz\u00edn\u0171, mivel a route spanek a frontend spanek gyerekei, \u00edgy azt val\u00f3sz\u00edn\u0171s\u00edtj\u00fck, hogy a route m\u0171veleteket a frontend m\u0171velete h\u00edvja. Ha van id\u0151nk, n\u00e9zz\u00fcnk r\u00e1 a frontend k\u00f3dj\u00e1ra: https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/services/frontend/best_eta.go , itt a getRoutes m\u0171velet az \u00e9rdekes. Azt l\u00e1tjuk, hogy egy poolt haszn\u00e1l az \u00fatvonalak megtervez\u00e9s\u00e9hez (a legkor\u00e1bbi v\u00e1rhat\u00f3 \u00e9rkez\u00e9si idej\u0171 \u00fatvonalat keress\u00fck). Azt is l\u00e1tjuk, hogy ha a pool m\u00e9rete kell\u0151en nagy, ak\u00e1r minden FindRoute m\u0171velet futhatna p\u00e1rhuzamosan. Arra k\u00f6vetkeztet\u00fcnk, hogy a pool m\u00e9rete nem kell\u0151en nagy (a kor\u00e1bbi tesztjeink alapj\u00e1n a pool m\u00e9ret\u00e9t 3-asnak v\u00e9lj\u00fck). N\u00f6velj\u00fck meg a pool m\u00e9ret\u00e9t 100-ra: A docker-compose.yml f\u00e1jlban v\u00e1ltoztassunk a command line param\u00e9tereken: \"-M\", \"-D\", \"100ms\", \"-W\", \"100\", \"all\" legyen az \u00faj halmaz, ments\u00fck el a v\u00e1ltoztat\u00e1st Ind\u00edtsuk \u00fajra a hotrod szolg\u00e1ltat\u00e1st: docker-compose up -d Tesztelj\u00fck a jav\u00edtott megold\u00e1st: gener\u00e1ljunk kb. 20 p\u00e1rhuzamos k\u00e9r\u00e9st \u00e9s ellen\u0151rizz\u00fck az egyik friss trace-t, a route spanek p\u00e1rhuzamosan kell fussanak. A driver szolg\u00e1ltat\u00e1son is lehetne optimaliz\u00e1lni, a Redis GetDriver lek\u00e9rdez\u00e9sek szekvenci\u00e1lisak, ezzel most nem foglalkozunk.","title":"Tov\u00e1bbi teljes\u00edtm\u00e9ny optimaliz\u00e1ci\u00f3"},{"location":"DevOps/Nyomkovetes/#eroforrashasznalat-attributalt-merese-baggage-segitsegevel","text":"Gyakran mer\u00fcl fel arra \u00fczleti ig\u00e9ny, hogy az er\u0151forr\u00e1s (pl. CPU) haszn\u00e1latot valamilyen magasabb szint\u0171 param\u00e9ter alapj\u00e1n m\u00e9rj\u00fck \u00e9s attribut\u00e1ljuk . A p\u00e9ld\u00e1nkban a route szolg\u00e1ltat\u00e1sban az \u00fatvonalkeres\u00e9s relat\u00edve CPU intenz\u00edv m\u0171velet, j\u00f3 lenne m\u00e9rni, hogy \u00fcgyfelenk\u00e9nt (customer) mennyi CPU id\u0151t haszn\u00e1l. Ugyanakkor a route szolg\u00e1ltat\u00e1s a f\u00fcgg\u0151s\u00e9gi gr\u00e1fban m\u00e9lyen van, itt m\u00e1r nincs inform\u00e1ci\u00f3 az \u00fcgyf\u00e9lr\u0151l, akinek kapcs\u00e1n az \u00fatvonalkeres\u00e9s t\u00f6rt\u00e9nik. Csak a m\u00e9r\u00e9s \u00e9rdek\u00e9ben egy explicit customer id param\u00e9tert bevezetni a route szolg\u00e1ltat\u00e1s API-j\u00e1n rossz tervez\u0151i d\u00f6nt\u00e9s lenne. Itt l\u00e9p k\u00e9pbe a tracing, illetve annak baggage mechanizmusa. Egy adott trace kontextus\u00e1ban m\u00e1r tudjuk, mely \u00fcgyf\u00e9l sz\u00e1m\u00e1ra t\u00f6rt\u00e9nik az \u00fatvonalkeres\u00e9s. A baggage pedig lehet\u0151s\u00e9get ny\u00fajt arra, hogy a szolg\u00e1ltat\u00e1sok k\u00f3dj\u00e1nak m\u00f3dos\u00edt\u00e1sa n\u00e9lk\u00fcl transzparens m\u00f3don tov\u00e1bb\u00edtsunk a trace-hez kapcsol\u00f3d\u00f3 metaadatokat a szolg\u00e1ltat\u00e1sok k\u00f6z\u00f6tt. Eset\u00fcnkban ez a customer id lesz, de ilyen lehet egy k\u00e9r\u00e9s/session/felhaszn\u00e1l\u00f3 azonos\u00edt\u00f3 is. A fenti koncepci\u00f3 demonstr\u00e1l\u00e1s\u00e1ra a route szolg\u00e1ltat\u00e1s olyan k\u00f3dot tartalmaz, mely m\u00e9ri az \u00fatvonalsz\u00e1m\u00edt\u00e1s idej\u00e9t \u00e9s a Go expvar metrika \"kezel\u0151\" standard library package seg\u00edts\u00e9g\u00e9vel \u00f6sszegy\u0171jti, nyilv\u00e1ntartja \u00e9s lek\u00e9rdezhet\u0151v\u00e9 teszi azt. A k\u00f3d itt tal\u00e1lhat\u00f3: https://github.com/jaegertracing/jaeger/blob/master/examples/hotrod/services/route/stats.go . Alapelve: A routeCalcByCustomer \u00e9s routeCalcBySession v\u00e1ltoz\u00f3k egy-egy map t\u00edpus\u00fa metrik\u00e1t defini\u00e1lnak, adott kulcsokkal. A stats egy strukt\u00fara t\u00f6mb, k\u00e9t elem\u0171. A strukt\u00fara objektumokban egy expvar metrika \u00e9s egy baggage kulcs tal\u00e1lhat\u00f3. Az updateCalcStats lek\u00e9rdezi az aktu\u00e1lis spant, egy cikulsban v\u00e9gigmegy a stats -ban t\u00e1rolt strukt\u00fara elemeken: minden elemre lek\u00e9rdezi a baggage-b\u0151l a stukt\u00farban t\u00e1rolt kulcs alapj\u00e1n a baggage elemet (\"customer\" \u00e9s \"sesion\"), majd ez \u00e9s a fut\u00e1si id\u0151 alapj\u00e1n friss\u00edti a strukt\u00far\u00e1ban t\u00e1rolt metrik\u00e1t. Az expvar metrik\u00e1k lek\u00e9rdezhet\u0151k a fut\u00f3 szolg\u00e1ltat\u00e1st\u00f3l a 8083-as porton: A docker-compose.yml f\u00e1jlban a hotrod szolg\u00e1ltat\u00e1sn\u00e1l publik\u00e1ljuk a 8083-os portot (vegy\u00fck fel a \"8083:8083\"-at a \"ports:\" al\u00e1), ments\u00fck el, a docker-compose up -d -vel friss\u00edts\u00fck a fut\u00f3 kont\u00e9nert. A HotROD frontendden gener\u00e1ljunk j\u00f3p\u00e1r k\u00e9r\u00e9st: az els\u0151 gombon kattintsunk sokat, a m\u00e1sodikon p\u00e1rat, a t\u00f6bbin ne kattintsunk. B\u00f6ng\u00e9sz\u0151b\u0151l kapcsol\u00f3djunk a hotrod kont\u00e9ner expvar szolg\u00e1ltat\u00e1s\u00e1hoz \u00e9s k\u00e9rdezz\u00fck le a metrik\u00e1kat: http://localhost:8083/debug/vars Keress\u00fcnk r\u00e1 sz\u00f6veg szerint a route.calc.by.customer.sec \u00e9s az alatta lev\u0151 route.calc.by.session.sec kulcsokra \u00e9s tekints\u00fck meg a metrik\u00e1k \u00e9rt\u00e9k\u00e9t. Az al\u00e1bbihoz hasonl\u00f3 kimenetet kapunk: \"route.calc.by.customer.sec\": {\"Amazing Coffee Roasters\": 1.0790000000000002, \"Japanese Desserts\": 1.5280000000000002, \"Rachel's Floral Designs\": 19.54500000000001, \"Trom Chocolatier\": 0.46199999999999997}, \"route.calc.by.session.sec\": {\"8221\": 22.61399999999999},","title":"Er\u0151forr\u00e1shaszn\u00e1lat attribut\u00e1lt m\u00e9r\u00e9se baggage seg\u00edts\u00e9g\u00e9vel"},{"location":"DevOps/Nyomkovetes/#kodinstrumentalas","text":"Felmer\u00fcl a k\u00e9rd\u00e9s, mennyit kellett a k\u00f3d instrument\u00e1l\u00e1s\u00e1n dolgozni a HotROD alkalmaz\u00e1s fejleszt\u00e9se sor\u00e1n. Meglep\u0151en keveset: Be kell konfigur\u00e1lni, \u00f6ssze kell k\u00f6tni Jaegerrel. Az intrument\u00e1l\u00e1s a kommunik\u00e1ci\u00f3t v\u00e9gz\u0151 librarykben van: a Go hoz rendelkez\u00e9sre \u00e1ll\u00f3 REST \u00e9s TChannel kommunik\u00e1ci\u00f3t t\u00e1mogat\u00f3 open source k\u00f6nyvt\u00e1rak instrument\u00e1ltak, ezzel nek\u00fcnk nem kell foglalkozni. A k\u00f3dban a Mysql \u00e9s Redis szimul\u00e1ci\u00f3 eset\u00e9n tal\u00e1lhat\u00f3 explicit instrument\u00e1l\u00e1s.","title":"K\u00f3dinstrument\u00e1l\u00e1s"},{"location":"DevOps/Nyomkovetes/#feladat-2-opentracing-es-jaeger-alapu-nyomkovetes-net-core-alapu-alkalmazas-eseten","text":"Kiindul\u00f3 projekt \u00e9s megold\u00e1s: https://github.com/bmeviauav42/nyomkovetes A feladat keret\u00e9ben .NET Core 3.0 k\u00f6rnyezetben dolgozunk, de az OpenTracing \u00e9s Jaeger .NET kliens k\u00f6nyvt\u00e1rak kor\u00e1bbi .NET Core verzi\u00f3kkal is haszn\u00e1lhat\u00f3k, nincs benn\u00fck semmi .NET Core 3.0 specifikus. Vannak kliens k\u00f6nyvt\u00e1rak, melyek azt v\u00e1rj\u00e1k, hogy a jaeger-agent a helyi g\u00e9pen (pl. ugyanabban a docker containerben) fut, az agenttel UDP protokollon t\u00f6rt\u00e9nik a kommunik\u00e1ci\u00f3. A tapasztalatok szerint sz\u00e1mos kliens k\u00f6nyvt\u00e1r tud m\u00e1s g\u00e9pen/containerben fut\u00f3 agenttel kommunik\u00e1lni, konfigur\u00e1lhat\u00f3 a kliensben az agent c\u00edme (host \u00e9s a port). Ez lehet\u0151v\u00e9 teszi a jaegertracing/all-in-one k\u00e9nyelmes haszn\u00e1lat\u00e1t, a szolg\u00e1ltat\u00e1sokat futtat\u00f3 kont\u00e9netben nem kell jaeger agentet telep\u00edteni. \u00c9les k\u00f6rnyezetben ez a megold\u00e1s korl\u00e1tozottan aj\u00e1nlott: az UDP megb\u00edzhat\u00f3an m\u0171k\u00f6dik localhost eset\u00e9n, de egy val\u00f3di h\u00e1l\u00f3zaton veszhetnek el csomagok. A legt\u00f6bb kliens k\u00f6nyvt\u00e1r azt is t\u00e1mogatja, hogy az agent kihagy\u00e1s\u00e1val, k\u00f6zvetlen a collectornak t\u00f6lt\u00e9njen az adatk\u00fcld\u00e9s, ak\u00e1r HTTP protokollon. A kommunik\u00e1ci\u00f3r\u00f3l sz\u00f3l\u00f3 gyakorlat n\u00e9mik\u00e9ppen tov\u00e1bbfejlesztett Order+Customer kiindul\u00e1si p\u00e9ld\u00e1j\u00e1t instrument\u00e1ljuk k\u00f6tj\u00fck \u00f6ssze Jaegerrel \u00e9s instrument\u00e1ljuk OpenTracing alapokon \u00e9s. Miben m\u00e1s a kiindul\u00f3 k\u00f3d, mint a kor\u00e1bbi kommunik\u00e1ci\u00f3s \u00f3r\u00e1n szerepl\u0151 kiindul\u00f3 gyakorlat? Repository mint\u00e1t haszn\u00e1l. A Catalog szolg\u00e1ltat\u00e1s Sqlite adatb\u00e1zisban t\u00e1rolja az adatokat (in-memory \u00fczemm\u00f3dra konfigur\u00e1ltan), indul\u00e1skori seed-et, vagyis tesztadat gener\u00e1l\u00e1st k\u00f6vet\u0151en. Ez \u00e9les k\u00f6rnyezetben nem haszn\u00e1lhat\u00f3, a c\u00e9lja mind\u00f6ssze a nyomk\u00f6vet\u00e9s demonstr\u00e1l\u00e1sa. A v\u00e1laszt\u00e1s az\u00e9rt esett az Sqlite-ra az Entity Framework In Memory Database-zel szemben, mert r\u00e9szletesebb nyomk\u00f6vet\u00e9si inform\u00e1ci\u00f3t szolg\u00e1ltat.","title":"Feladat 2 - OpenTracing \u00e9s Jaeger alap\u00fa nyomk\u00f6vet\u00e9s .NET Core alap\u00fa alkalmaz\u00e1s eset\u00e9n"},{"location":"DevOps/Nyomkovetes/#elokeszites-konfiguracio","text":"Kiindul\u00f3 l\u00e9p\u00e9sek: GitHub-r\u00f3l kl\u00f3nozzuk ki a kiindul\u00f3 solution-t: Hozzunk l\u00e9tre egy tracing mapp\u00e1t a c:\\work\\<saj\u00e1tn\u00e9v> munkak\u00f6nyv\u00e1trunkban \u00e9s ind\u00edtsunk egy command prompotot innen, futtasuk az al\u00e1bbi parancsot: git clone https://github.com/bmeviauav42/nyomkovetes Ind\u00edtsuk el VS alatt a szolg\u00e1ltat\u00e1sokat A b\u00f6ng\u00e9sz\u0151ben hibaoldal jelenik meg. Friss\u00edts\u00fck (ha kell t\u00f6bbsz\u00f6r is), a hiba elt\u0171nik. Az oka: az OrderService h\u00edvja a CatalogService-t, de az el\u0151sz\u00f6r m\u00e9g nem \u00e1llt fel teljesen, az Sqlite adatb\u00e1zis seed-el\u00e9se id\u0151t ig\u00e9nyel. Ha kev\u00e9s az id\u0151, n\u00e9zhetj\u00fck a feladat megold\u00e1s\u00e1t: ez a megoldas/1-konfig-es-beepitett-instrumentalas \u00e1gon tal\u00e1lhat\u00f3, az \u00e1g checkout l\u00e9p\u00e9sei: git fetch git checkout megoldas/1-konfig-es-beepitett-instrumentalas VS alatt startup projektnek \u00e1ll\u00edtsuk be a docker-compose projektet.","title":"El\u0151k\u00e9sz\u00edt\u00e9s, konfigur\u00e1ci\u00f3"},{"location":"DevOps/Nyomkovetes/#projekt-referenciak-felvetele","text":"A solution\u00fcnk t\u00f6bb projektb\u0151l/szolg\u00e1ltat\u00e1sb\u00f3l \u00e1ll. Mindegyiket hasonl\u00f3 m\u00f3don kell OpenTracing \u00e9s Jaeger vonatkoz\u00e1s\u00e1ban inicializ\u00e1lni, illetve ugyanarra a Jaeger \"kiszolg\u00e1l\u00f3ra\" kell r\u00e1k\u00f6tni. Ezt a k\u00f3dot ne copy-paste-tel szapor\u00edtsuk, hanem tegy\u00fck ki egy k\u00fcl\u00f6n megosztott k\u00f6nyvt\u00e1rba. Ez a kiindul\u00f3 megold\u00e1sban el\u0151 van k\u00e9sz\u00edtve, m\u00e1r van egy ilyen projekt (Msa.Comm.Lab.Shared), csak a projekt referenci\u00e1k nincsenek felv\u00e9ve. Tegy\u00fck ezt meg minden projektn\u00e9l, ahol a nyomk\u00f6vet\u00e9st haszn\u00e1lni akarjuk: Msa.Comm.Lab.Services.Catalog -> Msa.Comm.Lab.Shared projekt referencia felv\u00e9tele Msa.Comm.Lab.Services.Order -> Msa.Comm.Lab.Shared projekt referencia felv\u00e9tele","title":"Projekt referenci\u00e1k felv\u00e9tele"},{"location":"DevOps/Nyomkovetes/#opentracing-es-jaeger-kliens-konyvtarak","text":".NET Core k\u00f6rnyezetben az al\u00e1bbi NuGet csomagok haszn\u00e1lat\u00e1ra van sz\u00fcks\u00e9g: \"OpenTracing\" OpenTracing API a k\u00f3d instrument\u00e1l\u00e1s\u00e1hoz. https://github.com/opentracing/opentracing-csharp \"OpenTracing.Contrib.NetCore\" https://github.com/opentracing-contrib/csharp-netcore Nem k\u00f6telez\u0151, de .NET Core k\u00f6rnyezetben javasolt: an\u00e9lk\u00fcl tudunk a seg\u00edts\u00e9g\u00e9vel pl. REST h\u00edv\u00e1sokat trace-elni, hogy a k\u00f3dunkat instument\u00e1lni kellene. Be\u00e9p\u00fcl az ASP.NET-be, Entity Framework-be, bizonyos .NET Core BCL t\u00edpusokba,melyek k\u00f6z\u00fcl a legfontosabb a HttpClient. Minden .NET k\u00f6nyvt\u00e1rat, keretrendszert, stb.-t t\u00e1mogat, ami a .NET DiagnosticSource-t haszn\u00e1lja (minden Activity-hez spant k\u00e9sz\u00edt \u00e9s span log-ot az egy\u00e9b esem\u00e9nyekhez). Beregisztr\u00e1lja mag\u00e1t a Microsoft.Extensions.Logging rendszerbe, \u00e9s minden logba \u00edr\u00e1s eset\u00e9n span log-ot k\u00e9sz\u00edt, de csak akkor, ha van akt\u00edv span. F\u00fcgg\u0151s\u00e9gk\u00e9nt felteszi az OpenTracing package-et is! \"Jaeger\" https://github.com/jaegertracing/jaeger-client-csharp Jaeger .NET kliens k\u00f6nyvt\u00e1r Az Msa.Comm.Lab.Shared k\u00f6nyvt\u00e1rba m\u00e1r fel vannak v\u00e9ve ezek a NuGet f\u00fcgg\u0151s\u00e9gek (az OpenTracing csak k\u00f6zvetve, az OpenTracing.Contrib.NetCor e alatt), \u00edgy nek\u00fcnk nem kell megtenn\u00fcnk. A szolg\u00e1ltat\u00e1s projektekben nem fogunk els\u0151 k\u00f6rben k\u00f6zvetlen OpenTracing instrument\u00e1l\u00e1st v\u00e9gezni, csak haszn\u00e1ljuk a Msa.Comm.Lab.Shared k\u00f6nyvt\u00e1r egyetelen oszt\u00e1ly\u00e1t/m\u0171velet\u00e9t a szolg\u00e1ltat\u00e1sok konfigur\u00e1l\u00e1sakor: vegy\u00fck fel a Msa.Comm.Lab.Services.Catalog projekt Startup.ConfigureServices m\u0171velet\u00e9nek elej\u00e9re: // Registers and starts Jaeger (see Shared.JaegerServiceCollectionExtensions) // Also registers OpenTracing services . AddJaeger ( currentEnvironment ); A szolg\u00e1ltat\u00e1s projektekbe \u00edgy (egyel\u0151re legal\u00e1bbis) egyetlen OpenTracing/NuGet package-et sem vett\u00fcnk/vesz\u00fcnk fel, csak k\u00f6zvetett f\u00fcgg\u00e9s van! Tekints\u00fck \u00e1t a JaegerServiceCollectionExtensions.AddJaeger m\u0171veletet, a legfontosabbak: A DI kont\u00e9nerbe egy ITrace r implement\u00e1ci\u00f3t regisztr\u00e1lunk be singletonk\u00e9nt. Ez eset\u00fcnkben egy Jaeger tracer lesz, amit a p\u00e9ld\u00e1ban k\u00f6rnyezeti v\u00e1ltoz\u00f3k alapj\u00e1n inicializ\u00e1lunk (docker k\u00f6rnyezetben praktikus megk\u00f6zel\u00edt\u00e9s): Jaeger.Configuration.FromEnv(loggerFactory) A k\u00f6rnyezeti v\u00e1ltoz\u00f3k k\u00f6z\u00fcl a JAEGER_SERVICE_NAME , JAEGER_AGENT_HOST , JAEGER_AGENT_PORT \u00e9s JAEGER_SAMPLER_TYPE a fontosabbak. A GlobalTracer egy klasszikus sigleton hozz\u00e1f\u00e9r\u00e9st biztos\u00edt b\u00e1rhol a k\u00f3dban a tracer-hez, ezt is \u00e1ll\u00edtsuk be: GlobalTracer.Register(tracer) . Ez azon k\u00f3d sz\u00e1m\u00e1ra fontos, mely nem tud DI alapokon m\u0171k\u00f6dni. A v\u00e9g\u00e9n fontos az OpenTracing szolg\u00e1ltat\u00e1sok regisztr\u00e1l\u00e1sa is, eddig csak Jaegerrel foglalkoztunk: services.AddOpenTracing();","title":"OpenTracing \u00e9s Jaeger kliens k\u00f6nyvt\u00e1rak"},{"location":"DevOps/Nyomkovetes/#jaeger-szolgaltatas-beepitese-docker-composeyml-be","text":"Eg\u00e9sz\u00edts\u00fck ki a docker-compose.yml f\u00e1jlt (id\u0151hi\u00e1ny eset\u00e9n m\u00e1soljuk be az eg\u00e9szet): jaeger szolg\u00e1ltat\u00e1s felv\u00e9tele a m\u00e1r megl\u00e9v\u0151 k\u00e9t szolg\u00e1ltat\u00e1sn\u00e1l depends_on alatt jaeger megad\u00e1sa k\u00f6rnyezeti v\u00e1ltoz\u00f3k felv\u00e9tele (ez eet\u00fcnkben nem k\u00f6telez\u0151, mert ha nem adjuk meg, az Msa.Comm.Lab.Shared \u00e1ltal be\u00e1ll\u00edtott alap\u00e9rt\u00e9kek megfelel\u0151k) version : '3.7' services : msa.comm.lab.services.catalog : image : ${DOCKER_REGISTRY-}msacommlabservicescatalog build : context : . dockerfile : Msa.Comm.Lab.Services.Catalog/Dockerfile environment : - JAEGER_AGENT_HOST=jaeger - JAEGER_AGENT_PORT=6831 - JAEGER_SAMPLER_TYPE=const depends_on : - jaeger msa.comm.lab.services.order : image : ${DOCKER_REGISTRY-}msacommlabservicesorder build : context : . dockerfile : Msa.Comm.Lab.Services.Order/Dockerfile environment : - JAEGER_AGENT_HOST=jaeger - JAEGER_AGENT_PORT=6831 - JAEGER_SAMPLER_TYPE=const depends_on : - msa.comm.lab.services.catalog - jaeger jaeger : image : jaegertracing/all-in-one:latest ports : - \"16686:16686\" # For Jaeger web UI Tesztelj\u00fck a m\u0171k\u00f6d\u00e9st: Ind\u00edtsuk el a szolg\u00e1ltat\u00e1sokat VS alatt, friss\u00edts\u00fck p\u00e1rszor a b\u00f6ng\u00e9sz\u0151ablakot Jelen\u00edts\u00fck meg Jaeger UI-t: http://localhost:16686/ A sz\u0171r\u0151panelen v\u00e1lasszuk ki a Msa.Comm.Lab.Services.Order szolg\u00e1ltat\u00e1st, friss\u00edts\u00fck a megjelen\u00edtett trace.eket (Find traces gomb), v\u00e1lasszunk ki jobboldalt egy hib\u00e1val nem rendelkez\u0151 trace-t \u00e9s nyissuk meg. A r\u00e9szteles trace megjelen\u00edt\u0151ben l\u00e1tjuk span hierarchi\u00e1t: Http h\u00edv\u00e1sok \u00e9s DB ExecuteReader a m\u00e9ly\u00e9n. Az Action Msa.Comm.Lab.Services.Catalog.Controllers.ProductController/Get spanben sz\u00e1mos log van, t\u00f6bbek k\u00f6z\u00f6tt EF-h\u00f6z kapcsol\u00f3d\u00f3k is. Mindezt \u00fagy \u00e9rt\u00fck el, hogy semmif\u00e9le OpenTracing instrument\u00e1l\u00e1st nem v\u00e9gezt\u00fcnk a saj\u00e1t k\u00f3dunkban .","title":"Jaeger szolg\u00e1ltat\u00e1s be\u00e9p\u00edt\u00e9se docker-compose.yml-be"},{"location":"DevOps/Nyomkovetes/#kod-instrumentalas","text":"Checkoutoljuk ki Git-ben k\u00e9sz megold\u00e1st, a megoldas/1-konfig-es-beepitett-instrumentalas \u00e1gon tal\u00e1lhat\u00f3: git checkout megoldas/2-kod-instrumentalas","title":"K\u00f3d instrument\u00e1l\u00e1s"},{"location":"DevOps/Nyomkovetes/#egyszeru-naplozas-instr_log","text":"Itt m\u00e9g nem haszn\u00e1lunk OpenTracing specifikus instrument\u00e1l\u00e1st, az Microsoft.Extensions.Logging ILogger seg\u00edts\u00e9g\u00e9vel napl\u00f3zunk. Az Order szolg\u00e1ltat\u00e1s TestController oszt\u00e1lyt n\u00e9zz\u00fck DI-vel kap egy ILogger<TestController> objektumot. Ezt a Microsoft.Extensions.Logging be\u00e9p\u00edtve t\u00e1mogatja. A Get() m\u0171veletben a log.LogInformation h\u00edv\u00e1ssal napl\u00f3zunk. Struktur\u00e1lt napl\u00f3z\u00e1st haszn\u00e1lunk, az els\u0151 param\u00e9terben a {ProdCount} defini\u00e1l egy kulcsot, az \u00e9rt\u00e9ke a count param\u00e9ter lesz. Ez nemcsak mint string, hanem egy kulcs-\u00e9rt\u00e9k p\u00e1rk\u00e9nt is megjelenik a napl\u00f3z\u00e1s sor\u00e1n: a spanhez f\u0171z\u00f6tt log-ban lesz egy ilyen kulcs-\u00e9rt\u00e9k p\u00e1r. Futtassuk az alkalmaz\u00e1st, gener\u00e1ljunk p\u00e1r nem hib\u00e1s h\u00edv\u00e1st. Jager UI frontenden nyissunk meg egy nem hib\u00e1s trace-t. A fels\u0151 Find keres\u0151be \u00edrjuk be: ProdCount \u00e9s keress\u00fcnk r\u00e1. Azon spanek, ahol van tal\u00e1lat, s\u00e1rga sz\u00ednnel jelennek meg. Egy tal\u00e1latunk van. Nyissuk le a spant, \u00e9s keress\u00fck ki szemmel a sz\u00e1munkra \u00e9rdekes logbejegyz\u00e9st. Itt l\u00e1tjuk, hogy a logon megjelenik a ProdCount = 3 kulcs-\u00e9rt\u00e9k p\u00e1r, \u00edgy lehet(ne) erre is \u00e9rtelmesen keresni (a nyit\u00f3oldalon a trace keres\u0151ben igen, a trace r\u00e9szletes oldalon a span keres\u0151ben nem). Tipp: A span keres\u0151 egyel\u0151re el\u00e9g b\u00e9na: ha valamit nem tal\u00e1lunk, a jobb fels\u0151 sarokban lev\u0151 gombbal v\u00e1ltsunk JSON n\u00e9zetre \u00e9s keress\u00fcnk abban sz\u00f6veg szerint.","title":"Egyszer\u0171 napl\u00f3z\u00e1s (##Instr_Log)"},{"location":"DevOps/Nyomkovetes/#sajat-span-keszitese-taggeles-instr_createspan","text":"Saj\u00e1t spant k\u00e9sz\u00edt\u00fcnk. Itt m\u00e1r explicit OpenTracing API instument\u00e1l\u00e1st v\u00e9gz\u00fcnk. Ehhez \"logikailag\" fel kellene vegy\u00fck az \u00e9rintett projektben az OpenTracing NuGet package hivatkoz\u00e1st ( Jager \u00e9s OpenTracing.Contrib.NetCore nem kell, hiszen mi csak az API-t haszn\u00e1ljuk). Eset\u00fcnkben nem kell megtenni, mert az Msa.Comm.Lab.Shared projektre van referencia, aminek m\u00e1r van (k\u00f6zvetett) OpenTracing f\u00fcgg\u00e9se, a .NET Core 3.0 eset\u00e9ben ez m\u00e1r el\u00e9g a megfelel\u0151 m\u0171k\u00f6d\u00e9shez. Feladat : a Catalog szolg\u00e1ltat\u00e1s ProductController oszt\u00e1lyban a repository-hoz val\u00f3 hozz\u00e1r\u00e9s el\u0151tt cache-ben val\u00f3 keres\u00e9st szimul\u00e1lunk, ezt egy \u00faj span hat\u00f3k\u00f6r\u00e9ben trace-elj\u00fck. A ProductController f\u00fcgg\u0151s\u00e9ginjekt\u00e1l\u00e1ssal kap egy ITracer objektumot A Get(int id) -ban lev\u0151 k\u00f3dot \u00e9rtelmezz\u00fck \u00daj span l\u00e9trehoz\u00e1sa, param\u00e9terben m\u0171veletn\u00e9v Tag hozz\u00e1f\u0171z\u00e9se akt\u00edv spanhez Log esem\u00e9ny felv\u00e9tele akt\u00edv spanhez Futtasuk az alkalmaz\u00e1st, b\u00f6ng\u00e9sz\u0151ben egym\u00e1s ut\u00e1n k\u00e9rj\u00fck le az egyes term\u00e9kek adatait a Test szolg\u00e1ltat\u00e1s seg\u00edts\u00e9g\u00e9vel, a k\u00fcl\u00f6nb\u00f6z\u0151 k\u00f3d \u00e1gak tesztel\u00e9s\u00e9hez: https://localhost:44385/api/test/1 , cache hib\u00e1t gener\u00e1l https://localhost:44385/api/test/1 , nincs cache tal\u00e1lat https://localhost:44385/api/test/1 , van cache tal\u00e1lat A Jaeger UI seg\u00edts\u00e9g\u00e9vel vizsg\u00e1ljuk meg a h\u00e1rom trace-t","title":"Saj\u00e1t span k\u00e9sz\u00edt\u00e9se, taggel\u00e9s (##Instr_CreateSpan)"},{"location":"DevOps/Nyomkovetes/#baggage-hasznalata-instr_baggage","text":"A Catalog szolg\u00e1ltat\u00e1s ProductRepositor y oszt\u00e1lyban \u00edrjuk ki a felhaszn\u00e1l\u00f3nevet \u00e9s k\u00e9r\u00e9st azonos\u00edt\u00f3t Log-ba, melyet a p\u00e9ld\u00e1nkban az Order szolg\u00e1ltat\u00e1s gener\u00e1l. A megold\u00e1s elve: Nem szennyezz\u00fck az API-t, nem vessz\u00fck fel explicit param\u00e9terk\u00e9nt Helyette baggage -ben tov\u00e1bb\u00edtjuk. Pontosabban r\u00e1b\u00edzzuk az OpenTracing instrument\u00e1lt HttpClient-re (az bepakolja http headerbe a baggage tartalm\u00e1t). L\u00e9p\u00e9sek OrderService.TestController -t n\u00e9zz\u00fck meg, itt rt\u00f6rt\u00e9nik az akt\u00edv span baggage-\u00e9be a kulcs-\u00e9rt\u00e9k p\u00e1rok felv\u00e9tele (string-string). CatalogService.ProductRepository -t n\u00e9zz\u00fck meg, itt olvassuk ki az akt\u00edv span Bbaggage-\u00e9b\u0151l az \u00e9rt\u00e9keket. Ezeket logban hozz\u00e1\u00edrjuk az akt\u00edv span-hez. (Nagyobb \u00e9rtelme lenne pl. valamilyen countert/merik\u00e1t nyilv\u00e1ntartani ez alapj\u00e1n). Futtassuk (b\u00f6ng\u00e9sz\u0151ben https://localhost:44385/api/test ) Jaeger UI-n a trace r\u00e9szletes n\u00e9zetben az ablak tetej\u00e9n a keres\u0151be \u00edrjuk be: Msa.Comm.Lab.Services.Catalog.Controllers.ProductController/Get . A s\u00e1rg\u00e1val kiement spant nyissuk le, kb. a 10. logbejegyz\u00e9s a ProductRepository.GetProducts is executed , l\u00e1tjuk a username \u00e9s requestid kulcsokat \u00e9s azok \u00e9rt\u00e9k\u00e9t.","title":"Baggage haszn\u00e1lata (##Instr_Baggage)"},{"location":"DevOps/Prometheus-monitoring/","text":"C\u00e9l \u00b6 A labor c\u00e9lja egy p\u00e9ld\u00e1n kereszt\u00fcl megismerni a Prometheus-alap\u00fa monitoroz\u00e1st Kubernetes k\u00f6rnyezetben. El\u0151k\u00f6vetelm\u00e9nyek \u00b6 Kubernetes B\u00e1rmely felh\u0151 platform \u00e1ltal biztos\u00edtott klaszter Linux platformon: minikube Windows platformon: Docker Desktop kubectl A bin\u00e1risa legyen el\u00e9rhet\u0151 PATH-on. helm v3 A bin\u00e1risa legyen el\u00e9rhet\u0151 PATH-on. Prometheusr\u00f3l, r\u00f6viden \u00b6 A Prometheus egy metrik\u00e1k gy\u0171jt\u00e9s\u00e9re \u00e9s lek\u00e9rdez\u00e9s\u00e9re alkalmas adatb\u00e1zis. Feladata egy rendszer (nem csak Kubernetes!) komponenseit\u0151l azok \u00e1ltal publik\u00e1lt metrika adatok \u00f6sszegy\u0171jt\u00e9se. Saj\u00e1t grafikus felhaszn\u00e1l\u00f3i fel\u00fclettel rendelkezik ugyan, de az csak nagyon alapvet\u0151 funkci\u00f3kra haszn\u00e1lhat\u00f3, ez\u00e9rt a Grafana dashboard rendszerrel egy\u00fctt szokt\u00e1k haszn\u00e1lni. A Prometheus architekt\u00far\u00e1ja a hivatalos dokument\u00e1ci\u00f3b\u00f3l: Prometheus telep\u00edt\u00e9se \u00b6 A Prometheus csak egy metrikagy\u0171jt\u0151 szerver. Ahhoz, hogy Kubernetes alatt monitorozni tudjuk, sz\u00e1mos tov\u00e1bbi komponensre van sz\u00fcks\u00e9g. Ezek telep\u00edt\u00e9s\u00e9t fogja \u00f6ssze t\u00f6bb projekt is (amelyek zavarbaejt\u0151en hasonl\u00f3 nev\u0171ek, ez\u00e9rt vigy\u00e1zzunk, ha r\u00e1keres\u00fcnk). Ezek k\u00f6z\u00fcl a prometheus-operator helm chart telep\u00edt\u00e9se a legegyszer\u0171bb sz\u00e1munka. Helm seg\u00edts\u00e9g\u00e9vel regisztr\u00e1ljuk a chart repository-j\u00e1t, majd telep\u00edts\u00fck a chart-ot: helm repo add stable https://kubernetes-charts.storage.googleapis.com helm repo update helm install prometheus stable/prometheus-operator Prometheus webfel\u00fclet \u00e9s Grafana \u00b6 A Prometheus saj\u00e1t webfel\u00fclete nem t\u00fal felhaszn\u00e1l\u00f3bar\u00e1t, de c\u00e9lrat\u00f6r\u0151 eszk\u00f6z. A Grafana a m\u00e1r j\u00f3l \u00f6ssze\u00e1ll\u00edtott rendszer\u00fcnk metrik\u00e1inak grafikus megjelen\u00edt\u00e9s\u00e9\u00e9rt felel. A Prometheus saj\u00e1t webfel\u00fclet\u00e9t a prometheus-t futtat\u00f3 pod 9090-es porj\u00e1n \u00e9rj\u00fck el. K\u00fcl\u00f6n\u00f6sebb bonyol\u00edt\u00e1s n\u00e9lk\u00fcl tov\u00e1bb\u00edtsuk ezt egy helyi portra egy \u00faj termin\u00e1lban kiadott parancssal: kubectl port-forward $(kubectl get pods --selector \"app=prometheus\" --output=name) 9090:9090 Nyissuk meg b\u00f6ng\u00e9sz\u0151ben a http://localhost:9090 c\u00edmen. N\u00e9zz\u00fck meg a megtal\u00e1lt target -eket \u00e9s az alert -eket is. Tegy\u00fck el\u00e9rhet\u0151v\u00e9 a Grafana-t is az el\u0151z\u0151h\u00f6z hasonl\u00f3an egy \u00faj termin\u00e1lban kiadott paranccsal: kubectl port-forward $(kubectl get pods --selector \"app.kubernetes.io/name=grafana\" --output=name) 3000:3000 A Grafana a http://localhost:3000 c\u00edmen lesz el\u00e9rhet\u0151. Nyissuk meg \u00e9s l\u00e9pj\u00fcnk be az admin / prom-operator felhaszn\u00e1l\u00f3n\u00e9vvel \u00e9s jelsz\u00f3val, majd n\u00e9zz\u00fcnk meg p\u00e1r k\u00e9sz dashboard-ot. Ezeket mind a prometheus-operator helm chart telep\u00edtette sz\u00e1munkra. Saj\u00e1t komponensek monitoroz\u00e1sa \u00b6 A prometheus-operator chart el\u0151re konfigur\u00e1l sz\u00e1mos monitorozott c\u00e9lt, ezek t\u00f6bbnyire a Kubernetes saj\u00e1t m\u0171k\u00f6d\u00e9s\u00e9t seg\u00edtik l\u00e1that\u00f3v\u00e1 tenni. Ha a saj\u00e1t alkalmaz\u00e1sunkat szeretn\u00e9nk monitorozni, h\u00e1rom lehet\u0151s\u00e9g\u00fcnk van. T\u00f6bb eszk\u00f6z t\u00e1mogatja a Prometheus monitoroz\u00e1st, csak enged\u00e9lyezni kell. Erre p\u00e9lda a Traefik, amelyben egy kapcsol\u00f3val enged\u00e9lyezhetj\u00fck , hogy a Prometheus sz\u00e1m\u00e1ra scrapelhet\u0151v\u00e9 tegye a metrik\u00e1it. Ezen metrik\u00e1kra gyakran tal\u00e1lunk k\u00e9sz Grafana dashboard-ot is, amit csak import\u00e1lnunk kell. Bizonyos komponensek \u00f6nmagukban nem publik\u00e1lnak Prometheus sz\u00e1m\u00e1ra metrik\u00e1kat, azonban saj\u00e1t bels\u0151 metrik\u00e1ik vannak, \u00edgy azokat \"csak\" transzform\u00e1lni kell a Prometheus sz\u00e1m\u00e1ra. Erre egy p\u00e9lda az Elasticsearch, amely saj\u00e1t mag\u00e1r\u00f3l r\u00e9szletes inform\u00e1ci\u00f3kat ny\u00fajt http k\u00e9r\u00e9seken kereszt\u00fcl, de nem direktben a Prometheus \u00e1ltal v\u00e1rt form\u00e1ban. Ilyenkor un. exporter -t telep\u00edthet\u00fcnk, amely egy kis program, maga is kont\u00e9ner form\u00e1j\u00e1ban, ami az Elasticsearch-t\u0151l lek\u00e9rdezett adatokat transzform\u00e1lja a Prometheus-nak. Ha pedig magunk \u00e1ltal \u00edrt komponenseket akarunk monitorozni, akkor a forr\u00e1sk\u00f3dunkat kell instrument\u00e1lni. Ez az adott k\u00f6rnyezet ismeret\u00e9ben egyszer\u0171bb vagy bonyolultabb feladat. P\u00e9ld\u00e1ul .NET Core eset\u00e9ben a Prometheus-net NuGet csomagon kereszt\u00fck p\u00e1r sor k\u00f3d seg\u00edts\u00e9g\u00e9vel kivitelezhet\u0151.","title":"Prometheus monitoroz\u00e1s Kubernetes-ben"},{"location":"DevOps/Prometheus-monitoring/#cel","text":"A labor c\u00e9lja egy p\u00e9ld\u00e1n kereszt\u00fcl megismerni a Prometheus-alap\u00fa monitoroz\u00e1st Kubernetes k\u00f6rnyezetben.","title":"C\u00e9l"},{"location":"DevOps/Prometheus-monitoring/#elokovetelmenyek","text":"Kubernetes B\u00e1rmely felh\u0151 platform \u00e1ltal biztos\u00edtott klaszter Linux platformon: minikube Windows platformon: Docker Desktop kubectl A bin\u00e1risa legyen el\u00e9rhet\u0151 PATH-on. helm v3 A bin\u00e1risa legyen el\u00e9rhet\u0151 PATH-on.","title":"El\u0151k\u00f6vetelm\u00e9nyek"},{"location":"DevOps/Prometheus-monitoring/#prometheusrol-roviden","text":"A Prometheus egy metrik\u00e1k gy\u0171jt\u00e9s\u00e9re \u00e9s lek\u00e9rdez\u00e9s\u00e9re alkalmas adatb\u00e1zis. Feladata egy rendszer (nem csak Kubernetes!) komponenseit\u0151l azok \u00e1ltal publik\u00e1lt metrika adatok \u00f6sszegy\u0171jt\u00e9se. Saj\u00e1t grafikus felhaszn\u00e1l\u00f3i fel\u00fclettel rendelkezik ugyan, de az csak nagyon alapvet\u0151 funkci\u00f3kra haszn\u00e1lhat\u00f3, ez\u00e9rt a Grafana dashboard rendszerrel egy\u00fctt szokt\u00e1k haszn\u00e1lni. A Prometheus architekt\u00far\u00e1ja a hivatalos dokument\u00e1ci\u00f3b\u00f3l:","title":"Prometheusr\u00f3l, r\u00f6viden"},{"location":"DevOps/Prometheus-monitoring/#prometheus-telepitese","text":"A Prometheus csak egy metrikagy\u0171jt\u0151 szerver. Ahhoz, hogy Kubernetes alatt monitorozni tudjuk, sz\u00e1mos tov\u00e1bbi komponensre van sz\u00fcks\u00e9g. Ezek telep\u00edt\u00e9s\u00e9t fogja \u00f6ssze t\u00f6bb projekt is (amelyek zavarbaejt\u0151en hasonl\u00f3 nev\u0171ek, ez\u00e9rt vigy\u00e1zzunk, ha r\u00e1keres\u00fcnk). Ezek k\u00f6z\u00fcl a prometheus-operator helm chart telep\u00edt\u00e9se a legegyszer\u0171bb sz\u00e1munka. Helm seg\u00edts\u00e9g\u00e9vel regisztr\u00e1ljuk a chart repository-j\u00e1t, majd telep\u00edts\u00fck a chart-ot: helm repo add stable https://kubernetes-charts.storage.googleapis.com helm repo update helm install prometheus stable/prometheus-operator","title":"Prometheus telep\u00edt\u00e9se"},{"location":"DevOps/Prometheus-monitoring/#prometheus-webfelulet-es-grafana","text":"A Prometheus saj\u00e1t webfel\u00fclete nem t\u00fal felhaszn\u00e1l\u00f3bar\u00e1t, de c\u00e9lrat\u00f6r\u0151 eszk\u00f6z. A Grafana a m\u00e1r j\u00f3l \u00f6ssze\u00e1ll\u00edtott rendszer\u00fcnk metrik\u00e1inak grafikus megjelen\u00edt\u00e9s\u00e9\u00e9rt felel. A Prometheus saj\u00e1t webfel\u00fclet\u00e9t a prometheus-t futtat\u00f3 pod 9090-es porj\u00e1n \u00e9rj\u00fck el. K\u00fcl\u00f6n\u00f6sebb bonyol\u00edt\u00e1s n\u00e9lk\u00fcl tov\u00e1bb\u00edtsuk ezt egy helyi portra egy \u00faj termin\u00e1lban kiadott parancssal: kubectl port-forward $(kubectl get pods --selector \"app=prometheus\" --output=name) 9090:9090 Nyissuk meg b\u00f6ng\u00e9sz\u0151ben a http://localhost:9090 c\u00edmen. N\u00e9zz\u00fck meg a megtal\u00e1lt target -eket \u00e9s az alert -eket is. Tegy\u00fck el\u00e9rhet\u0151v\u00e9 a Grafana-t is az el\u0151z\u0151h\u00f6z hasonl\u00f3an egy \u00faj termin\u00e1lban kiadott paranccsal: kubectl port-forward $(kubectl get pods --selector \"app.kubernetes.io/name=grafana\" --output=name) 3000:3000 A Grafana a http://localhost:3000 c\u00edmen lesz el\u00e9rhet\u0151. Nyissuk meg \u00e9s l\u00e9pj\u00fcnk be az admin / prom-operator felhaszn\u00e1l\u00f3n\u00e9vvel \u00e9s jelsz\u00f3val, majd n\u00e9zz\u00fcnk meg p\u00e1r k\u00e9sz dashboard-ot. Ezeket mind a prometheus-operator helm chart telep\u00edtette sz\u00e1munkra.","title":"Prometheus webfel\u00fclet \u00e9s Grafana"},{"location":"DevOps/Prometheus-monitoring/#sajat-komponensek-monitorozasa","text":"A prometheus-operator chart el\u0151re konfigur\u00e1l sz\u00e1mos monitorozott c\u00e9lt, ezek t\u00f6bbnyire a Kubernetes saj\u00e1t m\u0171k\u00f6d\u00e9s\u00e9t seg\u00edtik l\u00e1that\u00f3v\u00e1 tenni. Ha a saj\u00e1t alkalmaz\u00e1sunkat szeretn\u00e9nk monitorozni, h\u00e1rom lehet\u0151s\u00e9g\u00fcnk van. T\u00f6bb eszk\u00f6z t\u00e1mogatja a Prometheus monitoroz\u00e1st, csak enged\u00e9lyezni kell. Erre p\u00e9lda a Traefik, amelyben egy kapcsol\u00f3val enged\u00e9lyezhetj\u00fck , hogy a Prometheus sz\u00e1m\u00e1ra scrapelhet\u0151v\u00e9 tegye a metrik\u00e1it. Ezen metrik\u00e1kra gyakran tal\u00e1lunk k\u00e9sz Grafana dashboard-ot is, amit csak import\u00e1lnunk kell. Bizonyos komponensek \u00f6nmagukban nem publik\u00e1lnak Prometheus sz\u00e1m\u00e1ra metrik\u00e1kat, azonban saj\u00e1t bels\u0151 metrik\u00e1ik vannak, \u00edgy azokat \"csak\" transzform\u00e1lni kell a Prometheus sz\u00e1m\u00e1ra. Erre egy p\u00e9lda az Elasticsearch, amely saj\u00e1t mag\u00e1r\u00f3l r\u00e9szletes inform\u00e1ci\u00f3kat ny\u00fajt http k\u00e9r\u00e9seken kereszt\u00fcl, de nem direktben a Prometheus \u00e1ltal v\u00e1rt form\u00e1ban. Ilyenkor un. exporter -t telep\u00edthet\u00fcnk, amely egy kis program, maga is kont\u00e9ner form\u00e1j\u00e1ban, ami az Elasticsearch-t\u0151l lek\u00e9rdezett adatokat transzform\u00e1lja a Prometheus-nak. Ha pedig magunk \u00e1ltal \u00edrt komponenseket akarunk monitorozni, akkor a forr\u00e1sk\u00f3dunkat kell instrument\u00e1lni. Ez az adott k\u00f6rnyezet ismeret\u00e9ben egyszer\u0171bb vagy bonyolultabb feladat. P\u00e9ld\u00e1ul .NET Core eset\u00e9ben a Prometheus-net NuGet csomagon kereszt\u00fck p\u00e1r sor k\u00f3d seg\u00edts\u00e9g\u00e9vel kivitelezhet\u0151.","title":"Saj\u00e1t komponensek monitoroz\u00e1sa"},{"location":"Docker/Docker-alapok/","text":"El\u0151ad\u00e1s \u00b6 Kont\u00e9ner-technol\u00f3gi\u00e1k \u00e9s Docker alapok C\u00e9l \u00b6 A labor c\u00e9lja megismerni a Docker kont\u00e9nerek haszn\u00e1lat\u00e1nak alapjait \u00e9s a leggyakrabban haszn\u00e1lt Docker CLI parancsokat. El\u0151k\u00f6vetelm\u00e9nyek \u00b6 Docker Desktop A laboron Windows platformot haszn\u00e1lunk, azonban a feladatok Linuxon \u00e9s Mac-en is megoldhat\u00f3ak (a k\u00f6nyvt\u00e1r el\u00e9r\u00e9si \u00fatvonalakat megfelel\u0151en \u00e1t\u00edrva). Verzi\u00f3: 2.1 vagy 2.3+ (2.2 nem j\u00f3) Alap Linux parancsok ismerete. \u00c9rdemes \u00e1tn\u00e9zni pl.: http://bmeaut.github.io/snippets/snippets/0700_LinuxBev/ https://maker.pro/linux/tutorial/basic-linux-commands-for-beginners https://www.pcsuggest.com/basic-linux-commands/ Feladatok \u00b6 Docker Desktop \u00b6 Ind\u00edtsuk el a Docker Desktop -ot. Keress\u00fck meg a t\u00e1lc\u00e1n az ikont. N\u00e9zz\u00fck meg a be\u00e1ll\u00edt\u00e1si lehet\u0151s\u00e9geit. Keress\u00fck meg a Switch to Linux containers / Switch to Windows containers parancsokat. Csak Windows-on: keress\u00fck meg a h\u00e1tt\u00e9rben fut\u00f3 Hyper-V VM-et. Csak Windows-on: Ha tudjuk, kapcsoljuk be a WSL2-t. Docker hello world \u00b6 Nyissunk egy PowerShell konzolt, \u00e9s adjuk ki a k\u00f6vetkez\u0151 parancsokat. docker --version Ezzel ellen\u0151rizhetj\u00fck, hogy a docker CLI el\u00e9rhet\u0151-e. docker run hello-world hello-word egy image neve: https://hub.docker.com/_/hello-world Image let\u00f6lt\u0151dik, elindul, lefut a benne le\u00edrt program. Kont\u00e9ner futtat\u00e1sa interakt\u00edv m\u00f3don \u00b6 Add ki a k\u00f6vetkez\u0151 parancsot: docker run -it ubuntu N\u00e9zz\u00fck meg a f\u00e1jlrendszert: ls L\u00e9pj\u00fcnk ki az interakt\u00edv shell-b\u0151l: exit Kont\u00e9ner termin\u00e1lt, mert a bash folyamat meg\u00e1llt az exit hat\u00e1s\u00e1ra. Kont\u00e9ner addig fut, am\u00edg a benne lev\u0151 alkalmaz\u00e1s (folyamat) fut. Le\u00e1llt kont\u00e9ner nem t\u00f6rl\u0151dik automatikusan, tartalma nem veszik el: docker ps -a . N\u00e9zz\u00fck meg a parancs eredm\u00e9ny\u00e9t. Keress\u00fck meg a kont\u00e9nerek id-j\u00e1t. T\u00e1vol\u00edtsuk el a k\u00e9t kont\u00e9nert, amit mi ind\u00edtottunk: docker rm <id1> <id2> Docker CLI parancsok strukt\u00far\u00e1ja \u00b6 Adjuk ki a docker parancsot a help-hez. Adminisztrat\u00edv parancsok: mivel mit , pl. docker image ls Kezel\u0151 parancsok: parancs argumentumok , pl. docker rmi <id> Gyakran haszn\u00e1ltak: Kont\u00e9nerek kezel\u00e9se docker container ls [-all] avagy docker ps [-a] docker run [opci\u00f3k] <image> docker stop <id> docker rm <id> Image-ek kezel\u00e9se docker pull <image> docker image ls avagy docker images docker rmi <image> docker tag <id> <tag> Konkr\u00e9t parancshoz seg\u00edts\u00e9g: docker <parancs> --help Minden kont\u00e9ner (fut\u00f3k is!) elt\u00e1vol\u00edt\u00e1sa docker rm -f $(docker ps -aq) (PowerShell) Volume csatol\u00e1sa ( bind mount ) \u00b6 Hozzunk l\u00e9tre egy munkak\u00f6nyvt\u00e1rat, pl. c:\\work\\<neptun>\\foo Ind\u00edtsunk el egy kont\u00e9nert \u00fagy, hogy ezt a k\u00f6nyvt\u00e1rat felcsatoljuk docker run -it --rm -v c:\\work\\<neptun>\\foo:/bar ubuntu szintaktika: helyi teljes el\u00e9r\u00e9si \u00fatvonal kett\u0151spont kont\u00e9neren bel\u00fcli teljes el\u00e9r\u00e9si \u00fatvonal Kont\u00e9neren bel\u00fcl: ls , l\u00e1tjuk a /bar k\u00f6nyvt\u00e1rat \u00cdrjunk bele: echo \"hello\" > /bar/a.txt exit N\u00e9zz\u00fck meg a munkak\u00f6nyvt\u00e1runkat. --rm A --rm opci\u00f3 t\u00f6rli a kont\u00e9nert le\u00e1ll\u00e1s ut\u00e1n; pl. tesztel\u00e9shez hasznos, mint most. Port mappel\u00e9s \u00b6 Ind\u00edtsunk el egy nginx webszervert: docker run -d -p 8085:80 nginx -d opci\u00f3: h\u00e1tt\u00e9rben fut, a konzolt \"visszakaptunk\", amint elindult a kont\u00e9ner, \u00e9s ki\u00edrja az image id-t -p helyi port kett\u0151spont kont\u00e9neren bel\u00fcli port Nyissuk meg b\u00f6ng\u00e9sz\u0151ben ezt a c\u00edmet: http://localhost:8085 N\u00e9zz\u00fck meg a kont\u00e9ner logjait: docker logs <id> \u00c1ll\u00edtsuk le a kont\u00e9nert: docker stop <id> M\u0171veletv\u00e9gz\u00e9s fut\u00f3 kont\u00e9nerben \u00b6 Ind\u00edtsunk el egy nginx webszervert: docker run -d -p 8085:80 nginx Jegyezz\u00fck meg a ki\u00edrt kont\u00e9ner id-t, al\u00e1bb haszn\u00e1lni fogjuk. Futtassunk le egy parancsot a kont\u00e9nerben: docker exec <id> ls / A parancs kilist\u00e1zta a kont\u00e9ner f\u00e1jlrendszer\u00e9nek gy\u00f6ker\u00e9t. K\u00e9rhet\u00fcnk egy shell-t is a kont\u00e9nerbe ily m\u00f3don: docker exec -it <id> /bin/bash Az -it opci\u00f3 az interaktivit\u00e1sra utal, azaz a konzolunkat \"hozz\u00e1k\u00f6ti\" a kont\u00e9nerben fut\u00f3 shellhez. Tipikusan vagy /bin/bash vagy /bin/sh a Linux kont\u00e9nerekben a shell. Ut\u00f3bbi az alpine alap\u00fa kont\u00e9nerekben gyakori. Ebben az interakt\u00edv shell-ben b\u00e1rmit csin\u00e1lhatunk, bel\u00e9phet\u00fcnk k\u00f6nyvt\u00e1rakba, megn\u00e9zhet\u00fcnk f\u00e1jlokat, stb. (Arra viszont \u00fcgyelj\u00fcnk, hogy az \u00edgy v\u00e9gzett m\u00f3dos\u00edt\u00e1saink elvesznek, amikor a kont\u00e9ner t\u00f6rl\u00e9sre ker\u00fcl.) P\u00e9ld\u00e1ul n\u00e9zz\u00fck meg az nginx konfigur\u00e1ci\u00f3j\u00e1t: cat /etc/nginx/conf.d/default.conf L\u00e9pj\u00fcnk ki az exit utas\u00edt\u00e1ssal. Ez csak a \"m\u00e1sodik\" shellt \u00e1ll\u00edtja le, a kont\u00e9ner m\u00e9g fut, mert az eredeti ind\u00edt\u00e1si pont is m\u00e9g fut. Ha sz\u00fcks\u00e9g\u00fcnk van egy f\u00e1jlra, akkor azt kim\u00e1solhatjuk a fut\u00f3 kont\u00e9nerb\u0151l: docker cp <id>:/etc/nginx/conf.d/default.conf c:\\work\\nginx.conf Szintaktik\u00e1ja: docker cp <id>:</full/path> <c\u00e9l/hely> A m\u00e1sol\u00e1s az ellenkez\u0151 ir\u00e1nyba is m\u0171k\u00f6dik, helyi g\u00e9pr\u0151l a kont\u00e9nerbe. Docker registry \u00b6 Kor\u00e1bban haszn\u00e1lt parancs: docker run ubuntu Az ubuntu az image neve. Ez egy un. registry-b\u0151l j\u00f6n. Alap\u00e9rtelmezett registry: https://hub.docker.com Tipikusan open-source szoftverek image-ei, \u00e9s az \u00e1ltalunk is haszn\u00e1lt alap image-ek. Vannak tov\u00e1bbiak is (Azure, Google, stb.) Anal\u00f3gia: NPM csomagkezel\u0151, NuGet.org Image neve val\u00f3j\u00e1ban nem ubuntu , hanem index.docker.io/ubuntu:latest index.docker.io registry szerver el\u00e9r\u00e9si \u00fatvonala ubuntu image neve (lehet t\u00f6bbszint\u0171 is) :latest tag neve Hasonl\u00f3 p\u00e9lda: mcr.microsoft.com/dotnet/core/runtime:2.1 K\u00e9t fajta registry: publikus (pl. Docker Hub) \u00e9s priv\u00e1t Priv\u00e1t registry eset\u00e9n: docker login <url> \u00e9s docker logout <url> Let\u00f6lt\u00e9s a registry-b\u0151l: docker pull mcr.microsoft.com/dotnet/core/runtime:2.1 Ugyan a run parancs is let\u00f6lti, de csak akkor, ha m\u00e9g nem l\u00e9tezik. Nem ellen\u0151rzi viszont, hogy nincs-e \u00fajabb image verzi\u00f3 publik\u00e1lva. A pull mindig frisset szed le. K\u00e9sz\u00edts\u00fcnk saj\u00e1t image-et \u00b6 K\u00f6vesd az al\u00e1bbi l\u00e9p\u00e9seket egy saj\u00e1t image elk\u00e9sz\u00edt\u00e9s\u00e9hez. Az al\u00e1bbin\u00e1l lesz jobb megold\u00e1s, l\u00e1sd k\u00f6vetkez\u0151 \u00f3r\u00e1n. Ind\u00edts el egy nginx image-et: docker run -d -p 8085:80 nginx Jegyezd meg az image id-t, al\u00e1bb t\u00f6bbsz\u00f6r is haszn\u00e1lni fogjuk. \"L\u00e9pj be\" a fut\u00f3 kont\u00e9nerbe egy interakt\u00edv bash shellben: docker exec -it <id> /bin/bash A bash shellben l\u00e9pj be az nginx \u00e1ltal kiszolg\u00e1lt index html-t tartalmaz\u00f3 mapp\u00e1ba: cd /usr/share/nginx/html/ N\u00e9zd meg a mappa tartalm\u00e1t: ls \u00cdrd fel\u00fcl az index.html tartalm\u00e1t: echo \"hello from nginx\" > index.html L\u00e9pj ki a shellb\u0151l: exit Ellen\u0151rizd meg, hogy a kont\u00e9ner m\u00e9g fut: docker ps Nyisd meg b\u00f6ng\u00e9sz\u0151b\u0151l a http://localhost:8085 c\u00edmet, ellen\u0151rizd, hogy megjelenik a saj\u00e1t tartalom K\u00e9sz\u00edts egy pillanatment\u00e9st a kont\u00e9ner jelenlegi \u00e1llapot\u00e1r\u00f3l: docker commit <id> Az el\u0151bbi parancs k\u00e9sz\u00edtett egy image-et, aminek ki\u00edrta a hash-\u00e9t. Ellen\u0151rizd, hogy t\u00e9nyleg l\u00e9tezik-e ez az image: docker images \u00c1ll\u00edtsd le a h\u00e1tt\u00e9rben fut\u00f3 kont\u00e9nert: docker stop <id> Taggeld meg az image-et: docker tag <imageid> mynginx (itt m\u00e1r az image id-ja kell) Ind\u00edts el egy \u00faj kont\u00e9nert az el\u0151bb l\u00e9trehozott saj\u00e1t image-b\u0151l: docker run -it --rm -p 8086:80 mynginx (A portsz\u00e1m sz\u00e1nd\u00e9kosan m\u00e1s, hogy biztosan legy\u00fcnk benne, nem a kor\u00e1bban fut\u00f3hoz csatlakozunk - ha m\u00e9gse \u00e1ll\u00edtottuk volna azt le.) Nyisd meg b\u00f6ng\u00e9sz\u0151b\u0151l a http://localhost:8086 c\u00edmet. L\u00e1that\u00f3, hogy ez a m\u00f3dos\u00edtott tartalmat jelen\u00edti meg. Teh\u00e1t mynginx n\u00e9ven l\u00e9trehoztunk egy saj\u00e1t image-et. Takar\u00edt\u00e1s \u00b6 Fejleszt\u00e9s k\u00f6zben sok ideiglenes image keletkezik, \u00e9s kont\u00e9nereket hagyunk h\u00e1tra. Add ki a k\u00f6vetkez\u0151 parancsot a nem fut\u00f3 kont\u00e9nerek t\u00f6rl\u00e9s\u00e9hez \u00e9s az ideiglenes (c\u00edmke n\u00e9lk\u00fcli) image-ek t\u00f6rl\u00e9s\u00e9hez: docker system prune Docker kont\u00e9rek esem\u00e9nyek megfigyel\u00e9se \u00b6 Nyissunk meg 2 PowerShell konzolt egym\u00e1st mell\u00e9. A bal oldali konzolon ind\u00edtsuk el a docker esem\u00e9nyfigyel\u0151 szolg\u00e1ltat\u00e1s\u00e1t docker events A jobb oldali konzolon ind\u00edtsunk el egy nginx kont\u00e9nert docker run -d -p 8085:80 --name nginx nginx Ekkor a bal oldalon l\u00e1that\u00f3ak az esem\u00e9nyek. Figyelj\u00fck meg, milyen metrik\u00e1k tal\u00e1lhat\u00f3ak itt. Hogy olvashat\u00f3bb legyen, \u00e1ll\u00edtsuk le a processt, CTRL-C -vel, majd: docker events | cut -c1-70 Szimul\u00e1ljunk kont\u00e9ner meghib\u00e1sod\u00e1st: docker kill nginx Adjuk ki a docker rm -f nginx parancsot, mit l\u00e1thatunk? Vess\u00fck \u00f6ssze a mell\u00e9kelt \u00e1bra esem\u00e9nyeivel. Tov\u00e1bbi olvasnival\u00f3 \u00b6 Digest-ek: https://engineering.remind.com/docker-image-digests/ Image layerek: https://docs.docker.com/v17.09/engine/userguide/storagedriver/imagesandcontainers/","title":"Docker alapok"},{"location":"Docker/Docker-alapok/#eloadas","text":"Kont\u00e9ner-technol\u00f3gi\u00e1k \u00e9s Docker alapok","title":"El\u0151ad\u00e1s"},{"location":"Docker/Docker-alapok/#cel","text":"A labor c\u00e9lja megismerni a Docker kont\u00e9nerek haszn\u00e1lat\u00e1nak alapjait \u00e9s a leggyakrabban haszn\u00e1lt Docker CLI parancsokat.","title":"C\u00e9l"},{"location":"Docker/Docker-alapok/#elokovetelmenyek","text":"Docker Desktop A laboron Windows platformot haszn\u00e1lunk, azonban a feladatok Linuxon \u00e9s Mac-en is megoldhat\u00f3ak (a k\u00f6nyvt\u00e1r el\u00e9r\u00e9si \u00fatvonalakat megfelel\u0151en \u00e1t\u00edrva). Verzi\u00f3: 2.1 vagy 2.3+ (2.2 nem j\u00f3) Alap Linux parancsok ismerete. \u00c9rdemes \u00e1tn\u00e9zni pl.: http://bmeaut.github.io/snippets/snippets/0700_LinuxBev/ https://maker.pro/linux/tutorial/basic-linux-commands-for-beginners https://www.pcsuggest.com/basic-linux-commands/","title":"El\u0151k\u00f6vetelm\u00e9nyek"},{"location":"Docker/Docker-alapok/#feladatok","text":"","title":"Feladatok"},{"location":"Docker/Docker-alapok/#docker-desktop","text":"Ind\u00edtsuk el a Docker Desktop -ot. Keress\u00fck meg a t\u00e1lc\u00e1n az ikont. N\u00e9zz\u00fck meg a be\u00e1ll\u00edt\u00e1si lehet\u0151s\u00e9geit. Keress\u00fck meg a Switch to Linux containers / Switch to Windows containers parancsokat. Csak Windows-on: keress\u00fck meg a h\u00e1tt\u00e9rben fut\u00f3 Hyper-V VM-et. Csak Windows-on: Ha tudjuk, kapcsoljuk be a WSL2-t.","title":"Docker Desktop"},{"location":"Docker/Docker-alapok/#docker-hello-world","text":"Nyissunk egy PowerShell konzolt, \u00e9s adjuk ki a k\u00f6vetkez\u0151 parancsokat. docker --version Ezzel ellen\u0151rizhetj\u00fck, hogy a docker CLI el\u00e9rhet\u0151-e. docker run hello-world hello-word egy image neve: https://hub.docker.com/_/hello-world Image let\u00f6lt\u0151dik, elindul, lefut a benne le\u00edrt program.","title":"Docker hello world"},{"location":"Docker/Docker-alapok/#kontener-futtatasa-interaktiv-modon","text":"Add ki a k\u00f6vetkez\u0151 parancsot: docker run -it ubuntu N\u00e9zz\u00fck meg a f\u00e1jlrendszert: ls L\u00e9pj\u00fcnk ki az interakt\u00edv shell-b\u0151l: exit Kont\u00e9ner termin\u00e1lt, mert a bash folyamat meg\u00e1llt az exit hat\u00e1s\u00e1ra. Kont\u00e9ner addig fut, am\u00edg a benne lev\u0151 alkalmaz\u00e1s (folyamat) fut. Le\u00e1llt kont\u00e9ner nem t\u00f6rl\u0151dik automatikusan, tartalma nem veszik el: docker ps -a . N\u00e9zz\u00fck meg a parancs eredm\u00e9ny\u00e9t. Keress\u00fck meg a kont\u00e9nerek id-j\u00e1t. T\u00e1vol\u00edtsuk el a k\u00e9t kont\u00e9nert, amit mi ind\u00edtottunk: docker rm <id1> <id2>","title":"Kont\u00e9ner futtat\u00e1sa interakt\u00edv m\u00f3don"},{"location":"Docker/Docker-alapok/#docker-cli-parancsok-strukturaja","text":"Adjuk ki a docker parancsot a help-hez. Adminisztrat\u00edv parancsok: mivel mit , pl. docker image ls Kezel\u0151 parancsok: parancs argumentumok , pl. docker rmi <id> Gyakran haszn\u00e1ltak: Kont\u00e9nerek kezel\u00e9se docker container ls [-all] avagy docker ps [-a] docker run [opci\u00f3k] <image> docker stop <id> docker rm <id> Image-ek kezel\u00e9se docker pull <image> docker image ls avagy docker images docker rmi <image> docker tag <id> <tag> Konkr\u00e9t parancshoz seg\u00edts\u00e9g: docker <parancs> --help Minden kont\u00e9ner (fut\u00f3k is!) elt\u00e1vol\u00edt\u00e1sa docker rm -f $(docker ps -aq) (PowerShell)","title":"Docker CLI parancsok strukt\u00far\u00e1ja"},{"location":"Docker/Docker-alapok/#volume-csatolasa-bind-mount","text":"Hozzunk l\u00e9tre egy munkak\u00f6nyvt\u00e1rat, pl. c:\\work\\<neptun>\\foo Ind\u00edtsunk el egy kont\u00e9nert \u00fagy, hogy ezt a k\u00f6nyvt\u00e1rat felcsatoljuk docker run -it --rm -v c:\\work\\<neptun>\\foo:/bar ubuntu szintaktika: helyi teljes el\u00e9r\u00e9si \u00fatvonal kett\u0151spont kont\u00e9neren bel\u00fcli teljes el\u00e9r\u00e9si \u00fatvonal Kont\u00e9neren bel\u00fcl: ls , l\u00e1tjuk a /bar k\u00f6nyvt\u00e1rat \u00cdrjunk bele: echo \"hello\" > /bar/a.txt exit N\u00e9zz\u00fck meg a munkak\u00f6nyvt\u00e1runkat. --rm A --rm opci\u00f3 t\u00f6rli a kont\u00e9nert le\u00e1ll\u00e1s ut\u00e1n; pl. tesztel\u00e9shez hasznos, mint most.","title":"Volume csatol\u00e1sa (bind mount)"},{"location":"Docker/Docker-alapok/#port-mappeles","text":"Ind\u00edtsunk el egy nginx webszervert: docker run -d -p 8085:80 nginx -d opci\u00f3: h\u00e1tt\u00e9rben fut, a konzolt \"visszakaptunk\", amint elindult a kont\u00e9ner, \u00e9s ki\u00edrja az image id-t -p helyi port kett\u0151spont kont\u00e9neren bel\u00fcli port Nyissuk meg b\u00f6ng\u00e9sz\u0151ben ezt a c\u00edmet: http://localhost:8085 N\u00e9zz\u00fck meg a kont\u00e9ner logjait: docker logs <id> \u00c1ll\u00edtsuk le a kont\u00e9nert: docker stop <id>","title":"Port mappel\u00e9s"},{"location":"Docker/Docker-alapok/#muveletvegzes-futo-kontenerben","text":"Ind\u00edtsunk el egy nginx webszervert: docker run -d -p 8085:80 nginx Jegyezz\u00fck meg a ki\u00edrt kont\u00e9ner id-t, al\u00e1bb haszn\u00e1lni fogjuk. Futtassunk le egy parancsot a kont\u00e9nerben: docker exec <id> ls / A parancs kilist\u00e1zta a kont\u00e9ner f\u00e1jlrendszer\u00e9nek gy\u00f6ker\u00e9t. K\u00e9rhet\u00fcnk egy shell-t is a kont\u00e9nerbe ily m\u00f3don: docker exec -it <id> /bin/bash Az -it opci\u00f3 az interaktivit\u00e1sra utal, azaz a konzolunkat \"hozz\u00e1k\u00f6ti\" a kont\u00e9nerben fut\u00f3 shellhez. Tipikusan vagy /bin/bash vagy /bin/sh a Linux kont\u00e9nerekben a shell. Ut\u00f3bbi az alpine alap\u00fa kont\u00e9nerekben gyakori. Ebben az interakt\u00edv shell-ben b\u00e1rmit csin\u00e1lhatunk, bel\u00e9phet\u00fcnk k\u00f6nyvt\u00e1rakba, megn\u00e9zhet\u00fcnk f\u00e1jlokat, stb. (Arra viszont \u00fcgyelj\u00fcnk, hogy az \u00edgy v\u00e9gzett m\u00f3dos\u00edt\u00e1saink elvesznek, amikor a kont\u00e9ner t\u00f6rl\u00e9sre ker\u00fcl.) P\u00e9ld\u00e1ul n\u00e9zz\u00fck meg az nginx konfigur\u00e1ci\u00f3j\u00e1t: cat /etc/nginx/conf.d/default.conf L\u00e9pj\u00fcnk ki az exit utas\u00edt\u00e1ssal. Ez csak a \"m\u00e1sodik\" shellt \u00e1ll\u00edtja le, a kont\u00e9ner m\u00e9g fut, mert az eredeti ind\u00edt\u00e1si pont is m\u00e9g fut. Ha sz\u00fcks\u00e9g\u00fcnk van egy f\u00e1jlra, akkor azt kim\u00e1solhatjuk a fut\u00f3 kont\u00e9nerb\u0151l: docker cp <id>:/etc/nginx/conf.d/default.conf c:\\work\\nginx.conf Szintaktik\u00e1ja: docker cp <id>:</full/path> <c\u00e9l/hely> A m\u00e1sol\u00e1s az ellenkez\u0151 ir\u00e1nyba is m\u0171k\u00f6dik, helyi g\u00e9pr\u0151l a kont\u00e9nerbe.","title":"M\u0171veletv\u00e9gz\u00e9s fut\u00f3 kont\u00e9nerben"},{"location":"Docker/Docker-alapok/#docker-registry","text":"Kor\u00e1bban haszn\u00e1lt parancs: docker run ubuntu Az ubuntu az image neve. Ez egy un. registry-b\u0151l j\u00f6n. Alap\u00e9rtelmezett registry: https://hub.docker.com Tipikusan open-source szoftverek image-ei, \u00e9s az \u00e1ltalunk is haszn\u00e1lt alap image-ek. Vannak tov\u00e1bbiak is (Azure, Google, stb.) Anal\u00f3gia: NPM csomagkezel\u0151, NuGet.org Image neve val\u00f3j\u00e1ban nem ubuntu , hanem index.docker.io/ubuntu:latest index.docker.io registry szerver el\u00e9r\u00e9si \u00fatvonala ubuntu image neve (lehet t\u00f6bbszint\u0171 is) :latest tag neve Hasonl\u00f3 p\u00e9lda: mcr.microsoft.com/dotnet/core/runtime:2.1 K\u00e9t fajta registry: publikus (pl. Docker Hub) \u00e9s priv\u00e1t Priv\u00e1t registry eset\u00e9n: docker login <url> \u00e9s docker logout <url> Let\u00f6lt\u00e9s a registry-b\u0151l: docker pull mcr.microsoft.com/dotnet/core/runtime:2.1 Ugyan a run parancs is let\u00f6lti, de csak akkor, ha m\u00e9g nem l\u00e9tezik. Nem ellen\u0151rzi viszont, hogy nincs-e \u00fajabb image verzi\u00f3 publik\u00e1lva. A pull mindig frisset szed le.","title":"Docker registry"},{"location":"Docker/Docker-alapok/#keszitsunk-sajat-image-et","text":"K\u00f6vesd az al\u00e1bbi l\u00e9p\u00e9seket egy saj\u00e1t image elk\u00e9sz\u00edt\u00e9s\u00e9hez. Az al\u00e1bbin\u00e1l lesz jobb megold\u00e1s, l\u00e1sd k\u00f6vetkez\u0151 \u00f3r\u00e1n. Ind\u00edts el egy nginx image-et: docker run -d -p 8085:80 nginx Jegyezd meg az image id-t, al\u00e1bb t\u00f6bbsz\u00f6r is haszn\u00e1lni fogjuk. \"L\u00e9pj be\" a fut\u00f3 kont\u00e9nerbe egy interakt\u00edv bash shellben: docker exec -it <id> /bin/bash A bash shellben l\u00e9pj be az nginx \u00e1ltal kiszolg\u00e1lt index html-t tartalmaz\u00f3 mapp\u00e1ba: cd /usr/share/nginx/html/ N\u00e9zd meg a mappa tartalm\u00e1t: ls \u00cdrd fel\u00fcl az index.html tartalm\u00e1t: echo \"hello from nginx\" > index.html L\u00e9pj ki a shellb\u0151l: exit Ellen\u0151rizd meg, hogy a kont\u00e9ner m\u00e9g fut: docker ps Nyisd meg b\u00f6ng\u00e9sz\u0151b\u0151l a http://localhost:8085 c\u00edmet, ellen\u0151rizd, hogy megjelenik a saj\u00e1t tartalom K\u00e9sz\u00edts egy pillanatment\u00e9st a kont\u00e9ner jelenlegi \u00e1llapot\u00e1r\u00f3l: docker commit <id> Az el\u0151bbi parancs k\u00e9sz\u00edtett egy image-et, aminek ki\u00edrta a hash-\u00e9t. Ellen\u0151rizd, hogy t\u00e9nyleg l\u00e9tezik-e ez az image: docker images \u00c1ll\u00edtsd le a h\u00e1tt\u00e9rben fut\u00f3 kont\u00e9nert: docker stop <id> Taggeld meg az image-et: docker tag <imageid> mynginx (itt m\u00e1r az image id-ja kell) Ind\u00edts el egy \u00faj kont\u00e9nert az el\u0151bb l\u00e9trehozott saj\u00e1t image-b\u0151l: docker run -it --rm -p 8086:80 mynginx (A portsz\u00e1m sz\u00e1nd\u00e9kosan m\u00e1s, hogy biztosan legy\u00fcnk benne, nem a kor\u00e1bban fut\u00f3hoz csatlakozunk - ha m\u00e9gse \u00e1ll\u00edtottuk volna azt le.) Nyisd meg b\u00f6ng\u00e9sz\u0151b\u0151l a http://localhost:8086 c\u00edmet. L\u00e1that\u00f3, hogy ez a m\u00f3dos\u00edtott tartalmat jelen\u00edti meg. Teh\u00e1t mynginx n\u00e9ven l\u00e9trehoztunk egy saj\u00e1t image-et.","title":"K\u00e9sz\u00edts\u00fcnk saj\u00e1t image-et"},{"location":"Docker/Docker-alapok/#takaritas","text":"Fejleszt\u00e9s k\u00f6zben sok ideiglenes image keletkezik, \u00e9s kont\u00e9nereket hagyunk h\u00e1tra. Add ki a k\u00f6vetkez\u0151 parancsot a nem fut\u00f3 kont\u00e9nerek t\u00f6rl\u00e9s\u00e9hez \u00e9s az ideiglenes (c\u00edmke n\u00e9lk\u00fcli) image-ek t\u00f6rl\u00e9s\u00e9hez: docker system prune","title":"Takar\u00edt\u00e1s"},{"location":"Docker/Docker-alapok/#docker-konterek-esemenyek-megfigyelese","text":"Nyissunk meg 2 PowerShell konzolt egym\u00e1st mell\u00e9. A bal oldali konzolon ind\u00edtsuk el a docker esem\u00e9nyfigyel\u0151 szolg\u00e1ltat\u00e1s\u00e1t docker events A jobb oldali konzolon ind\u00edtsunk el egy nginx kont\u00e9nert docker run -d -p 8085:80 --name nginx nginx Ekkor a bal oldalon l\u00e1that\u00f3ak az esem\u00e9nyek. Figyelj\u00fck meg, milyen metrik\u00e1k tal\u00e1lhat\u00f3ak itt. Hogy olvashat\u00f3bb legyen, \u00e1ll\u00edtsuk le a processt, CTRL-C -vel, majd: docker events | cut -c1-70 Szimul\u00e1ljunk kont\u00e9ner meghib\u00e1sod\u00e1st: docker kill nginx Adjuk ki a docker rm -f nginx parancsot, mit l\u00e1thatunk? Vess\u00fck \u00f6ssze a mell\u00e9kelt \u00e1bra esem\u00e9nyeivel.","title":"Docker kont\u00e9rek esem\u00e9nyek megfigyel\u00e9se"},{"location":"Docker/Docker-alapok/#tovabbi-olvasnivalo","text":"Digest-ek: https://engineering.remind.com/docker-image-digests/ Image layerek: https://docs.docker.com/v17.09/engine/userguide/storagedriver/imagesandcontainers/","title":"Tov\u00e1bbi olvasnival\u00f3"},{"location":"Docker/Dockerfile-compose/","text":"C\u00e9l \u00b6 A labor c\u00e9lja megismerni a Docker-alap fejleszt\u00e9s m\u00f3djait, a Dockerfile \u00e9s docker-compose alap\u00fa megk\u00f6zel\u00edt\u00e9seket, valamint a Microsoft Visual Studio fejleszt\u0151k\u00f6rnyezet kont\u00e9nerfejleszt\u00e9st t\u00e1mogat\u00f3 szolg\u00e1ltat\u00e1sait. El\u0151k\u00f6vetelm\u00e9nyek \u00b6 Docker Desktop Docker-compose Csak Linux eset\u00e9n sz\u00fcks\u00e9ges k\u00fcl\u00f6n telep\u00edteni Microsoft Visual Studio Code Javasolt: Docker extension Microsoft Visual Studio 2017/2019 2017 eset\u00e9n legal\u00e1bb 15.6 verzi\u00f3 A .NET Core cross-platform development nev\u0171 workload sz\u00fcks\u00e9ges Feladatok \u00b6 Dockerfile k\u00e9sz\u00edt\u00e9se \u00b6 El\u0151z\u0151 laboron k\u00e9sz\u00edtett\u00fcnk m\u00e1r egy saj\u00e1t image-et. Az image elk\u00e9sz\u00edt\u00e9s\u00e9re azonban nem az a m\u00f3dszer a javasolt megold\u00e1s, hanem a Dockerfile alap\u00fa \"recept k\u00e9sz\u00edt\u00e9s\". K\u00e9sz\u00edts egy munkak\u00f6nyt\u00e1rat, pl. c:\\work\\<neptun>\\dockerlab\\pythonweb Nyisd meg a mapp\u00e1t Visual Studio Code-ban. K\u00e9sz\u00edts egy Dockerfile nev\u0171 f\u00e1jt (kiterjeszt\u00e9s n\u00e9lk\u00fcl!) az al\u00e1bbi tartalommal. FROM python:3.8-slim WORKDIR /app COPY . /app RUN pip install --trusted-host pypi.python.org -r requirements.txt EXPOSE 80 ENV NAME NEPTUN # Ide a saj\u00e1t Neptun k\u00f3dodat \u00edrd CMD [ \"python\" , \"app.py\" ] K\u00e9sz\u00edts egy requirements.txt f\u00e1jlt az al\u00e1bbi tartalommal Flask Redis K\u00e9sz\u00edts egy app.py f\u00e1jlt az al\u00e1bbi tartalommal from flask import Flask from redis import Redis , RedisError import os import socket # Connect to Redis redis = Redis ( host = \"redis\" , db = 0 , socket_connect_timeout = 2 , socket_timeout = 2 ) app = Flask ( __name__ ) @app . route ( \"/\" ) def hello (): try : visits = redis . incr ( \"counter\" ) except RedisError : visits = \"<i>cannot connect to Redis, counter disabled</i>\" html = \"<h3>Hello {name} !</h3><b>Visits:</b> {visits} \" return html . format ( name = os . getenv ( \"NAME\" , \"world\" ), visits = visits ) if __name__ == \"__main__\" : app . run ( host = '0.0.0.0' , port = 80 ) K\u00e9sz\u00edtsd el a fenti f\u00e1jlokb\u00f3l az image-et. Konzolb\u00f3l a munkak\u00f6nyvt\u00e1rban add ki a k\u00f6vetkez\u0151 parancsot: docker build -t hellopython:v1 . (a v\u00e9g\u00e9n egy pont van, az is a parancs r\u00e9sze!) Ellen\u0151rizd, hogy t\u00e9nyleg l\u00e9trej\u00f6tt-e az image. Ind\u00edts el egy \u00faj kont\u00e9nert ebb\u0151l az image-b\u0151l: docker run -it --rm -p 8085:80 hellopython:v1 Nyisd meg b\u00f6ng\u00e9sz\u0151ben a http://localhost:8085 oldalt. A weboldal ki kell \u00edrja a neptun k\u00f3dodat, \u00e9s egy hiba\u00fczenetet a Redis-szel kapcsolatban. Dockerignore \u00e9s build kontextus \u00b6 A Dockerfile -ban hivatkoztunk az aktu\u00e1lis k\u00f6nyvt\u00e1rra a . -tal. Vizsg\u00e1ljuk meg, hogy ez mit is jelent. K\u00e9sz\u00edts\u00fcnk az aktu\u00e1lis k\u00f6nyvt\u00e1runkba, az app.py mell\u00e9 egy nagy f\u00e1jlt. PowerShell-ben addjuk ki a k\u00f6vetkez\u0151 parancsot. $out = new-object byte [] 134217728 ; ( new-object Random ). NextBytes ( $out ); [IO.File] :: WriteAllBytes ( \"$pwd\\file.bin\" , $out ) Buildelj\u00fck le ism\u00e9t a fenti image-et: docker build -t hellopython:v1 . Menet k\u00f6zben l\u00e1tni fogjuk a k\u00f6vetkez\u0151 sort a build logban: Sending build context to Docker daemon 111.4MB , \u00e9s azt is tapasztalni fogjuk, hogy ez el tart egy kis ideig. A docker build parancs v\u00e9g\u00e9n a . az aktu\u00e1lis k\u00f6nyvt\u00e1r. Ezzel tudatjuk a Docker-rel, hogy a buildel\u00e9shez ezt a kontextust haszn\u00e1lja, azon f\u00e1jlok legyenek el\u00e9rhet\u0151ek a build sor\u00e1n, amelyek ebben a kontextusban vannak. A Dockerfile -ban a COPY \u00edgy relat\u00edv \u00fatvonallal hivatkozik a kontextusban lev\u0151 f\u00e1jlokra. El\u00e9rhet\u0151 f\u00e1jlok Ennek k\u00f6vetkezm\u00e9nye az is, hogy csak a build kontextusban lev\u0151 f\u00e1jlokra tudunk hivatkozni. Teh\u00e1t nem lehet pl. COPY ..\\..\\file haszn\u00e1lat\u00e1val tetsz\u0151leges f\u00e1jlt felm\u00e1solni a build k\u00f6zben. Ha a build kontextusb\u00f3l szeretn\u00e9nk kihagyni f\u00e1jlokat, hogy a build ne tartson sok\u00e1ig, akkor egy .dockerignore f\u00e1jlra lesz sz\u00fcks\u00e9g\u00fcnk (a .gitignore mint\u00e1j\u00e1ra). Ide szok\u00e1s p\u00e9ld\u00e1ul a build k\u00f6rnyezet saj\u00e1t k\u00f6nyvt\u00e1rait ( obj , bin , .vs , node_modules , stb.) is felvenni. K\u00e9sz\u00edts\u00fcnk egy .dockerignore -t az al\u00e1bbi tartalommal file.bin Futtassuk ism\u00e9t a buildet. \u00cdgy m\u00e1r gyorsabb lesz. Docker-compose \u00b6 A fenti alkalmaz\u00e1s egy r\u00e9sze m\u00e9g nem m\u0171k\u00f6dik. A Python alkalmaz\u00e1s mellett egy Redis-re is sz\u00fcks\u00e9g\u00fcnk lenne. Futtassunk t\u00f6bb kont\u00e9nert egyszerre. Visual Studio Code-ban nyisd meg a mapp\u00e1t, amiben az el\u0151z\u0151 feladat sor\u00e1n haszn\u00e1lt pythonweb mappa van. Teh\u00e1t nem az el\u0151z\u0151leg haszn\u00e1lt mappa kell, hanem egy szinttel feljebb (pl. c:\\work\\<neptun>\\dockerlab ). K\u00e9sz\u00edts ide egy docker-compose.yml nev\u0171 f\u00e1jlt az al\u00e1bbi tartalommal. version : \"3\" services : redis : image : redis:5.0.5-alpine networks : - mikroszolg_network web : build : pythonweb ports : - 5000:80 depends_on : - redis networks : - mikroszolg_network networks : mikroszolg_network : driver : bridge Nyiss egy PowerShell konzolt ugyanebbe a mapp\u00e1ba. Ind\u00edtsd el az alkalmaz\u00e1sokat az al\u00e1bbi paranccsal: docker-compose up --build K\u00e9t l\u00e9p\u00e9sben a parancs: docker-compose build \u00e9s docker-compose up Nyisd meg b\u00f6ng\u00e9sz\u0151ben a http://localhost:8085 oldalt. Egy \u00faj konzolban n\u00e9zd meg a fut\u00f3 kont\u00e9nereket a docker ps parancs seg\u00edts\u00e9g\u00e9vel. docker-compose \u00fczemeltet\u00e9shez A docker-compose alkalmas \u00fczemeltet\u00e9sre is. A docker-compose.yml f\u00e1jl nem csak fejleszt\u0151i k\u00f6rnyezetet \u00edr le, hanem \u00fczemeltet\u00e9shez sz\u00fcks\u00e9ges k\u00f6rnyezetet is. Ha a compose f\u00e1jlt megfelel\u0151en \u00edrjuk meg (pl. haszn\u00e1ljuk a restart direkt\u00edv\u00e1t is), az elind\u00edtott szolg\u00e1ltat\u00e1sok automatikusan \u00fajraindulnak a rendszer indul\u00e1sakor. T\u00f6bb compose yaml f\u00e1jl \u00b6 A docker-compose parancsnak nem adtuk meg, hogy milyen yaml f\u00e1jlb\u00f3l dolgozzon. Alap\u00e9rtelmez\u00e9sk\u00e9nt a docker-compose.yaml kiterjeszt\u00e9s\u0171 f\u00e1jlt \u00e9s ezzel \u00f6sszef\u00e9s\u00fclve a docker-compose.override.yaml f\u00e1jlt haszn\u00e1lja. K\u00e9sz\u00edts egy docker-compose.override.yaml f\u00e1jlt a m\u00e1sik compose yaml mell\u00e9 az al\u00e1bbi tartalommal version : \"3\" services : redis : command : redis-server --loglevel verbose Ind\u00edtsd el a rendszert docker-compose up paranccsal. A redis kont\u00e9ner r\u00e9szletesebben fog napl\u00f3zni a command direkt\u00edv\u00e1ban megadott utas\u00edt\u00e1s szerint. \u00c1ll\u00edtsd le a rendszert. Nevezd \u00e1t az el\u0151bbi override f\u00e1jlt docker-compose.debug.yaml -re. K\u00e9sz\u00edts egy \u00faj docker-compose.prod.yaml f\u00e1jlt a t\u00f6bbi yaml mell\u00e9 az al\u00e1bbi tartalommal version : \"3\" services : redis : command : redis-server --loglevel warning Ind\u00edtsuk el a rendszert az al\u00e1bbi paranccsal docker-compose -f docker-compose.yml -f docker-compose.prod.yml up A -f kapcsol\u00f3val tudjuk k\u00e9rni a megadott yaml f\u00e1jlok \u00f6sszef\u00e9s\u00fcl\u00e9s\u00e9t. \u00c1ltal\u00e1ban a docker-compose.yaml -be ker\u00fclnek a k\u00f6z\u00f6s konfigur\u00e1ci\u00f3k, \u00e9s a tov\u00e1bbi f\u00e1jlokba a k\u00f6rnyezet specifikus konfigur\u00e1ci\u00f3k. Tipikusan haszn\u00e1lt image-ek \u00b6 Kont\u00e9ner alap\u00fa fejleszt\u00e9sn\u00e9l teh\u00e1t k\u00e9t f\u00e9le image-et haszn\u00e1lunk: amit magunk k\u00e9sz\u00edt\u00fcnk egy adott alap image-re \u00e9p\u00fclve, illetve k\u00e9sz image-eket, amiket csak futtatunk (\u00e9s esetleg konfigur\u00e1lunk). Alap image-ek, amikre tipikusan saj\u00e1t alkalmaz\u00e1st \u00e9p\u00edt\u00fcnk: Linux disztrib\u00faci\u00f3k Ubuntu , pl. ubuntu:18.04 Debian , pl. debian:stretch Alpine , pl. alpine:3.10 Futtat\u00f3 platformok .NET Core , pl. mcr.microsoft.com/dotnet/core/runtime:2.1.12-bionic OpenJDK pl. openjdk:11-jre-stretch NodeJS pl. node:12.7.0-alpine Python pl. python:2.7-slim scratch : \u00fcres image, speci\u00e1lis esetek, pl. go A k\u00e9sz image-ek, amiket pedig felhaszn\u00e1lunk: Adatb\u00e1zis szerverek, webszerverek, gyakran haszn\u00e1lt szolg\u00e1ltat\u00e1sok MSSQL Server, redis, mongodb, mysql, nginx, ... Term\u00e9rdek el\u00e9rhet\u0151 image: https://hub.docker.com Verzi\u00f3z\u00e1s fontos Az image-ek verzi\u00f3z\u00e1s\u00e1t minden esetben meg kell \u00e9rteni! Minden image m\u00e1s-m\u00e1s megk\u00f6zel\u00edt\u00e9st alkalmaz. K\u00e9sz image testreszab\u00e1sa \u00b6 Az el\u0151bb a Redis mem\u00f3ria alap\u00fa adatb\u00e1zist minden konfigur\u00e1ci\u00f3 n\u00e9lk\u00fcl felhaszn\u00e1ltuk. Gyakran az ilyen image-ek \"majdnem\" j\u00f3k k\u00f6zvetlen felhaszn\u00e1l\u00e1sra, de az\u00e9rt sz\u00fcks\u00e9g van egy kev\u00e9s testreszab\u00e1sra. Ilyen esetben a k\u00f6vetkez\u0151 lehet\u0151s\u00e9geink vannak: Saj\u00e1t image-et k\u00e9sz\u00edt\u00fcnk kiindulva a sz\u00e1munkra megfelel\u0151 alap image-b\u0151l. A saj\u00e1t image-ben m\u00f3dos\u00edthatunk a konfigur\u00e1ci\u00f3s f\u00e1jlokat, avagy tov\u00e1bbi f\u00e1jlokat adhatunk az image-be. Ezt a megold\u00e1st alkalmazhatjuk p\u00e9ld\u00e1ul tipikusan weboldal kiszolg\u00e1l\u00e1s\u00e1n\u00e1l, ahol is a kiindul\u00f3 image a webszerver, viszont a kiszolg\u00e1land\u00f3 f\u00e1jlokat m\u00e9g mell\u00e9 kell tenn\u00fcnk. K\u00f6rnyezeti v\u00e1ltoz\u00f3kon kereszt\u00fcl konfigur\u00e1ljuk a futtatand\u00f3 szolg\u00e1ltat\u00e1st. Az alap image-ek \u00e1ltal\u00e1ban el\u00e9g j\u00f3 konfigur\u00e1ci\u00f3val rendelkeznek, csak keveset kell rajta m\u00f3dos\u00edtanunk. Erre t\u00f6k\u00e9letesen alkalmas egy-egy k\u00f6rnyezeti v\u00e1ltoz\u00f3. A j\u00f3l fel\u00e9p\u00edtett Docker image-ek el\u0151re meghat\u00e1rozott k\u00f6rnyezeti v\u00e1ltoz\u00f3kon kereszt\u00fcl testreszabhat\u00f3ak. Erre egy j\u00f3 p\u00e9lda a Microsoft SQL Server Docker v\u00e1ltozata . Az al\u00e1bbi parancsban a -e argumentumokban adunk \u00e1t k\u00f6rnyezeti v\u00e1ltoz\u00f3kat: docker run -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=yourStrong(!)Password' -p 1433 :1433 mcr.microsoft.com/mssql/server:2017-CU8-ubuntu Becsatolhatjuk a saj\u00e1t konfigur\u00e1ci\u00f3s f\u00e1jljainkat a kont\u00e9nerbe. Kor\u00e1bban l\u00e1ttuk, hogy lehet\u0151s\u00e9g\u00fcnk van egy mapp\u00e1t a host g\u00e9ptr\u0151l a kont\u00e9nerbe csatolni. Ha elk\u00e9sz\u00edtj\u00fck a testreszabott konfigur\u00e1ci\u00f3s f\u00e1jlt, akkor a docker-compose.yml le\u00edr\u00e1sban a k\u00f6vetkez\u0151 m\u00f3don tudjuk ezt a f\u00e1jlt becsatolni a kont\u00e9ner indul\u00e1sakor. services : redis : image : redis:5.0.5-alpine volumes : - my-redis.conf:/usr/local/etc/redis/redis.conf Ezen megold\u00e1s el\u0151nye, hogy nincs sz\u00fcks\u00e9g saj\u00e1t image-et k\u00e9sz\u00edteni, t\u00e1rolni, kezelni. Amikor a k\u00f6rnyezeti v\u00e1ltoz\u00f3 m\u00e1r nem elegend\u0151 a testreszab\u00e1shoz, ez a javasolt megold\u00e1s. Microsoft Visual Studio t\u00e1mogat\u00e1s Docker-alap\u00fa fejleszt\u00e9shez \u00b6 A Microsoft Visual Studio a 2017-es verzi\u00f3 \u00f3ta t\u00e1mogatja \u00e9s megk\u00f6nny\u00edti a kont\u00e9ner alap\u00fa szoftverfejleszt\u00e9st. Seg\u00edti a fejleszt\u0151t a fejlesztett alkalmaz\u00e1s kont\u00e9neriz\u00e1l\u00e1s\u00e1ban, \u00e9s t\u00e1mogatja a kont\u00e9nerben val\u00f3 debuggol\u00e1st is. Enged\u00e9lyezz\u00fck Docker-ben a volume sharing-et! Ind\u00edtsuk el a Visual Studio-t (nem a Code-ot!). K\u00e9sz\u00edts\u00fcnk egy \u00faj ASP.NET Core Web Application t\u00edpus\u00fa projektet a Web application sablonnal, \u00e9s enged\u00e9lyezz\u00fck a Linux-alap\u00fa kont\u00e9ner t\u00e1mogat\u00e1st a projektben. (A pontos l\u00e9p\u00e9sek Visual Studio verzi\u00f3 f\u00fcgg\u0151ek, l\u00e1sd itt .) N\u00e9zz\u00fck meg \u00e9s \u00e9rts\u00fck meg az elk\u00e9sz\u00fclt solution strukt\u00far\u00e1t, valamint a Dockerfile -t \u00e9s a docker-compose f\u00e1jlokat is. Ind\u00edtsuk el az alkalmaz\u00e1st Docker-compose haszn\u00e1lat\u00e1val: legyen a docker-compose a startup projekt\u00fcnk, \u00e9s F5-tel ind\u00edtsuk debug m\u00f3dban az alkalmaz\u00e1st. Egy konzolb\u00f3l list\u00e1zzuk ki a fut\u00f3 kont\u00e9nereket: docker ps Debuggol\u00e1s kont\u00e9nerben is A Visual Studio Code is t\u00e1mogatja a kont\u00e9nerek debuggol\u00e1s\u00e1t. Err\u0151l r\u00e9szletesen l\u00e1sd itt . Tov\u00e1bbi olvasnival\u00f3 \u00b6 Dockerfile szintaktika: https://docs.docker.com/engine/reference/builder/ .dockerignore f\u00e1jl szintaktika: https://docs.docker.com/engine/reference/builder/#dockerignore-file Dockerfile best practice-ek: https://docs.docker.com/develop/develop-images/dockerfile_best-practices/ compose f\u00e1jl szintaktika: https://docs.docker.com/compose/compose-file/ T\u00f6bb compose f\u00e1jl haszn\u00e1lata: https://docs.docker.com/compose/extends/#multiple-compose-files Multistage build-ek: https://docs.docker.com/develop/develop-images/multistage-build/","title":"Dockerfile \u00e9s docker-compose"},{"location":"Docker/Dockerfile-compose/#cel","text":"A labor c\u00e9lja megismerni a Docker-alap fejleszt\u00e9s m\u00f3djait, a Dockerfile \u00e9s docker-compose alap\u00fa megk\u00f6zel\u00edt\u00e9seket, valamint a Microsoft Visual Studio fejleszt\u0151k\u00f6rnyezet kont\u00e9nerfejleszt\u00e9st t\u00e1mogat\u00f3 szolg\u00e1ltat\u00e1sait.","title":"C\u00e9l"},{"location":"Docker/Dockerfile-compose/#elokovetelmenyek","text":"Docker Desktop Docker-compose Csak Linux eset\u00e9n sz\u00fcks\u00e9ges k\u00fcl\u00f6n telep\u00edteni Microsoft Visual Studio Code Javasolt: Docker extension Microsoft Visual Studio 2017/2019 2017 eset\u00e9n legal\u00e1bb 15.6 verzi\u00f3 A .NET Core cross-platform development nev\u0171 workload sz\u00fcks\u00e9ges","title":"El\u0151k\u00f6vetelm\u00e9nyek"},{"location":"Docker/Dockerfile-compose/#feladatok","text":"","title":"Feladatok"},{"location":"Docker/Dockerfile-compose/#dockerfile-keszitese","text":"El\u0151z\u0151 laboron k\u00e9sz\u00edtett\u00fcnk m\u00e1r egy saj\u00e1t image-et. Az image elk\u00e9sz\u00edt\u00e9s\u00e9re azonban nem az a m\u00f3dszer a javasolt megold\u00e1s, hanem a Dockerfile alap\u00fa \"recept k\u00e9sz\u00edt\u00e9s\". K\u00e9sz\u00edts egy munkak\u00f6nyt\u00e1rat, pl. c:\\work\\<neptun>\\dockerlab\\pythonweb Nyisd meg a mapp\u00e1t Visual Studio Code-ban. K\u00e9sz\u00edts egy Dockerfile nev\u0171 f\u00e1jt (kiterjeszt\u00e9s n\u00e9lk\u00fcl!) az al\u00e1bbi tartalommal. FROM python:3.8-slim WORKDIR /app COPY . /app RUN pip install --trusted-host pypi.python.org -r requirements.txt EXPOSE 80 ENV NAME NEPTUN # Ide a saj\u00e1t Neptun k\u00f3dodat \u00edrd CMD [ \"python\" , \"app.py\" ] K\u00e9sz\u00edts egy requirements.txt f\u00e1jlt az al\u00e1bbi tartalommal Flask Redis K\u00e9sz\u00edts egy app.py f\u00e1jlt az al\u00e1bbi tartalommal from flask import Flask from redis import Redis , RedisError import os import socket # Connect to Redis redis = Redis ( host = \"redis\" , db = 0 , socket_connect_timeout = 2 , socket_timeout = 2 ) app = Flask ( __name__ ) @app . route ( \"/\" ) def hello (): try : visits = redis . incr ( \"counter\" ) except RedisError : visits = \"<i>cannot connect to Redis, counter disabled</i>\" html = \"<h3>Hello {name} !</h3><b>Visits:</b> {visits} \" return html . format ( name = os . getenv ( \"NAME\" , \"world\" ), visits = visits ) if __name__ == \"__main__\" : app . run ( host = '0.0.0.0' , port = 80 ) K\u00e9sz\u00edtsd el a fenti f\u00e1jlokb\u00f3l az image-et. Konzolb\u00f3l a munkak\u00f6nyvt\u00e1rban add ki a k\u00f6vetkez\u0151 parancsot: docker build -t hellopython:v1 . (a v\u00e9g\u00e9n egy pont van, az is a parancs r\u00e9sze!) Ellen\u0151rizd, hogy t\u00e9nyleg l\u00e9trej\u00f6tt-e az image. Ind\u00edts el egy \u00faj kont\u00e9nert ebb\u0151l az image-b\u0151l: docker run -it --rm -p 8085:80 hellopython:v1 Nyisd meg b\u00f6ng\u00e9sz\u0151ben a http://localhost:8085 oldalt. A weboldal ki kell \u00edrja a neptun k\u00f3dodat, \u00e9s egy hiba\u00fczenetet a Redis-szel kapcsolatban.","title":"Dockerfile k\u00e9sz\u00edt\u00e9se"},{"location":"Docker/Dockerfile-compose/#dockerignore-es-build-kontextus","text":"A Dockerfile -ban hivatkoztunk az aktu\u00e1lis k\u00f6nyvt\u00e1rra a . -tal. Vizsg\u00e1ljuk meg, hogy ez mit is jelent. K\u00e9sz\u00edts\u00fcnk az aktu\u00e1lis k\u00f6nyvt\u00e1runkba, az app.py mell\u00e9 egy nagy f\u00e1jlt. PowerShell-ben addjuk ki a k\u00f6vetkez\u0151 parancsot. $out = new-object byte [] 134217728 ; ( new-object Random ). NextBytes ( $out ); [IO.File] :: WriteAllBytes ( \"$pwd\\file.bin\" , $out ) Buildelj\u00fck le ism\u00e9t a fenti image-et: docker build -t hellopython:v1 . Menet k\u00f6zben l\u00e1tni fogjuk a k\u00f6vetkez\u0151 sort a build logban: Sending build context to Docker daemon 111.4MB , \u00e9s azt is tapasztalni fogjuk, hogy ez el tart egy kis ideig. A docker build parancs v\u00e9g\u00e9n a . az aktu\u00e1lis k\u00f6nyvt\u00e1r. Ezzel tudatjuk a Docker-rel, hogy a buildel\u00e9shez ezt a kontextust haszn\u00e1lja, azon f\u00e1jlok legyenek el\u00e9rhet\u0151ek a build sor\u00e1n, amelyek ebben a kontextusban vannak. A Dockerfile -ban a COPY \u00edgy relat\u00edv \u00fatvonallal hivatkozik a kontextusban lev\u0151 f\u00e1jlokra. El\u00e9rhet\u0151 f\u00e1jlok Ennek k\u00f6vetkezm\u00e9nye az is, hogy csak a build kontextusban lev\u0151 f\u00e1jlokra tudunk hivatkozni. Teh\u00e1t nem lehet pl. COPY ..\\..\\file haszn\u00e1lat\u00e1val tetsz\u0151leges f\u00e1jlt felm\u00e1solni a build k\u00f6zben. Ha a build kontextusb\u00f3l szeretn\u00e9nk kihagyni f\u00e1jlokat, hogy a build ne tartson sok\u00e1ig, akkor egy .dockerignore f\u00e1jlra lesz sz\u00fcks\u00e9g\u00fcnk (a .gitignore mint\u00e1j\u00e1ra). Ide szok\u00e1s p\u00e9ld\u00e1ul a build k\u00f6rnyezet saj\u00e1t k\u00f6nyvt\u00e1rait ( obj , bin , .vs , node_modules , stb.) is felvenni. K\u00e9sz\u00edts\u00fcnk egy .dockerignore -t az al\u00e1bbi tartalommal file.bin Futtassuk ism\u00e9t a buildet. \u00cdgy m\u00e1r gyorsabb lesz.","title":"Dockerignore \u00e9s build kontextus"},{"location":"Docker/Dockerfile-compose/#docker-compose","text":"A fenti alkalmaz\u00e1s egy r\u00e9sze m\u00e9g nem m\u0171k\u00f6dik. A Python alkalmaz\u00e1s mellett egy Redis-re is sz\u00fcks\u00e9g\u00fcnk lenne. Futtassunk t\u00f6bb kont\u00e9nert egyszerre. Visual Studio Code-ban nyisd meg a mapp\u00e1t, amiben az el\u0151z\u0151 feladat sor\u00e1n haszn\u00e1lt pythonweb mappa van. Teh\u00e1t nem az el\u0151z\u0151leg haszn\u00e1lt mappa kell, hanem egy szinttel feljebb (pl. c:\\work\\<neptun>\\dockerlab ). K\u00e9sz\u00edts ide egy docker-compose.yml nev\u0171 f\u00e1jlt az al\u00e1bbi tartalommal. version : \"3\" services : redis : image : redis:5.0.5-alpine networks : - mikroszolg_network web : build : pythonweb ports : - 5000:80 depends_on : - redis networks : - mikroszolg_network networks : mikroszolg_network : driver : bridge Nyiss egy PowerShell konzolt ugyanebbe a mapp\u00e1ba. Ind\u00edtsd el az alkalmaz\u00e1sokat az al\u00e1bbi paranccsal: docker-compose up --build K\u00e9t l\u00e9p\u00e9sben a parancs: docker-compose build \u00e9s docker-compose up Nyisd meg b\u00f6ng\u00e9sz\u0151ben a http://localhost:8085 oldalt. Egy \u00faj konzolban n\u00e9zd meg a fut\u00f3 kont\u00e9nereket a docker ps parancs seg\u00edts\u00e9g\u00e9vel. docker-compose \u00fczemeltet\u00e9shez A docker-compose alkalmas \u00fczemeltet\u00e9sre is. A docker-compose.yml f\u00e1jl nem csak fejleszt\u0151i k\u00f6rnyezetet \u00edr le, hanem \u00fczemeltet\u00e9shez sz\u00fcks\u00e9ges k\u00f6rnyezetet is. Ha a compose f\u00e1jlt megfelel\u0151en \u00edrjuk meg (pl. haszn\u00e1ljuk a restart direkt\u00edv\u00e1t is), az elind\u00edtott szolg\u00e1ltat\u00e1sok automatikusan \u00fajraindulnak a rendszer indul\u00e1sakor.","title":"Docker-compose"},{"location":"Docker/Dockerfile-compose/#tobb-compose-yaml-fajl","text":"A docker-compose parancsnak nem adtuk meg, hogy milyen yaml f\u00e1jlb\u00f3l dolgozzon. Alap\u00e9rtelmez\u00e9sk\u00e9nt a docker-compose.yaml kiterjeszt\u00e9s\u0171 f\u00e1jlt \u00e9s ezzel \u00f6sszef\u00e9s\u00fclve a docker-compose.override.yaml f\u00e1jlt haszn\u00e1lja. K\u00e9sz\u00edts egy docker-compose.override.yaml f\u00e1jlt a m\u00e1sik compose yaml mell\u00e9 az al\u00e1bbi tartalommal version : \"3\" services : redis : command : redis-server --loglevel verbose Ind\u00edtsd el a rendszert docker-compose up paranccsal. A redis kont\u00e9ner r\u00e9szletesebben fog napl\u00f3zni a command direkt\u00edv\u00e1ban megadott utas\u00edt\u00e1s szerint. \u00c1ll\u00edtsd le a rendszert. Nevezd \u00e1t az el\u0151bbi override f\u00e1jlt docker-compose.debug.yaml -re. K\u00e9sz\u00edts egy \u00faj docker-compose.prod.yaml f\u00e1jlt a t\u00f6bbi yaml mell\u00e9 az al\u00e1bbi tartalommal version : \"3\" services : redis : command : redis-server --loglevel warning Ind\u00edtsuk el a rendszert az al\u00e1bbi paranccsal docker-compose -f docker-compose.yml -f docker-compose.prod.yml up A -f kapcsol\u00f3val tudjuk k\u00e9rni a megadott yaml f\u00e1jlok \u00f6sszef\u00e9s\u00fcl\u00e9s\u00e9t. \u00c1ltal\u00e1ban a docker-compose.yaml -be ker\u00fclnek a k\u00f6z\u00f6s konfigur\u00e1ci\u00f3k, \u00e9s a tov\u00e1bbi f\u00e1jlokba a k\u00f6rnyezet specifikus konfigur\u00e1ci\u00f3k.","title":"T\u00f6bb compose yaml f\u00e1jl"},{"location":"Docker/Dockerfile-compose/#tipikusan-hasznalt-image-ek","text":"Kont\u00e9ner alap\u00fa fejleszt\u00e9sn\u00e9l teh\u00e1t k\u00e9t f\u00e9le image-et haszn\u00e1lunk: amit magunk k\u00e9sz\u00edt\u00fcnk egy adott alap image-re \u00e9p\u00fclve, illetve k\u00e9sz image-eket, amiket csak futtatunk (\u00e9s esetleg konfigur\u00e1lunk). Alap image-ek, amikre tipikusan saj\u00e1t alkalmaz\u00e1st \u00e9p\u00edt\u00fcnk: Linux disztrib\u00faci\u00f3k Ubuntu , pl. ubuntu:18.04 Debian , pl. debian:stretch Alpine , pl. alpine:3.10 Futtat\u00f3 platformok .NET Core , pl. mcr.microsoft.com/dotnet/core/runtime:2.1.12-bionic OpenJDK pl. openjdk:11-jre-stretch NodeJS pl. node:12.7.0-alpine Python pl. python:2.7-slim scratch : \u00fcres image, speci\u00e1lis esetek, pl. go A k\u00e9sz image-ek, amiket pedig felhaszn\u00e1lunk: Adatb\u00e1zis szerverek, webszerverek, gyakran haszn\u00e1lt szolg\u00e1ltat\u00e1sok MSSQL Server, redis, mongodb, mysql, nginx, ... Term\u00e9rdek el\u00e9rhet\u0151 image: https://hub.docker.com Verzi\u00f3z\u00e1s fontos Az image-ek verzi\u00f3z\u00e1s\u00e1t minden esetben meg kell \u00e9rteni! Minden image m\u00e1s-m\u00e1s megk\u00f6zel\u00edt\u00e9st alkalmaz.","title":"Tipikusan haszn\u00e1lt image-ek"},{"location":"Docker/Dockerfile-compose/#kesz-image-testreszabasa","text":"Az el\u0151bb a Redis mem\u00f3ria alap\u00fa adatb\u00e1zist minden konfigur\u00e1ci\u00f3 n\u00e9lk\u00fcl felhaszn\u00e1ltuk. Gyakran az ilyen image-ek \"majdnem\" j\u00f3k k\u00f6zvetlen felhaszn\u00e1l\u00e1sra, de az\u00e9rt sz\u00fcks\u00e9g van egy kev\u00e9s testreszab\u00e1sra. Ilyen esetben a k\u00f6vetkez\u0151 lehet\u0151s\u00e9geink vannak: Saj\u00e1t image-et k\u00e9sz\u00edt\u00fcnk kiindulva a sz\u00e1munkra megfelel\u0151 alap image-b\u0151l. A saj\u00e1t image-ben m\u00f3dos\u00edthatunk a konfigur\u00e1ci\u00f3s f\u00e1jlokat, avagy tov\u00e1bbi f\u00e1jlokat adhatunk az image-be. Ezt a megold\u00e1st alkalmazhatjuk p\u00e9ld\u00e1ul tipikusan weboldal kiszolg\u00e1l\u00e1s\u00e1n\u00e1l, ahol is a kiindul\u00f3 image a webszerver, viszont a kiszolg\u00e1land\u00f3 f\u00e1jlokat m\u00e9g mell\u00e9 kell tenn\u00fcnk. K\u00f6rnyezeti v\u00e1ltoz\u00f3kon kereszt\u00fcl konfigur\u00e1ljuk a futtatand\u00f3 szolg\u00e1ltat\u00e1st. Az alap image-ek \u00e1ltal\u00e1ban el\u00e9g j\u00f3 konfigur\u00e1ci\u00f3val rendelkeznek, csak keveset kell rajta m\u00f3dos\u00edtanunk. Erre t\u00f6k\u00e9letesen alkalmas egy-egy k\u00f6rnyezeti v\u00e1ltoz\u00f3. A j\u00f3l fel\u00e9p\u00edtett Docker image-ek el\u0151re meghat\u00e1rozott k\u00f6rnyezeti v\u00e1ltoz\u00f3kon kereszt\u00fcl testreszabhat\u00f3ak. Erre egy j\u00f3 p\u00e9lda a Microsoft SQL Server Docker v\u00e1ltozata . Az al\u00e1bbi parancsban a -e argumentumokban adunk \u00e1t k\u00f6rnyezeti v\u00e1ltoz\u00f3kat: docker run -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=yourStrong(!)Password' -p 1433 :1433 mcr.microsoft.com/mssql/server:2017-CU8-ubuntu Becsatolhatjuk a saj\u00e1t konfigur\u00e1ci\u00f3s f\u00e1jljainkat a kont\u00e9nerbe. Kor\u00e1bban l\u00e1ttuk, hogy lehet\u0151s\u00e9g\u00fcnk van egy mapp\u00e1t a host g\u00e9ptr\u0151l a kont\u00e9nerbe csatolni. Ha elk\u00e9sz\u00edtj\u00fck a testreszabott konfigur\u00e1ci\u00f3s f\u00e1jlt, akkor a docker-compose.yml le\u00edr\u00e1sban a k\u00f6vetkez\u0151 m\u00f3don tudjuk ezt a f\u00e1jlt becsatolni a kont\u00e9ner indul\u00e1sakor. services : redis : image : redis:5.0.5-alpine volumes : - my-redis.conf:/usr/local/etc/redis/redis.conf Ezen megold\u00e1s el\u0151nye, hogy nincs sz\u00fcks\u00e9g saj\u00e1t image-et k\u00e9sz\u00edteni, t\u00e1rolni, kezelni. Amikor a k\u00f6rnyezeti v\u00e1ltoz\u00f3 m\u00e1r nem elegend\u0151 a testreszab\u00e1shoz, ez a javasolt megold\u00e1s.","title":"K\u00e9sz image testreszab\u00e1sa"},{"location":"Docker/Dockerfile-compose/#microsoft-visual-studio-tamogatas-docker-alapu-fejleszteshez","text":"A Microsoft Visual Studio a 2017-es verzi\u00f3 \u00f3ta t\u00e1mogatja \u00e9s megk\u00f6nny\u00edti a kont\u00e9ner alap\u00fa szoftverfejleszt\u00e9st. Seg\u00edti a fejleszt\u0151t a fejlesztett alkalmaz\u00e1s kont\u00e9neriz\u00e1l\u00e1s\u00e1ban, \u00e9s t\u00e1mogatja a kont\u00e9nerben val\u00f3 debuggol\u00e1st is. Enged\u00e9lyezz\u00fck Docker-ben a volume sharing-et! Ind\u00edtsuk el a Visual Studio-t (nem a Code-ot!). K\u00e9sz\u00edts\u00fcnk egy \u00faj ASP.NET Core Web Application t\u00edpus\u00fa projektet a Web application sablonnal, \u00e9s enged\u00e9lyezz\u00fck a Linux-alap\u00fa kont\u00e9ner t\u00e1mogat\u00e1st a projektben. (A pontos l\u00e9p\u00e9sek Visual Studio verzi\u00f3 f\u00fcgg\u0151ek, l\u00e1sd itt .) N\u00e9zz\u00fck meg \u00e9s \u00e9rts\u00fck meg az elk\u00e9sz\u00fclt solution strukt\u00far\u00e1t, valamint a Dockerfile -t \u00e9s a docker-compose f\u00e1jlokat is. Ind\u00edtsuk el az alkalmaz\u00e1st Docker-compose haszn\u00e1lat\u00e1val: legyen a docker-compose a startup projekt\u00fcnk, \u00e9s F5-tel ind\u00edtsuk debug m\u00f3dban az alkalmaz\u00e1st. Egy konzolb\u00f3l list\u00e1zzuk ki a fut\u00f3 kont\u00e9nereket: docker ps Debuggol\u00e1s kont\u00e9nerben is A Visual Studio Code is t\u00e1mogatja a kont\u00e9nerek debuggol\u00e1s\u00e1t. Err\u0151l r\u00e9szletesen l\u00e1sd itt .","title":"Microsoft Visual Studio t\u00e1mogat\u00e1s Docker-alap\u00fa fejleszt\u00e9shez"},{"location":"Docker/Dockerfile-compose/#tovabbi-olvasnivalo","text":"Dockerfile szintaktika: https://docs.docker.com/engine/reference/builder/ .dockerignore f\u00e1jl szintaktika: https://docs.docker.com/engine/reference/builder/#dockerignore-file Dockerfile best practice-ek: https://docs.docker.com/develop/develop-images/dockerfile_best-practices/ compose f\u00e1jl szintaktika: https://docs.docker.com/compose/compose-file/ T\u00f6bb compose f\u00e1jl haszn\u00e1lata: https://docs.docker.com/compose/extends/#multiple-compose-files Multistage build-ek: https://docs.docker.com/develop/develop-images/multistage-build/","title":"Tov\u00e1bbi olvasnival\u00f3"},{"location":"Kubernetes/Kubernetes-alapok/","text":"El\u0151ad\u00e1s \u00b6 Kubernetes bevezet\u0151 C\u00e9l \u00b6 A labor c\u00e9lja megismerni a Kubernetes haszn\u00e1lat\u00e1nak alapjait, a podok , Deployment-ek \u00e9s ReplicaSet-ek l\u00e9trehoz\u00e1s\u00e1t \u00e9s kezel\u00e9s\u00e9t, valamint a leggyakrabban haszn\u00e1lt kubectl parancsokat. El\u0151k\u00f6vetelm\u00e9nyek \u00b6 Kubernetes B\u00e1rmely felh\u0151 platform \u00e1ltal biztos\u00edtott klaszter Linux platformon: minikube Windows platformon: Docker Desktop kubectl A bin\u00e1risa legyen el\u00e9rhet\u0151 PATH-on. Feladatok \u00b6 El\u0151k\u00e9sz\u00fclet Docker Desktop-on \u00b6 Praktikus, ha le\u00e1ll\u00edtunk minden fut\u00f3 kont\u00e9nert, amire nincs sz\u00fcks\u00e9g\u00fcnk. Haszn\u00e1lhatjuk a k\u00f6vetkez\u0151 parancsot PowerShell-ben: docker rm -f $(docker ps -aq) Nyissuk meg a Docker Desktop be\u00e1ll\u00edt\u00e1sait. A Kubernetes f\u00fcl\u00f6n pip\u00e1ljuk be az Enable Kubernetes opci\u00f3t, \u00e9s kattintsunk az Apply -ra. V\u00e1rjuk meg, am\u00edg befejez\u0151dik a m\u0171velet. Kubectl csatlakoz\u00e1s a klaszterhez \u00b6 Ellen\u0151rizz\u00fck, hogy a kubectl bin\u00e1ris el\u00e9rhet\u0151-e, \u00e9s tud-e csatlakozni a klaszterhez: kubectl version A kubectl a CLI kliens a klaszter kezel\u00e9s\u00e9hez. A Kubernetes API szerver\u00e9hez csatlakozik, annak REST API-j\u00e1n kereszt\u00fcl v\u00e9gzi a m\u0171veleteket. L\u00e1thatjuk mind a kliens, mind a klaszter verzi\u00f3 inform\u00e1ci\u00f3it. A kubectl egy konkr\u00e9t klaszterhez csatlakozik. N\u00e9zz\u00fck meg, milyen klasztereket ismer: kubectl config get-contexts Ha t\u00f6bb klaszterrel dolgozn\u00e1nk, itt l\u00e1thatn\u00e1nk \u0151ket. Ezek val\u00f3j\u00e1ban egy konfigur\u00e1ci\u00f3s f\u00e1jlban vannak: $HOME/.kube/config V\u00e1ltani a kubectl config use-context <n\u00e9v> parancssal lehet. Minden parancsn\u00e1l k\u00fcl\u00f6n megadhatjuk a kontextust a --context kapcsol\u00f3val, de ink\u00e1bb az implicit contextust szoktuk haszn\u00e1lni. Kontextus be\u00e1ll\u00edt\u00e1sr\u00f3l r\u00e9szletesebben itt . Podok \u00e9s n\u00e9vterek list\u00e1z\u00e1sa \u00b6 List\u00e1zzuk ki a fut\u00f3 podokat: kubectl get pod -A A -A vagy --all-namespaces kapcsol\u00f3 az \u00f6sszes n\u00e9vt\u00e9rben lev\u0151 podot list\u00e1zza. Ism\u00e9telj\u00fck meg a -A kapcsol\u00f3 n\u00e9lk\u00fcl: kubectl get pod Ez az alap\u00e9rtelmezett default n\u00e9vt\u00e9r podjait list\u00e1zza. (Az alap\u00e9rtelmezett n\u00e9vt\u00e9r is a kontextus be\u00e1ll\u00edt\u00e1sa.) N\u00e9zz\u00fck meg, milyen n\u00e9vterek vannak: kubectl get namespace List\u00e1zzuk a podokat egy konkr\u00e9t n\u00e9vt\u00e9rben: kubectl get pod -n kube-system Pod l\u00e9trehoz\u00e1sa \u00b6 A futtat\u00e1s elemi egys\u00e9ge a pod. Ind\u00edtsunk el egy podot. A l\u00e9trehoz\u00e1shoz podot yaml le\u00edr\u00f3ban defini\u00e1ljuk, \u00e9s a kubectl create parancsnak \u00e1tadjuk stdin-r\u0151l olvasva ( Windows Command promptban adjuk ki a parancsot): kubectl create -f - apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: ubuntu:16.04 args: [ bash, -c, 'for ((i=0; ;i++));do echo \"$i: $(date)\";sleep 5;done' ] A v\u00e9g\u00e9n nyomjunk egy Ctrl-Z-t a le\u00edr\u00f3 befejez\u00e9s\u00e9nek jelz\u00e9s\u00e9hez. A pod l\u00e9trej\u00f6tt. Ellen\u0151rizz\u00fck: kubectl get pod N\u00e9zz\u00fck meg a pod logjait: kubectl logs counter Ha gondoljuk, tegy\u00fck hozz\u00e1 a -f kapcsol\u00f3t is ( kubectl logs -f counter ) a log k\u00f6vet\u00e9s\u00e9hez. Ctrl-C-vel l\u00e9phet\u00fcnk ki a log folyamatos k\u00f6vet\u00e9s\u00e9b\u0151l. T\u00f6r\u00f6lj\u00fck a podot: kubectl delete pod counter Ellen\u0151rizz\u00fck, hogy a pod t\u00e9nyleg elt\u0171nik egy kis id\u0151 m\u00falva: kubectl get pod A pod t\u00f6rl\u00e9se nem azonnali. A benne fut\u00f3 kont\u00e9nerek le\u00e1ll\u00e1s jelz\u00e9st kapnak, \u00e9s \u0151k maguk termin\u00e1lhatnak. Ha ez nem t\u00f6rt\u00e9nik, meg, akkor kis id\u0151 m\u00falva megsz\u0171nteti \u0151ket a rendszer. Yaml le\u00edr\u00f3 f\u00e1jl \u00b6 A yaml le\u00edr\u00f3 beg\u00e9pel\u00e9se a fentiek szerint nem k\u00e9nyelmes. Tipikusan komplex pod \u00e9s egy\u00e9b er\u0151forr\u00e1s defin\u00edci\u00f3kkal dolgozunk. Tegy\u00fck ink\u00e1bb egy f\u00e1jlba. Hozzunk l\u00e9tre egy \u00faj yaml f\u00e1jt createpod.yml n\u00e9ven. Haszn\u00e1lhatjuk p\u00e9ld\u00e1ul Visual Studio Code-ot. \u00c9rdemes olyan sz\u00f6vegszerkeszt\u0151vel dolgozni, amely ismeri a yaml szintaktik\u00e1t. M\u00e1soljuk be a yaml f\u00e1jlba az al\u00e1bbiakat. apiVersion : v1 kind : Pod metadata : name : counter spec : containers : - name : count image : ubuntu:16.04 args : [ bash , -c , 'for ((i=0; ;i++));do echo \"$i: $(date)\";sleep 5;done' ] A konzolunkban navig\u00e1ljunk el abba a k\u00f6nyvt\u00e1rba, ahol a yaml f\u00e1jl van. Hozzuk l\u00e9tre a podot: kubectl create -f createpod.yml A pod l\u00e9trej\u00f6tt. Ellen\u0151rizz\u00fck: kubectl get pod , majd t\u00f6r\u00f6lj\u00fck a podot: kubectl delete pod counter Deployment l\u00e9trehoz\u00e1sa \u00b6 A podokat nem szoktuk k\u00f6zvetlen\u00fck l\u00e9trehozni, hanem Deployment -re \u00e9s ReplicaSet -re szoktunk b\u00edzni a kezel\u00e9s\u00fcket \u00e9s l\u00e9trehoz\u00e1sukat. Hozzunk l\u00e9tre egy \u00faj yaml f\u00e1jl createdepl.yml n\u00e9ven az al\u00e1bbi tartalommal. apiVersion : apps/v1 kind : Deployment metadata : name : counter spec : replicas : 1 selector : matchLabels : app : counter template : metadata : labels : app : counter spec : containers : - name : count image : ubuntu:16.04 args : [ bash , -c , 'for ((i=0; ;i++));do echo \"$i: $(date)\";sleep 5;done' ] Hozzuk l\u00e9tre a Deployment-et: kubectl apply -f createdepl.yml Ez\u00fattal nem create , hanem apply parancsot haszn\u00e1lunk. Az apply l\u00e9trehozza, ha nem l\u00e9tezik, \u00e9s m\u00f3dos\u00edtja az er\u0151forr\u00e1st, ha m\u00e1r l\u00e9tezik. List\u00e1zzuk a Deployment-eket, ReplicaSet-eket \u00e9s a podokat: kubectl get deployment kubectl get replicaset kubectl get pod Vegy\u00fck \u00e9szre, hogy a pod neve gener\u00e1lt, a Deployment \u00e9s a ReplicaSet alapj\u00e1n kap automatikusan egyet. V\u00e1ltoztassuk meg a program fut\u00e1s\u00e1t: ne 5, hanem 10 m\u00e1sodpercenk\u00e9nt \u00edrjuk ki az id\u0151t. Ezt a Deployment m\u00f3dos\u00edt\u00e1s\u00e1val fogjuk megtenni. Gondolhatunk arra is, hogy a podot szerkessz\u00fck, de azt nem tehetj\u00fck meg. Egy fut\u00f3 pod nem cser\u00e9lhet\u0151 le. Ehelyett val\u00f3j\u00e1ban egy \u00faj podot fogunk l\u00e9trehozni indirekten. \u00cdrjuk \u00e1t a yaml f\u00e1jlban a bash parancsban a sleep-et 10-re. Ments\u00fck a f\u00e1jlt. Alkalmazzuk a v\u00e1ltoztat\u00e1sokat: kubectl apply -f createdepl.yml N\u00e9zz\u00fck a podok v\u00e1ltoz\u00e1s\u00e1t: kubectl get pod Id\u0151z\u00edt\u00e9s f\u00fcgg\u0151en j\u00f3 es\u00e9llyel l\u00e1tni fogjuk a r\u00e9gi le\u00e1ll\u00f3 podot, \u00e9s az \u00fajat is. \"L\u00e9pj\u00fcnk be\" a fut\u00f3 podba egy \u00faj, interakt\u00edv shellben. M\u00e1soljuk ki a fut\u00f3 pod nev\u00e9t, \u00e9s adjuk ki a k\u00f6vetkez\u0151 parancsot: kubectl exec -it <podn\u00e9v> /bin/bash Ahogy a docker-n\u00e9l m\u00e1r l\u00e1thattuk, egy \u00faj shell indul a pod kont\u00e9ner\u00e9ben, \u00e9s ehhez csatlakozunk. Ebben a shellben, ahogy nat\u00edv docker eset\u00e9ben is, b\u00e1rmit megtehet\u00fcnk. T\u00f6r\u00f6lj\u00fck a Deployment -et: kubectl delete deployment counter Ez a ReplicaSet -et \u00e9s a podokat is t\u00f6r\u00f6lni fogja. A Deployment szolg\u00e1l az alkalmaz\u00e1s verzi\u00f3nak friss\u00edt\u00e9s\u00e9re, kiad\u00e1s\u00e1ra. Podok helyett leggyakrabban Deployment -eket defini\u00e1lunk. Kubectl parancsok \u00b6 A kubectl leggyakrabban haszn\u00e1lt parancsainak szerkezete: kubectl <ige> <er\u0151forr\u00e1s> <attrib\u00fatumok> . Az ige p\u00e9ld\u00e1ul: get : list\u00e1zza az er\u0151forr\u00e1sokat create : l\u00e9trehoz egy er\u0151forr\u00e1st delete : t\u00f6r\u00f6l egy er\u0151forr\u00e1st describe : lek\u00e9rdezi az er\u0151forr\u00e1s r\u00e9szletes \u00e1llapot\u00e1t edit : let\u00f6lti az er\u0151forr\u00e1s le\u00edr\u00f3j\u00e1t, \u00e9s megnyitja sz\u00f6vegszerkeszt\u0151ben; ment\u00e9s \u00e9s bez\u00e1r\u00e1s ut\u00e1n friss\u00edti a klaszterben az er\u0151forr\u00e1st a m\u00f3dos\u00edt\u00e1sok alapj\u00e1n Az er\u0151forr\u00e1sok a pod , replicaset vagy r\u00f6viden rs , a deployment , stb. A parancsokr\u00f3l -h kapcsol\u00f3val kaphatunk seg\u00edts\u00e9get, pl. kubectl describe -h Dashboard \u00e9s proxy-z\u00e1s \u00b6 A Web UI / Dashboard egy webalkalmaz\u00e1s, amely maga is Kubernetes alatt fut. Az alkalmaz\u00e1s a klaszter tartalm\u00e1t jelen\u00edti meg egy egyszer\u0171, de k\u00f6nnyen \u00e1ttekinthet\u0151 webes fel\u00fcleten. A dashboard alapvet\u0151en felhaszn\u00e1l\u00f3 authentik\u00e1ci\u00f3 ut\u00e1n \u00e9rhet\u0151 el. Az egyszer\u0171s\u00e9g v\u00e9gett mi ezt most kikapcsoljuk, \u00e9s authentik\u00e1ci\u00f3 n\u00e9lk\u00fcl fogjuk haszn\u00e1lni. Telep\u00edts\u00fck a dashboard-ot: Telep\u00edts\u00fck alapbe\u00e1ll\u00edt\u00e1sokkal: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/alternative.yaml Szerkessz\u00fcnk bele a kont\u00e9ner argumentumaiba, hogy enged\u00e9lyezz\u00fck az anonim bel\u00e9p\u00e9st: kubectl edit deployment kubernetes-dashboard --namespace kubernetes-dashboard A parancs let\u00f6lti \u00e9s megnyitja a deployment le\u00edr\u00f3j\u00e1t. Keress\u00fck meg a kont\u00e9ner ind\u00edt\u00e1si argumentumait, \u00e9s adjunk hozz\u00e1 m\u00e9g k\u00e9t sort (\u00fcgyelj\u00fcnk a megfelel\u0151 beh\u00faz\u00e1sra): - args : - --namespace=kubernetes-dashboard - --enable-insecure-login - --enable-skip-login - --authentication-mode=basic Csak fejleszt\u0151i g\u00e9pen Ez csak helyi m\u0171k\u00f6d\u00e9si m\u00f3dban javasolt! Most sz\u00e1nd\u00e9kosan kiker\u00fclt\u00fck az authentik\u00e1ci\u00f3t! \u00c9rtelemszer\u0171en a dashboard-ot csak egyszer kell egy klaszterbe telep\u00edteni. Adjunk egy p\u00e1r m\u00e1sodpercet a fel\u00e1ll\u00e1sra. N\u00e9zz\u00fck meg, hogy rendben fut-e: kubectl get pods -n kubernetes-dashboard Akkor j\u00f3, ha a kubernetes-dashboard-... nev\u0171 pod running \u00e1llapotban van. A webalkalmaz\u00e1s csak a klaszteren bel\u00fcl \u00e9rhet\u0151 el egyel\u0151re. L\u00e1tni fogjuk k\u00e9s\u0151bb, hogyan tudunk a klaszteren k\u00edv\u00fclre \"publik\u00e1lni\". Egyel\u0151re azonban egy m\u00e1sik, fejleszt\u00e9shez k\u00e9nyelmesen haszn\u00e1lhat\u00f3 megold\u00e1st alkalmazunk. Nyissunk egy \u00faj konzolt, \u00e9s adjuk ki a kubectl proxy parancsot. A fut\u00f3 proxy folyamatos kapcsolatot tart fenn a klaszterrel, \u00e9s a Kubernetes Api szervert haszn\u00e1lja \"bel\u00e9p\u00e9si pontnak\". Az proxy a helyi 8081 portra k\u00fcld\u00f6tt k\u00e9r\u00e9seket tov\u00e1bb\u00edtja a klaszteren bel\u00fclre az URL alapj\u00e1n. Ez a proxy nem csak helyben futtatott klaszterrel haszn\u00e1lhat\u00f3. Ha a felh\u0151ben, t\u0151l\u00fcnk t\u00e1vol fut a klaszter, ugyan\u00edgy tudunk vele kommunik\u00e1lni. Nyissuk meg b\u00f6ng\u00e9sz\u0151ben: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/kubernetes-dashboard:/proxy/ \u00c9rtelmezz\u00fck az URL-t: localhost:8001 : a proxy helyi portja /api/v1 : Kubernetes Api verzi\u00f3, ez mindig \u00edgy n\u00e9z ki /namespaces/kubernetes-dashboard : a kubernetes-dashboard n\u00e9vt\u00e9rben szeretn\u00e9nk el\u00e9rni egy elemet /services/kubernetes-dashboard: a n\u00e9vt\u00e9ren bel\u00fcl a service-ek k\u00f6z\u00fcl a kubernetes-dashboard nev\u0171 szolg\u00e1ltat\u00e1st szeretn\u00e9nk el\u00e9rni /proxy : k\u00e9rj\u00fck az er\u0151forr\u00e1s proxy-z\u00e1s\u00e1t (ez mindig \u00edgy n\u00e9z ki) Ismerkedj\u00fcnk meg a dashboard webes fel\u00fclet\u00e9vel: N\u00e9zz\u00fck meg a bal oldali men\u00fc elemeit. V\u00e1lasszunk n\u00e9vteret. N\u00e9zz\u00fck meg a podokat. Pod - ReplicaSet - Deployment - Service kapcsolata \u00b6 Haszn\u00e1ljuk a dashboard-ot a k\u00f6vetkez\u0151kh\u00f6z. A feladatban v\u00e9gig a kubernetes-dashboard n\u00e9vt\u00e9rben dolgozunk. Keress\u00fck ki a \"kubernetes-dashboard\" nev\u0171 service-t. N\u00e9zz\u00fck meg a service adatait, kifejezetten a \"Pods\" r\u00e9szt. Eml\u00e9kezz\u00fcnk arra, hogy a service teszi el\u00e9rhet\u0151v\u00e9 podok szolg\u00e1ltat\u00e1sait/v\u00e9gpontjait. A service-nek teh\u00e1t tudnia kell, mely podok \"\u00e1llnak m\u00f6g\u00f6tte\". Ezt l\u00e1tjuk a \"Pods\" list\u00e1ban. Sk\u00e1l\u00e1zzuk fel a dashboard-ot, futtassunk k\u00e9t p\u00e9ld\u00e1nyt. Keress\u00fck meg a \"kubernetes-dashboard\" deployment-et. A webes fel\u00fclet tetej\u00e9n a k\u00e9k sz\u00edn\u0171 sorban a jobb oldalon keress\u00fck meg a sk\u00e1l\u00e1z\u00e1s ikont. Nyissuk meg, \u00e9s k\u00e9rj\u00fcnk 3 p\u00e9ld\u00e1nyt. N\u00e9zz\u00fck meg a podok list\u00e1j\u00e1t. L\u00e1ssuk, hogy most m\u00e1r 3 \"kubernetes-dashboard-\" n\u00e9vvel kezd\u0151d\u0151 pod lesz. T\u00e9rj\u00fcnk vissza a \"kubernetes-dashboard\" service-hez. N\u00e9zz\u00fck meg, hogyan van \u00f6sszek\u00f6tve a podokkal. N\u00e9zz\u00fck meg a podokat. Vegy\u00fck \u00e9szre, hogy a service \"\u00e9szrevette\", hogy most m\u00e1r 3 pod felett rendelkezik. N\u00e9zz\u00fck meg a service le\u00edr\u00f3j\u00e1t. A webes fel\u00fcleten a k\u00e9k s\u00e1rban a ceruza ikonra kattintsunk. Keress\u00fck meg a spec le\u00edr\u00f3ban a selector -t. kind : Service apiVersion : v1 metadata : name : kubernetes-dashboard # service elem neve ... spec : ... selector : k8s-app : kubernetes-dashboard # label, amelyet a podokon keres Ez a service azon podok fel\u00e9 osztja sz\u00e9t a k\u00e9r\u00e9seket, amelyek a k8s-app c\u00edmk\u00e9ben kubernetes-dashboard \u00e9rt\u00e9kkel rendelkeznek. N\u00e9zz\u00fck meg a podban ugyanezt. A service alatt b\u00e1rmelyik podra kattintva navig\u00e1ljunk el a podhoz, \u00e9s k\u00e9rj\u00fck le a le\u00edr\u00f3j\u00e1t az el\u0151bbiek szerint. Keress\u00fck meg a pod metaadataiban a fenti labelt. kind : Pod apiVersion : v1 metadata : name : kubernetes-dashboard-57c9d8c7c4-z98z2 # pod saj\u00e1t, gener\u00e1lt neve labels : k8s-app : kubernetes-dashboard # pod c\u00edmk\u00e9je, amivel a service r\u00e1tal\u00e1l A pod oldal\u00e1n maradva keress\u00fck meg a \"Controlled by\" r\u00e9szt. L\u00e1thatjuk, hogy a podot egy ReplicaSet kezeli. Menj\u00fcnk a ReplicaSet-hez. Nyissuk meg a ReplicaSet le\u00edr\u00f3j\u00e1t is. kind : ReplicaSet apiVersion : extensions/v1beta1 metadata : name : kubernetes-dashboard-57c9d8c7c4 labels : k8s-app : kubernetes-dashboard # ReplicaSet c\u00edmk\u00e9je, amivel a felette \u00e1ll\u00f3 Deployment azonos\u00edtja \u0151t spec : replicas : 3 # ennyi replik\u00e1t k\u00e9rt\u00fcnk selector : matchLabels : k8s-app : kubernetes-dashboard # Label, amivel a podjait azonos\u00edtja template : # pod defini\u00e1l\u00e1s\u00e1hoz haszn\u00e1lt template, minden pod ez alapj\u00e1n j\u00f6n l\u00e9tre metadata : labels : k8s-app : kubernetes-dashboard # amikor a ReplicaSet l\u00e9trehoz egy podot, ezt a c\u00edmk\u00e9t teszi r\u00e1 # meg kell egyezzen p\u00e1r sorral feljebb a selector-ban defini\u00e1lttak spec : containers : # le\u00edrja a podban a kont\u00e9nert - name : kubernetes-dashboard image : \"kubernetesui/dashboard:v2.0.3\" args : - \"--namespace=kubernetes-dashboard\" ports : - containerPort : 9090 protocol : TCP Ha m\u00e9g eggyel \"feljebb\" l\u00e9p\u00fcnk, a ReplicaSet-b\u0151l a Deployment-be, hasonl\u00f3 \u00f6sszekapcsol\u00e1st tal\u00e1ln\u00e1nk. Pod \"\u00fajraind\u00edt\u00e1sa\" \u00b6 Maradva a kubernetes-dashboard n\u00e9vt\u00e9rben list\u00e1zzuk a podokat. V\u00e1lasszunk ki egy dashboard podot, \u00e9s t\u00f6r\u00f6lj\u00fck. A pod sor v\u00e9g\u00e9n a ... ikon alatt v\u00e1lasszuk a Delete -et. V\u00e1rjunk egy p\u00e1r m\u00e1sodpercet, \u00e9s friss\u00edts\u00fcnk r\u00e1 az oldalra. Egy \u00faj pod sz\u00fcletett. Az Age oszlop alapj\u00e1n l\u00e1thatjuk. (Lehet, hogy a t\u00f6r\u00f6ltet is l\u00e1tjuk m\u00e9g, le\u00e1ll\u00e1s alatt.) Ez a ReplicaSet feladata: garant\u00e1lni, hogy legyen 3 p\u00e9ld\u00e1ny. \u00dajraind\u00edt\u00e1s A pod t\u00f6rl\u00e9se megfelel egy komponens \u00fajraind\u00edt\u00e1s\u00e1nak. Ha t\u00f6bb p\u00e9ld\u00e1nyunk van, akkor mindegyiket k\u00e9zzel tudjuk \u00edgy t\u00f6r\u00f6lni. Ez persze csak akkor m\u0171k\u00f6dik, ha a pod felett van egy controller, ami sz\u00fcks\u00e9g szerint l\u00e9trehozza az \u00faj podokat.","title":"Kubernetes alapok"},{"location":"Kubernetes/Kubernetes-alapok/#eloadas","text":"Kubernetes bevezet\u0151","title":"El\u0151ad\u00e1s"},{"location":"Kubernetes/Kubernetes-alapok/#cel","text":"A labor c\u00e9lja megismerni a Kubernetes haszn\u00e1lat\u00e1nak alapjait, a podok , Deployment-ek \u00e9s ReplicaSet-ek l\u00e9trehoz\u00e1s\u00e1t \u00e9s kezel\u00e9s\u00e9t, valamint a leggyakrabban haszn\u00e1lt kubectl parancsokat.","title":"C\u00e9l"},{"location":"Kubernetes/Kubernetes-alapok/#elokovetelmenyek","text":"Kubernetes B\u00e1rmely felh\u0151 platform \u00e1ltal biztos\u00edtott klaszter Linux platformon: minikube Windows platformon: Docker Desktop kubectl A bin\u00e1risa legyen el\u00e9rhet\u0151 PATH-on.","title":"El\u0151k\u00f6vetelm\u00e9nyek"},{"location":"Kubernetes/Kubernetes-alapok/#feladatok","text":"","title":"Feladatok"},{"location":"Kubernetes/Kubernetes-alapok/#elokeszulet-docker-desktop-on","text":"Praktikus, ha le\u00e1ll\u00edtunk minden fut\u00f3 kont\u00e9nert, amire nincs sz\u00fcks\u00e9g\u00fcnk. Haszn\u00e1lhatjuk a k\u00f6vetkez\u0151 parancsot PowerShell-ben: docker rm -f $(docker ps -aq) Nyissuk meg a Docker Desktop be\u00e1ll\u00edt\u00e1sait. A Kubernetes f\u00fcl\u00f6n pip\u00e1ljuk be az Enable Kubernetes opci\u00f3t, \u00e9s kattintsunk az Apply -ra. V\u00e1rjuk meg, am\u00edg befejez\u0151dik a m\u0171velet.","title":"El\u0151k\u00e9sz\u00fclet Docker Desktop-on"},{"location":"Kubernetes/Kubernetes-alapok/#kubectl-csatlakozas-a-klaszterhez","text":"Ellen\u0151rizz\u00fck, hogy a kubectl bin\u00e1ris el\u00e9rhet\u0151-e, \u00e9s tud-e csatlakozni a klaszterhez: kubectl version A kubectl a CLI kliens a klaszter kezel\u00e9s\u00e9hez. A Kubernetes API szerver\u00e9hez csatlakozik, annak REST API-j\u00e1n kereszt\u00fcl v\u00e9gzi a m\u0171veleteket. L\u00e1thatjuk mind a kliens, mind a klaszter verzi\u00f3 inform\u00e1ci\u00f3it. A kubectl egy konkr\u00e9t klaszterhez csatlakozik. N\u00e9zz\u00fck meg, milyen klasztereket ismer: kubectl config get-contexts Ha t\u00f6bb klaszterrel dolgozn\u00e1nk, itt l\u00e1thatn\u00e1nk \u0151ket. Ezek val\u00f3j\u00e1ban egy konfigur\u00e1ci\u00f3s f\u00e1jlban vannak: $HOME/.kube/config V\u00e1ltani a kubectl config use-context <n\u00e9v> parancssal lehet. Minden parancsn\u00e1l k\u00fcl\u00f6n megadhatjuk a kontextust a --context kapcsol\u00f3val, de ink\u00e1bb az implicit contextust szoktuk haszn\u00e1lni. Kontextus be\u00e1ll\u00edt\u00e1sr\u00f3l r\u00e9szletesebben itt .","title":"Kubectl csatlakoz\u00e1s a klaszterhez"},{"location":"Kubernetes/Kubernetes-alapok/#podok-es-nevterek-listazasa","text":"List\u00e1zzuk ki a fut\u00f3 podokat: kubectl get pod -A A -A vagy --all-namespaces kapcsol\u00f3 az \u00f6sszes n\u00e9vt\u00e9rben lev\u0151 podot list\u00e1zza. Ism\u00e9telj\u00fck meg a -A kapcsol\u00f3 n\u00e9lk\u00fcl: kubectl get pod Ez az alap\u00e9rtelmezett default n\u00e9vt\u00e9r podjait list\u00e1zza. (Az alap\u00e9rtelmezett n\u00e9vt\u00e9r is a kontextus be\u00e1ll\u00edt\u00e1sa.) N\u00e9zz\u00fck meg, milyen n\u00e9vterek vannak: kubectl get namespace List\u00e1zzuk a podokat egy konkr\u00e9t n\u00e9vt\u00e9rben: kubectl get pod -n kube-system","title":"Podok \u00e9s n\u00e9vterek list\u00e1z\u00e1sa"},{"location":"Kubernetes/Kubernetes-alapok/#pod-letrehozasa","text":"A futtat\u00e1s elemi egys\u00e9ge a pod. Ind\u00edtsunk el egy podot. A l\u00e9trehoz\u00e1shoz podot yaml le\u00edr\u00f3ban defini\u00e1ljuk, \u00e9s a kubectl create parancsnak \u00e1tadjuk stdin-r\u0151l olvasva ( Windows Command promptban adjuk ki a parancsot): kubectl create -f - apiVersion: v1 kind: Pod metadata: name: counter spec: containers: - name: count image: ubuntu:16.04 args: [ bash, -c, 'for ((i=0; ;i++));do echo \"$i: $(date)\";sleep 5;done' ] A v\u00e9g\u00e9n nyomjunk egy Ctrl-Z-t a le\u00edr\u00f3 befejez\u00e9s\u00e9nek jelz\u00e9s\u00e9hez. A pod l\u00e9trej\u00f6tt. Ellen\u0151rizz\u00fck: kubectl get pod N\u00e9zz\u00fck meg a pod logjait: kubectl logs counter Ha gondoljuk, tegy\u00fck hozz\u00e1 a -f kapcsol\u00f3t is ( kubectl logs -f counter ) a log k\u00f6vet\u00e9s\u00e9hez. Ctrl-C-vel l\u00e9phet\u00fcnk ki a log folyamatos k\u00f6vet\u00e9s\u00e9b\u0151l. T\u00f6r\u00f6lj\u00fck a podot: kubectl delete pod counter Ellen\u0151rizz\u00fck, hogy a pod t\u00e9nyleg elt\u0171nik egy kis id\u0151 m\u00falva: kubectl get pod A pod t\u00f6rl\u00e9se nem azonnali. A benne fut\u00f3 kont\u00e9nerek le\u00e1ll\u00e1s jelz\u00e9st kapnak, \u00e9s \u0151k maguk termin\u00e1lhatnak. Ha ez nem t\u00f6rt\u00e9nik, meg, akkor kis id\u0151 m\u00falva megsz\u0171nteti \u0151ket a rendszer.","title":"Pod l\u00e9trehoz\u00e1sa"},{"location":"Kubernetes/Kubernetes-alapok/#yaml-leiro-fajl","text":"A yaml le\u00edr\u00f3 beg\u00e9pel\u00e9se a fentiek szerint nem k\u00e9nyelmes. Tipikusan komplex pod \u00e9s egy\u00e9b er\u0151forr\u00e1s defin\u00edci\u00f3kkal dolgozunk. Tegy\u00fck ink\u00e1bb egy f\u00e1jlba. Hozzunk l\u00e9tre egy \u00faj yaml f\u00e1jt createpod.yml n\u00e9ven. Haszn\u00e1lhatjuk p\u00e9ld\u00e1ul Visual Studio Code-ot. \u00c9rdemes olyan sz\u00f6vegszerkeszt\u0151vel dolgozni, amely ismeri a yaml szintaktik\u00e1t. M\u00e1soljuk be a yaml f\u00e1jlba az al\u00e1bbiakat. apiVersion : v1 kind : Pod metadata : name : counter spec : containers : - name : count image : ubuntu:16.04 args : [ bash , -c , 'for ((i=0; ;i++));do echo \"$i: $(date)\";sleep 5;done' ] A konzolunkban navig\u00e1ljunk el abba a k\u00f6nyvt\u00e1rba, ahol a yaml f\u00e1jl van. Hozzuk l\u00e9tre a podot: kubectl create -f createpod.yml A pod l\u00e9trej\u00f6tt. Ellen\u0151rizz\u00fck: kubectl get pod , majd t\u00f6r\u00f6lj\u00fck a podot: kubectl delete pod counter","title":"Yaml le\u00edr\u00f3 f\u00e1jl"},{"location":"Kubernetes/Kubernetes-alapok/#deployment-letrehozasa","text":"A podokat nem szoktuk k\u00f6zvetlen\u00fck l\u00e9trehozni, hanem Deployment -re \u00e9s ReplicaSet -re szoktunk b\u00edzni a kezel\u00e9s\u00fcket \u00e9s l\u00e9trehoz\u00e1sukat. Hozzunk l\u00e9tre egy \u00faj yaml f\u00e1jl createdepl.yml n\u00e9ven az al\u00e1bbi tartalommal. apiVersion : apps/v1 kind : Deployment metadata : name : counter spec : replicas : 1 selector : matchLabels : app : counter template : metadata : labels : app : counter spec : containers : - name : count image : ubuntu:16.04 args : [ bash , -c , 'for ((i=0; ;i++));do echo \"$i: $(date)\";sleep 5;done' ] Hozzuk l\u00e9tre a Deployment-et: kubectl apply -f createdepl.yml Ez\u00fattal nem create , hanem apply parancsot haszn\u00e1lunk. Az apply l\u00e9trehozza, ha nem l\u00e9tezik, \u00e9s m\u00f3dos\u00edtja az er\u0151forr\u00e1st, ha m\u00e1r l\u00e9tezik. List\u00e1zzuk a Deployment-eket, ReplicaSet-eket \u00e9s a podokat: kubectl get deployment kubectl get replicaset kubectl get pod Vegy\u00fck \u00e9szre, hogy a pod neve gener\u00e1lt, a Deployment \u00e9s a ReplicaSet alapj\u00e1n kap automatikusan egyet. V\u00e1ltoztassuk meg a program fut\u00e1s\u00e1t: ne 5, hanem 10 m\u00e1sodpercenk\u00e9nt \u00edrjuk ki az id\u0151t. Ezt a Deployment m\u00f3dos\u00edt\u00e1s\u00e1val fogjuk megtenni. Gondolhatunk arra is, hogy a podot szerkessz\u00fck, de azt nem tehetj\u00fck meg. Egy fut\u00f3 pod nem cser\u00e9lhet\u0151 le. Ehelyett val\u00f3j\u00e1ban egy \u00faj podot fogunk l\u00e9trehozni indirekten. \u00cdrjuk \u00e1t a yaml f\u00e1jlban a bash parancsban a sleep-et 10-re. Ments\u00fck a f\u00e1jlt. Alkalmazzuk a v\u00e1ltoztat\u00e1sokat: kubectl apply -f createdepl.yml N\u00e9zz\u00fck a podok v\u00e1ltoz\u00e1s\u00e1t: kubectl get pod Id\u0151z\u00edt\u00e9s f\u00fcgg\u0151en j\u00f3 es\u00e9llyel l\u00e1tni fogjuk a r\u00e9gi le\u00e1ll\u00f3 podot, \u00e9s az \u00fajat is. \"L\u00e9pj\u00fcnk be\" a fut\u00f3 podba egy \u00faj, interakt\u00edv shellben. M\u00e1soljuk ki a fut\u00f3 pod nev\u00e9t, \u00e9s adjuk ki a k\u00f6vetkez\u0151 parancsot: kubectl exec -it <podn\u00e9v> /bin/bash Ahogy a docker-n\u00e9l m\u00e1r l\u00e1thattuk, egy \u00faj shell indul a pod kont\u00e9ner\u00e9ben, \u00e9s ehhez csatlakozunk. Ebben a shellben, ahogy nat\u00edv docker eset\u00e9ben is, b\u00e1rmit megtehet\u00fcnk. T\u00f6r\u00f6lj\u00fck a Deployment -et: kubectl delete deployment counter Ez a ReplicaSet -et \u00e9s a podokat is t\u00f6r\u00f6lni fogja. A Deployment szolg\u00e1l az alkalmaz\u00e1s verzi\u00f3nak friss\u00edt\u00e9s\u00e9re, kiad\u00e1s\u00e1ra. Podok helyett leggyakrabban Deployment -eket defini\u00e1lunk.","title":"Deployment l\u00e9trehoz\u00e1sa"},{"location":"Kubernetes/Kubernetes-alapok/#kubectl-parancsok","text":"A kubectl leggyakrabban haszn\u00e1lt parancsainak szerkezete: kubectl <ige> <er\u0151forr\u00e1s> <attrib\u00fatumok> . Az ige p\u00e9ld\u00e1ul: get : list\u00e1zza az er\u0151forr\u00e1sokat create : l\u00e9trehoz egy er\u0151forr\u00e1st delete : t\u00f6r\u00f6l egy er\u0151forr\u00e1st describe : lek\u00e9rdezi az er\u0151forr\u00e1s r\u00e9szletes \u00e1llapot\u00e1t edit : let\u00f6lti az er\u0151forr\u00e1s le\u00edr\u00f3j\u00e1t, \u00e9s megnyitja sz\u00f6vegszerkeszt\u0151ben; ment\u00e9s \u00e9s bez\u00e1r\u00e1s ut\u00e1n friss\u00edti a klaszterben az er\u0151forr\u00e1st a m\u00f3dos\u00edt\u00e1sok alapj\u00e1n Az er\u0151forr\u00e1sok a pod , replicaset vagy r\u00f6viden rs , a deployment , stb. A parancsokr\u00f3l -h kapcsol\u00f3val kaphatunk seg\u00edts\u00e9get, pl. kubectl describe -h","title":"Kubectl parancsok"},{"location":"Kubernetes/Kubernetes-alapok/#dashboard-es-proxy-zas","text":"A Web UI / Dashboard egy webalkalmaz\u00e1s, amely maga is Kubernetes alatt fut. Az alkalmaz\u00e1s a klaszter tartalm\u00e1t jelen\u00edti meg egy egyszer\u0171, de k\u00f6nnyen \u00e1ttekinthet\u0151 webes fel\u00fcleten. A dashboard alapvet\u0151en felhaszn\u00e1l\u00f3 authentik\u00e1ci\u00f3 ut\u00e1n \u00e9rhet\u0151 el. Az egyszer\u0171s\u00e9g v\u00e9gett mi ezt most kikapcsoljuk, \u00e9s authentik\u00e1ci\u00f3 n\u00e9lk\u00fcl fogjuk haszn\u00e1lni. Telep\u00edts\u00fck a dashboard-ot: Telep\u00edts\u00fck alapbe\u00e1ll\u00edt\u00e1sokkal: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/alternative.yaml Szerkessz\u00fcnk bele a kont\u00e9ner argumentumaiba, hogy enged\u00e9lyezz\u00fck az anonim bel\u00e9p\u00e9st: kubectl edit deployment kubernetes-dashboard --namespace kubernetes-dashboard A parancs let\u00f6lti \u00e9s megnyitja a deployment le\u00edr\u00f3j\u00e1t. Keress\u00fck meg a kont\u00e9ner ind\u00edt\u00e1si argumentumait, \u00e9s adjunk hozz\u00e1 m\u00e9g k\u00e9t sort (\u00fcgyelj\u00fcnk a megfelel\u0151 beh\u00faz\u00e1sra): - args : - --namespace=kubernetes-dashboard - --enable-insecure-login - --enable-skip-login - --authentication-mode=basic Csak fejleszt\u0151i g\u00e9pen Ez csak helyi m\u0171k\u00f6d\u00e9si m\u00f3dban javasolt! Most sz\u00e1nd\u00e9kosan kiker\u00fclt\u00fck az authentik\u00e1ci\u00f3t! \u00c9rtelemszer\u0171en a dashboard-ot csak egyszer kell egy klaszterbe telep\u00edteni. Adjunk egy p\u00e1r m\u00e1sodpercet a fel\u00e1ll\u00e1sra. N\u00e9zz\u00fck meg, hogy rendben fut-e: kubectl get pods -n kubernetes-dashboard Akkor j\u00f3, ha a kubernetes-dashboard-... nev\u0171 pod running \u00e1llapotban van. A webalkalmaz\u00e1s csak a klaszteren bel\u00fcl \u00e9rhet\u0151 el egyel\u0151re. L\u00e1tni fogjuk k\u00e9s\u0151bb, hogyan tudunk a klaszteren k\u00edv\u00fclre \"publik\u00e1lni\". Egyel\u0151re azonban egy m\u00e1sik, fejleszt\u00e9shez k\u00e9nyelmesen haszn\u00e1lhat\u00f3 megold\u00e1st alkalmazunk. Nyissunk egy \u00faj konzolt, \u00e9s adjuk ki a kubectl proxy parancsot. A fut\u00f3 proxy folyamatos kapcsolatot tart fenn a klaszterrel, \u00e9s a Kubernetes Api szervert haszn\u00e1lja \"bel\u00e9p\u00e9si pontnak\". Az proxy a helyi 8081 portra k\u00fcld\u00f6tt k\u00e9r\u00e9seket tov\u00e1bb\u00edtja a klaszteren bel\u00fclre az URL alapj\u00e1n. Ez a proxy nem csak helyben futtatott klaszterrel haszn\u00e1lhat\u00f3. Ha a felh\u0151ben, t\u0151l\u00fcnk t\u00e1vol fut a klaszter, ugyan\u00edgy tudunk vele kommunik\u00e1lni. Nyissuk meg b\u00f6ng\u00e9sz\u0151ben: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/kubernetes-dashboard:/proxy/ \u00c9rtelmezz\u00fck az URL-t: localhost:8001 : a proxy helyi portja /api/v1 : Kubernetes Api verzi\u00f3, ez mindig \u00edgy n\u00e9z ki /namespaces/kubernetes-dashboard : a kubernetes-dashboard n\u00e9vt\u00e9rben szeretn\u00e9nk el\u00e9rni egy elemet /services/kubernetes-dashboard: a n\u00e9vt\u00e9ren bel\u00fcl a service-ek k\u00f6z\u00fcl a kubernetes-dashboard nev\u0171 szolg\u00e1ltat\u00e1st szeretn\u00e9nk el\u00e9rni /proxy : k\u00e9rj\u00fck az er\u0151forr\u00e1s proxy-z\u00e1s\u00e1t (ez mindig \u00edgy n\u00e9z ki) Ismerkedj\u00fcnk meg a dashboard webes fel\u00fclet\u00e9vel: N\u00e9zz\u00fck meg a bal oldali men\u00fc elemeit. V\u00e1lasszunk n\u00e9vteret. N\u00e9zz\u00fck meg a podokat.","title":"Dashboard \u00e9s proxy-z\u00e1s"},{"location":"Kubernetes/Kubernetes-alapok/#pod-replicaset-deployment-service-kapcsolata","text":"Haszn\u00e1ljuk a dashboard-ot a k\u00f6vetkez\u0151kh\u00f6z. A feladatban v\u00e9gig a kubernetes-dashboard n\u00e9vt\u00e9rben dolgozunk. Keress\u00fck ki a \"kubernetes-dashboard\" nev\u0171 service-t. N\u00e9zz\u00fck meg a service adatait, kifejezetten a \"Pods\" r\u00e9szt. Eml\u00e9kezz\u00fcnk arra, hogy a service teszi el\u00e9rhet\u0151v\u00e9 podok szolg\u00e1ltat\u00e1sait/v\u00e9gpontjait. A service-nek teh\u00e1t tudnia kell, mely podok \"\u00e1llnak m\u00f6g\u00f6tte\". Ezt l\u00e1tjuk a \"Pods\" list\u00e1ban. Sk\u00e1l\u00e1zzuk fel a dashboard-ot, futtassunk k\u00e9t p\u00e9ld\u00e1nyt. Keress\u00fck meg a \"kubernetes-dashboard\" deployment-et. A webes fel\u00fclet tetej\u00e9n a k\u00e9k sz\u00edn\u0171 sorban a jobb oldalon keress\u00fck meg a sk\u00e1l\u00e1z\u00e1s ikont. Nyissuk meg, \u00e9s k\u00e9rj\u00fcnk 3 p\u00e9ld\u00e1nyt. N\u00e9zz\u00fck meg a podok list\u00e1j\u00e1t. L\u00e1ssuk, hogy most m\u00e1r 3 \"kubernetes-dashboard-\" n\u00e9vvel kezd\u0151d\u0151 pod lesz. T\u00e9rj\u00fcnk vissza a \"kubernetes-dashboard\" service-hez. N\u00e9zz\u00fck meg, hogyan van \u00f6sszek\u00f6tve a podokkal. N\u00e9zz\u00fck meg a podokat. Vegy\u00fck \u00e9szre, hogy a service \"\u00e9szrevette\", hogy most m\u00e1r 3 pod felett rendelkezik. N\u00e9zz\u00fck meg a service le\u00edr\u00f3j\u00e1t. A webes fel\u00fcleten a k\u00e9k s\u00e1rban a ceruza ikonra kattintsunk. Keress\u00fck meg a spec le\u00edr\u00f3ban a selector -t. kind : Service apiVersion : v1 metadata : name : kubernetes-dashboard # service elem neve ... spec : ... selector : k8s-app : kubernetes-dashboard # label, amelyet a podokon keres Ez a service azon podok fel\u00e9 osztja sz\u00e9t a k\u00e9r\u00e9seket, amelyek a k8s-app c\u00edmk\u00e9ben kubernetes-dashboard \u00e9rt\u00e9kkel rendelkeznek. N\u00e9zz\u00fck meg a podban ugyanezt. A service alatt b\u00e1rmelyik podra kattintva navig\u00e1ljunk el a podhoz, \u00e9s k\u00e9rj\u00fck le a le\u00edr\u00f3j\u00e1t az el\u0151bbiek szerint. Keress\u00fck meg a pod metaadataiban a fenti labelt. kind : Pod apiVersion : v1 metadata : name : kubernetes-dashboard-57c9d8c7c4-z98z2 # pod saj\u00e1t, gener\u00e1lt neve labels : k8s-app : kubernetes-dashboard # pod c\u00edmk\u00e9je, amivel a service r\u00e1tal\u00e1l A pod oldal\u00e1n maradva keress\u00fck meg a \"Controlled by\" r\u00e9szt. L\u00e1thatjuk, hogy a podot egy ReplicaSet kezeli. Menj\u00fcnk a ReplicaSet-hez. Nyissuk meg a ReplicaSet le\u00edr\u00f3j\u00e1t is. kind : ReplicaSet apiVersion : extensions/v1beta1 metadata : name : kubernetes-dashboard-57c9d8c7c4 labels : k8s-app : kubernetes-dashboard # ReplicaSet c\u00edmk\u00e9je, amivel a felette \u00e1ll\u00f3 Deployment azonos\u00edtja \u0151t spec : replicas : 3 # ennyi replik\u00e1t k\u00e9rt\u00fcnk selector : matchLabels : k8s-app : kubernetes-dashboard # Label, amivel a podjait azonos\u00edtja template : # pod defini\u00e1l\u00e1s\u00e1hoz haszn\u00e1lt template, minden pod ez alapj\u00e1n j\u00f6n l\u00e9tre metadata : labels : k8s-app : kubernetes-dashboard # amikor a ReplicaSet l\u00e9trehoz egy podot, ezt a c\u00edmk\u00e9t teszi r\u00e1 # meg kell egyezzen p\u00e1r sorral feljebb a selector-ban defini\u00e1lttak spec : containers : # le\u00edrja a podban a kont\u00e9nert - name : kubernetes-dashboard image : \"kubernetesui/dashboard:v2.0.3\" args : - \"--namespace=kubernetes-dashboard\" ports : - containerPort : 9090 protocol : TCP Ha m\u00e9g eggyel \"feljebb\" l\u00e9p\u00fcnk, a ReplicaSet-b\u0151l a Deployment-be, hasonl\u00f3 \u00f6sszekapcsol\u00e1st tal\u00e1ln\u00e1nk.","title":"Pod - ReplicaSet - Deployment - Service kapcsolata"},{"location":"Kubernetes/Kubernetes-alapok/#pod-ujrainditasa","text":"Maradva a kubernetes-dashboard n\u00e9vt\u00e9rben list\u00e1zzuk a podokat. V\u00e1lasszunk ki egy dashboard podot, \u00e9s t\u00f6r\u00f6lj\u00fck. A pod sor v\u00e9g\u00e9n a ... ikon alatt v\u00e1lasszuk a Delete -et. V\u00e1rjunk egy p\u00e1r m\u00e1sodpercet, \u00e9s friss\u00edts\u00fcnk r\u00e1 az oldalra. Egy \u00faj pod sz\u00fcletett. Az Age oszlop alapj\u00e1n l\u00e1thatjuk. (Lehet, hogy a t\u00f6r\u00f6ltet is l\u00e1tjuk m\u00e9g, le\u00e1ll\u00e1s alatt.) Ez a ReplicaSet feladata: garant\u00e1lni, hogy legyen 3 p\u00e9ld\u00e1ny. \u00dajraind\u00edt\u00e1s A pod t\u00f6rl\u00e9se megfelel egy komponens \u00fajraind\u00edt\u00e1s\u00e1nak. Ha t\u00f6bb p\u00e9ld\u00e1nyunk van, akkor mindegyiket k\u00e9zzel tudjuk \u00edgy t\u00f6r\u00f6lni. Ez persze csak akkor m\u0171k\u00f6dik, ha a pod felett van egy controller, ami sz\u00fcks\u00e9g szerint l\u00e9trehozza az \u00faj podokat.","title":"Pod \"\u00fajraind\u00edt\u00e1sa\""},{"location":"Kubernetes/Kubernetes-alkalmazas-telepites/","text":"C\u00e9l \u00b6 A labor c\u00e9lja egy alkalmaz\u00e1s telep\u00edt\u00e9se Kubernetes klaszterbe, valamint a friss\u00edt\u00e9s m\u00f3dj\u00e1nak megismer\u00e9se. A telep\u00edt\u00e9shez \u00e9s friss\u00edt\u00e9shez r\u00e9szben Helm chartot, r\u00e9szben magunk \u00e1ltal elk\u00e9sz\u00edtett yaml er\u0151forr\u00e1s le\u00edr\u00f3kat haszn\u00e1lunk. El\u0151k\u00f6vetelm\u00e9nyek \u00b6 Kubernetes B\u00e1rmely felh\u0151 platform \u00e1ltal biztos\u00edtott klaszter Linux platformon: minikube Windows platformon: Docker Desktop kubectl A bin\u00e1risa legyen el\u00e9rhet\u0151 PATH-on. helm v3 A bin\u00e1risa legyen el\u00e9rhet\u0151 PATH-on. Kubernetes dashboard Telep\u00edt\u00e9se \u00e9s haszn\u00e1lata kor\u00e1bbi anyag szerint. Telep\u00edtend\u0151 alkalmaz\u00e1s https://github.com/bmeviauav42/todoapp git clone https://github.com/bmeviauav42/todoapp.git Feladatok \u00b6 C\u00e9lok, tervez\u00e9s \u00b6 A c\u00e9lunk a todo-kat kezel\u0151 kont\u00e9neralap\u00fa mikroszolg\u00e1ltat\u00e1sokra \u00e9p\u00fcl\u0151 webalkalmaz\u00e1s telep\u00edt\u00e9se Kubernetes-be. A rendszer\u00fcnk alapvet\u0151en h\u00e1rom f\u00e9le komponensb\u0151l \u00e9p\u00fcl fel: az \u00e1ltalunk megval\u00f3s\u00edtott mikroszolg\u00e1ltat\u00e1sok (backend \u00e9s frontend), az adatb\u00e1zis rendszerek (MongoDB, Elasticsearch \u00e9s Redis), valamint az api gateway. C\u00e9lunk nem csak az egyszeri telep\u00edt\u00e9s, hanem az alkalmaz\u00e1s naprak\u00e9szen tart\u00e1s\u00e1hoz a folyamatos friss\u00edt\u00e9s lehet\u0151s\u00e9g\u00e9nek megteremt\u00e9se. A fenti komponensek azonban nem ugyanolyan friss\u00edt\u00e9si ciklussal rendelkeznek: a saj\u00e1t komponenseink gyakran fognak v\u00e1ltozni, m\u00edg az adatb\u00e1zisok \u00e9s az api gateway ritk\u00e1n friss\u00fcl. A telep\u00edt\u00e9st ennek megfelel\u0151en kett\u00e9 v\u00e1gjuk: Az api gateway-t \u00e9s az adatb\u00e1zisokat egyszer telep\u00edtj\u00fck. Az alkalmaz\u00e1sunk saj\u00e1t komponenseihez yaml alap\u00fa le\u00edr\u00f3kat k\u00e9sz\u00edt\u00fcnk, amit kubectl apply seg\u00edts\u00e9g\u00e9vel fogunk telep\u00edteni. Helm \u00b6 Ellen\u0151rizz\u00fck, hogy a helm CLI el\u00e9rhet\u0151-e: helm version Helm 3 A feladat sor\u00e1n a Helm 3-as verzi\u00f3j\u00e1t fogjuk haszn\u00e1lni. A kor\u00e1bbi verzi\u00f3ja koncepci\u00f3ban azonos, de m\u0171k\u00f6d\u00e9s\u00e9ben elt\u00e9r\u0151. Ingress Controller (api gateway) telep\u00edt\u00e9se Helm charttal \u00b6 A Traefik-et Helm charttal fogjuk telep\u00edteni, mert a Traefik helyes m\u0171k\u00f6d\u00e9s\u00e9hez a Traefik kont\u00e9ner (Deployment) mellett egy\u00e9b elemekre is sz\u00fcks\u00e9g lesz (klaszteren bel\u00fcli hozz\u00e1f\u00e9r\u00e9s szab\u00e1lyz\u00e1s miatt). Chart-ok ellen\u0151rz\u00e9se A Helm chartok nagy r\u00e9sze harmadik f\u00e9lt\u0151l sz\u00e1rmazik, \u00edgy a klaszter\u00fcnbe val\u00f3 telep\u00edt\u00e9s el\u0151tt a tartalmukat \u00e9rdemes alaposan megn\u00e9zni. A Helm is repository-kkal dolgozik, ahonnan a chart-okat let\u00f6lti. Ezeket regisztr\u00e1lni kell. Regisztr\u00e1ljuk a Traefik hivatalos chart-j\u00e1t tartalmaz\u00f3 repository-t, majd friss\u00edts\u00fck az el\u00e9rhet\u0151 char-okat: helm repo add traefik https://containous.github.io/traefik-helm-chart helm repo update Telep\u00edts\u00fck: helm install traefik traefik/traefik --set ports.web.nodePort=32080 --set service.type=NodePort A legels\u0151 traefik a Helm release nev\u00e9t adja meg. Ezzel tudunk r\u00e1 hivatkozni a j\u00f6v\u0151ben. A traefik/traefik azonos\u00edtha a telep\u00edtend\u0151 chartot (repository/chartn\u00e9v). A --set kapcsol\u00f3val a chart v\u00e1ltoz\u00f3it \u00e1ll\u00edtjuk be. Publikus el\u00e9r\u00e9shez A Traefik jelen konfigur\u00e1ci\u00f3ban NodePort service t\u00edpussal van konfigur\u00e1lva, ami azt jelenti, lok\u00e1lisan, helyben a megadott porton lesz csak el\u00e9rhet\u0151. Ha publikusan el\u00e9rhet\u0151 klaszterben dolgozunk, akkor tipikusan LoadBalancer service t\u00edpust fogunk k\u00e9rni, hogy publikus IP c\u00edmet is kapjon a Traefik. Ellen\u0151rizz\u00fck, hogy fut-e: kubectl get pod L\u00e1tunk kell egy traefik kezdet\u0171 podot. A Traefik dashboard-ja nem el\u00e9rhet\u0151 \"k\u00edv\u00fclr\u0151l\". A dashboard seg\u00edt minket l\u00e1tni a Traefik konfigur\u00e1ci\u00f3j\u00e1t \u00e9s m\u0171k\u00f6d\u00e9s\u00e9t. Mivel ez a klaszter bels\u0151 \u00e1llapot\u00e1t publik\u00e1lja, production \u00fczemben valamilyen m\u00f3don authentik\u00e1lnunk kellene. Ezt most megker\u00fclve kubect seg\u00edts\u00e9g\u00e9vel egy helyi portra tov\u00e1bb\u00edtjuk a Traefik dashboard-ot. A k\u00f6vetkez\u0151 parancsot egy \u00faj konzolban adjuk ki, \u00e9s ut\u00e1na hagyjuk nyitva a konzolt: kubectl port-forward $( kubectl get pods --selector \"app.kubernetes.io/name=traefik\" --output = name ) 9000 :9000 Ha friss\u00edteni szeretn\u00e9nk k\u00e9s\u0151bb a Traefik-et, akkor azt a helm upgrade traefik traefik/traefik ... paranccsal tudjuk megtenni. Adatb\u00e1zisok telep\u00edt\u00e9se \u00b6 Az adatb\u00e1zisainkat saj\u00e1t magunk \u00e1ltal meg\u00edrt yaml le\u00edr\u00f3val telep\u00edtj\u00fck. Ez a le\u00edr\u00f3 f\u00e1jl m\u00e1r rendelkez\u00e9s\u00fcnkre \u00e1ll a todoapp alkalmaz\u00e1s repositoryj\u00e1ban. Navig\u00e1ljunk el a forr\u00e1sk\u00f3d repository-j\u00e1ban a kubernetes/db k\u00f6nyvt\u00e1rba. N\u00e9zz\u00fck meg a yaml le\u00edr\u00f3kat. Telep\u00edts\u00fck az adatb\u00e1zisokat: kubectl apply -f db Kapcsol\u00f3djunk a Kubernetes Dashboard-hoz a kor\u00e1bban ismertetett m\u00f3don (ill. telep\u00edts\u00fck a klaszterbe, ha nem lenne). Ellen\u0151rizz\u00fck, hogy az adatb\u00e1zis podok elindulnak-e. Minden a default n\u00e9vt\u00e9rbe kellett telep\u00fclj\u00f6n. N\u00e9zz\u00fck meg a Persistent Volume \u00e9s Persistent Volumen Claim -eket. Alkalmaz\u00e1sunk telep\u00edt\u00e9se \u00b6 Az alkalmaz\u00e1sunk telep\u00edt\u00e9s\u00e9hez szint\u00e9n yaml le\u00edr\u00f3kat tal\u00e1lunk a kubernetes/app k\u00f6nyvt\u00e1rban. N\u00e9zz\u00fck meg a le\u00edr\u00f3kat. Az el\u0151bb l\u00e1tott Deployment \u00e9s Service mellet Ingress-t is l\u00e1tni fogunk. Telep\u00edts\u00fck az adatb\u00e1zisokat: kubectl apply -f app Kubernetes dashboard seg\u00edts\u00e9g\u00e9vel n\u00e9zz\u00fck meg, hogy l\u00e9trej\u00f6ttek a Deployment-ek podok, stb. Viszont vegy\u00fck \u00e9szre, hogy piros lesz m\u00e9g p\u00e1r dolog. A hiba oka, hogy a hivatkozott image-eket nem tal\u00e1lja a rendszer. Navig\u00e1ljunk el az src/Docker k\u00f6nyvt\u00e1rba, \u00e9s buildelj\u00fck le az alkalmaz\u00e1st docker-compose seg\u00edts\u00e9g\u00e9vel \u00fagy, hogy a megfelel\u0151 taggel ell\u00e1tjuk az image-eket. Ehhez a compose f\u00e1jlunk az IMAGE_TAG k\u00f6rnyezeti v\u00e1ltoz\u00f3 be\u00e1ll\u00edt\u00e1s\u00e1t v\u00e1rja. Powershell-ben $env:IMAGE_TAG = \"v1\" docker-compose build Windows Command Prompt-ban setx IMAGE_TAG \"v1\" docker-compose build A build folyamat v\u00e9g\u00e9n el\u0151\u00e1llnak helyben az image-ek, pl. todoapp/web:v1 taggel. Ha t\u00e1voli registry-be szeretn\u00e9nk felt\u00f6lteni \u0151ket, akkor taggelhetn\u00e9nk \u0151ket a registry-nek megfelel\u0151en. A helyi fejleszt\u00e9shez nincs sz\u00fcks\u00e9g\u00fcnk ehhez, mert a helyben elind\u00edtott Kubernetes \"l\u00e1tja\" a Docker helyi image-eit. Menj\u00fcnk vissza a Kubernetes dashboard-ra. Egy kicsit v\u00e1rjunk, \u00e9s azt kell l\u00e1ssuk, hogy az eddig piros elemek kiz\u00f6ld\u00fclnek. A Kubernetes folyamatosan t\u00f6rekszik az elv\u00e1rt \u00e1llapot el\u00e9r\u00e9s\u00e9re, ez\u00e9rt a nem el\u00e9rhet\u0151 image-einket \u00fajra \u00e9s \u00fajra megpr\u00f3b\u00e1lta el\u00e9rni, m\u00edg nem siker\u00fclt. Pr\u00f3b\u00e1ljuk ki az alkalmaz\u00e1st a http://localhost:30080 c\u00edmen Alkalmaz\u00e1s friss\u00edt\u00e9se \u00b6 Tegy\u00fck fel, hogy az alkalmaz\u00e1sunkb\u00f3l \u00fajabb verzi\u00f3 k\u00e9sz\u00fcl, \u00e9s szeretn\u00e9nk friss\u00edteni. A fentebb haszn\u00e1lt yaml le\u00edr\u00f3kat az\u00e9rt (is) a verzi\u00f3kezel\u0151ben t\u00e1roljuk, mert \u00edgy a telep\u00edt\u00e9si \"\u00fatmutat\u00f3k\" is verzi\u00f3zottak. Teh\u00e1t nincs m\u00e1s dolgunk, mit a kont\u00e9ner image-ek elk\u00e9sz\u00edt\u00e9se ut\u00e1n a Deployment-ekben a megfelel\u0151 tag-ek lecser\u00e9l\u00e9se, \u00e9s a kubectl apply paranccsal a telep\u00edt\u00e9s friss\u00edt\u00e9se. Helm chart k\u00e9sz\u00edt\u00e9se \u00b6 Az alkalmaz\u00e1s fent ismertetett friss\u00edt\u00e9s\u00e9hez a yaml f\u00e1jlokba minden alkalommal bele kell \u00edrnunk. J\u00f3 lenne, ha az image tag-et mint egy v\u00e1ltoz\u00f3 tudn\u00e1nk atelep\u00edt\u00e9s sor\u00e1n \u00e1tadni. Erre szolg\u00e1l a Helm. K\u00e9sz\u00edts\u00fcnk egy chart -ot a szolg\u00e1ltat\u00e1sainknak. A chart a telep\u00edt\u00e9s le\u00edr\u00f3 neve, ami gyakorlatilag yaml f\u00e1jlok gy\u0171jtem\u00e9nye egy speci\u00e1lis szintaktik\u00e1val kieg\u00e9sz\u00edtva. Konzolban navig\u00e1ljunk egy el tetsz\u0151leges munkak\u00f6nyvt\u00e1rba. K\u00e9sz\u00edts\u00fcnk egy \u00faj, \u00fcres chart-ot: helm create todoapp . Ez l\u00e9trehoz egy todoapp nev\u0171 chartot egy azonos nev\u0171 k\u00f6nyvt\u00e1rban. N\u00e9zz\u00fck meg a chart f\u00e1jljait. Chart.yaml a metaadatokat \u00edrja le. values.yaml \u00edrja le a v\u00e1ltoz\u00f3ink alap\u00e9rtelmezett \u00e9rt\u00e9keit. .helmignore azon f\u00e1jlokat list\u00e1zza, amelyeket a chart \u00e9rtelmez\u00e9sekor nem kell figyelembe venni. templates k\u00f6nyvt\u00e1rban vannak a template f\u00e1jlok, amik a gener\u00e1l\u00e1s alapj\u00e1ul szolg\u00e1lnak. A Helm egy olyan template nyelvet haszn\u00e1l, amelyben v\u00e1ltoz\u00f3 behelyettes\u00edt\u00e9sek, ciklusok, egyszer\u0171 sz\u00f6vegm\u0171veletek t\u00e1mogatottak. Mi most csak a v\u00e1ltoz\u00f3 behelyettes\u00edt\u00e9st fogjuk haszn\u00e1lni. T\u00f6r\u00f6lj\u00fck ki a templates k\u00f6nyvt\u00e1rb\u00f3l az \u00f6sszes f\u00e1jlt a _helpers.tpl kiv\u00e9tel\u00e9vel. M\u00e1soljuk helyette be ide a kor\u00e1bban a telep\u00edt\u00e9shez haszn\u00e1lt yaml f\u00e1jljainkat (3 darab). Szerkessz\u00fck meg a todos.yaml f\u00e1jl tartalm\u00e1t. Leegyszer\u0171s\u00edtve az al\u00e1bbiakra lesz sz\u00fcks\u00e9g: Ha Visual Studio Code -ot haszn\u00e1lunk, akkor telep\u00edts\u00fck a ms-kubernetes-tools.vscode-kubernetes-tools extension-t. \u00cdgy kapunk n\u00e9mi seg\u00edts\u00e9get a szintaktik\u00e1val. Mindenhol, ahol labels vagy matchLabels szerepel, m\u00e9g egy sort fel kell venn\u00fcnk: app.kubernetes.io/instance: {{ .Release.Name }} Ez egy implicit v\u00e1ltoz\u00f3t helyettes\u00edt be: a release nev\u00e9t. Ezzel azonos\u00edtja a Helm a telep\u00edt\u00e9s \u00e9s friss\u00edt\u00e9s sor\u00e1n, hogy mely elemeket kell friss\u00edtenie, melyek tartoznak a fennhat\u00f3s\u00e1ga al\u00e1. A pod-ban az image be\u00e1ll\u00edt\u00e1s\u00e1hoz haszn\u00e1ljunk v\u00e1ltoz\u00f3t: image: todoapp/todos: {{ .Values.todos.tag }} Defini\u00e1ljuk az el\u0151bbi v\u00e1ltoz\u00f3 alap\u00e9rtelmezett \u00e9rt\u00e9k\u00e9t. A values.yaml f\u00e1jlban (egy k\u00f6nyvt\u00e1rral feljebb) t\u00f6r\u00f6lj\u00fcnk ki mindent, \u00e9s vegy\u00fck fel ezt a kulcsot: todos : tag : v1 A m\u00e1sik k\u00e9t komponens yaml le\u00edr\u00f3ival is hasonl\u00f3an kell elj\u00e1rnunk. A tov\u00e1bbiakhoz el kell t\u00e1vol\u00edtanunk az el\u0151bb telep\u00edtett alkalmaz\u00e1sunkat, mert \u00f6sszeakadna a Helm-mel. Ezt a parancsot a telep\u00edt\u00e9shez kor\u00e1bban haszn\u00e1lt app k\u00f6nyvt\u00e1rban adjuk ki: kubectl delete -f app N\u00e9zz\u00fck meg a template-eket ki\u00e9rt\u00e9kelve. A chartunk k\u00f6nyvt\u00e1r\u00e1b\u00f3l l\u00e9pj\u00fcnk eggyel feljebb, hogy a todoapp chart k\u00f6nyvt\u00e1r az aktu\u00e1lis k\u00f6nyvt\u00e1rban legyen. Futtassuk le csak a template gener\u00e1l\u00e1st a telep\u00edt\u00e9s n\u00e9lk\u00fcl: helm install todoapp --debug --dry-run todoapp Konzolra megkapjuk a ki\u00e9rt\u00e9kelt yaml-\u00f6ket. Telep\u00edts\u00fck \u00fajra az alkalmaz\u00e1st a chart seg\u00edts\u00e9g\u00e9vel: helm upgrade todoapp --install todoapp A release-nek todoapp nevet v\u00e1lasztottunk. Ez a Helm release azonos\u00edt\u00f3ja. Az upgrade parancs \u00e9s az --install kapcsol\u00f3 telep\u00edt, ha nem l\u00e9tezik, ill. friss\u00edt, ha m\u00e1r l\u00e9tezik ilyen telep\u00edt\u00e9s. N\u00e9zz\u00fck meg, hogy a Helm szerint l\u00e9tezik-e a release: helm list Pr\u00f3b\u00e1ljuk ki az alkalmaz\u00e1st a http://localhost:30080 c\u00edmen. Ezen chart seg\u00edts\u00e9g\u00e9vel a Docker image tag-et telep\u00edt\u00e9si param\u00e9terben adhatjuk \u00e1t, pl. ha a \"v2\" az \u00faj tag, akkor egy paranccsal tudjuk friss\u00edteni: helm upgrade todoapp --install todoapp --set todos.tag=v2","title":"Alkalmaz\u00e1s telep\u00edt\u00e9se Kubernetes klaszterbe"},{"location":"Kubernetes/Kubernetes-alkalmazas-telepites/#cel","text":"A labor c\u00e9lja egy alkalmaz\u00e1s telep\u00edt\u00e9se Kubernetes klaszterbe, valamint a friss\u00edt\u00e9s m\u00f3dj\u00e1nak megismer\u00e9se. A telep\u00edt\u00e9shez \u00e9s friss\u00edt\u00e9shez r\u00e9szben Helm chartot, r\u00e9szben magunk \u00e1ltal elk\u00e9sz\u00edtett yaml er\u0151forr\u00e1s le\u00edr\u00f3kat haszn\u00e1lunk.","title":"C\u00e9l"},{"location":"Kubernetes/Kubernetes-alkalmazas-telepites/#elokovetelmenyek","text":"Kubernetes B\u00e1rmely felh\u0151 platform \u00e1ltal biztos\u00edtott klaszter Linux platformon: minikube Windows platformon: Docker Desktop kubectl A bin\u00e1risa legyen el\u00e9rhet\u0151 PATH-on. helm v3 A bin\u00e1risa legyen el\u00e9rhet\u0151 PATH-on. Kubernetes dashboard Telep\u00edt\u00e9se \u00e9s haszn\u00e1lata kor\u00e1bbi anyag szerint. Telep\u00edtend\u0151 alkalmaz\u00e1s https://github.com/bmeviauav42/todoapp git clone https://github.com/bmeviauav42/todoapp.git","title":"El\u0151k\u00f6vetelm\u00e9nyek"},{"location":"Kubernetes/Kubernetes-alkalmazas-telepites/#feladatok","text":"","title":"Feladatok"},{"location":"Kubernetes/Kubernetes-alkalmazas-telepites/#celok-tervezes","text":"A c\u00e9lunk a todo-kat kezel\u0151 kont\u00e9neralap\u00fa mikroszolg\u00e1ltat\u00e1sokra \u00e9p\u00fcl\u0151 webalkalmaz\u00e1s telep\u00edt\u00e9se Kubernetes-be. A rendszer\u00fcnk alapvet\u0151en h\u00e1rom f\u00e9le komponensb\u0151l \u00e9p\u00fcl fel: az \u00e1ltalunk megval\u00f3s\u00edtott mikroszolg\u00e1ltat\u00e1sok (backend \u00e9s frontend), az adatb\u00e1zis rendszerek (MongoDB, Elasticsearch \u00e9s Redis), valamint az api gateway. C\u00e9lunk nem csak az egyszeri telep\u00edt\u00e9s, hanem az alkalmaz\u00e1s naprak\u00e9szen tart\u00e1s\u00e1hoz a folyamatos friss\u00edt\u00e9s lehet\u0151s\u00e9g\u00e9nek megteremt\u00e9se. A fenti komponensek azonban nem ugyanolyan friss\u00edt\u00e9si ciklussal rendelkeznek: a saj\u00e1t komponenseink gyakran fognak v\u00e1ltozni, m\u00edg az adatb\u00e1zisok \u00e9s az api gateway ritk\u00e1n friss\u00fcl. A telep\u00edt\u00e9st ennek megfelel\u0151en kett\u00e9 v\u00e1gjuk: Az api gateway-t \u00e9s az adatb\u00e1zisokat egyszer telep\u00edtj\u00fck. Az alkalmaz\u00e1sunk saj\u00e1t komponenseihez yaml alap\u00fa le\u00edr\u00f3kat k\u00e9sz\u00edt\u00fcnk, amit kubectl apply seg\u00edts\u00e9g\u00e9vel fogunk telep\u00edteni.","title":"C\u00e9lok, tervez\u00e9s"},{"location":"Kubernetes/Kubernetes-alkalmazas-telepites/#helm","text":"Ellen\u0151rizz\u00fck, hogy a helm CLI el\u00e9rhet\u0151-e: helm version Helm 3 A feladat sor\u00e1n a Helm 3-as verzi\u00f3j\u00e1t fogjuk haszn\u00e1lni. A kor\u00e1bbi verzi\u00f3ja koncepci\u00f3ban azonos, de m\u0171k\u00f6d\u00e9s\u00e9ben elt\u00e9r\u0151.","title":"Helm"},{"location":"Kubernetes/Kubernetes-alkalmazas-telepites/#ingress-controller-api-gateway-telepitese-helm-charttal","text":"A Traefik-et Helm charttal fogjuk telep\u00edteni, mert a Traefik helyes m\u0171k\u00f6d\u00e9s\u00e9hez a Traefik kont\u00e9ner (Deployment) mellett egy\u00e9b elemekre is sz\u00fcks\u00e9g lesz (klaszteren bel\u00fcli hozz\u00e1f\u00e9r\u00e9s szab\u00e1lyz\u00e1s miatt). Chart-ok ellen\u0151rz\u00e9se A Helm chartok nagy r\u00e9sze harmadik f\u00e9lt\u0151l sz\u00e1rmazik, \u00edgy a klaszter\u00fcnbe val\u00f3 telep\u00edt\u00e9s el\u0151tt a tartalmukat \u00e9rdemes alaposan megn\u00e9zni. A Helm is repository-kkal dolgozik, ahonnan a chart-okat let\u00f6lti. Ezeket regisztr\u00e1lni kell. Regisztr\u00e1ljuk a Traefik hivatalos chart-j\u00e1t tartalmaz\u00f3 repository-t, majd friss\u00edts\u00fck az el\u00e9rhet\u0151 char-okat: helm repo add traefik https://containous.github.io/traefik-helm-chart helm repo update Telep\u00edts\u00fck: helm install traefik traefik/traefik --set ports.web.nodePort=32080 --set service.type=NodePort A legels\u0151 traefik a Helm release nev\u00e9t adja meg. Ezzel tudunk r\u00e1 hivatkozni a j\u00f6v\u0151ben. A traefik/traefik azonos\u00edtha a telep\u00edtend\u0151 chartot (repository/chartn\u00e9v). A --set kapcsol\u00f3val a chart v\u00e1ltoz\u00f3it \u00e1ll\u00edtjuk be. Publikus el\u00e9r\u00e9shez A Traefik jelen konfigur\u00e1ci\u00f3ban NodePort service t\u00edpussal van konfigur\u00e1lva, ami azt jelenti, lok\u00e1lisan, helyben a megadott porton lesz csak el\u00e9rhet\u0151. Ha publikusan el\u00e9rhet\u0151 klaszterben dolgozunk, akkor tipikusan LoadBalancer service t\u00edpust fogunk k\u00e9rni, hogy publikus IP c\u00edmet is kapjon a Traefik. Ellen\u0151rizz\u00fck, hogy fut-e: kubectl get pod L\u00e1tunk kell egy traefik kezdet\u0171 podot. A Traefik dashboard-ja nem el\u00e9rhet\u0151 \"k\u00edv\u00fclr\u0151l\". A dashboard seg\u00edt minket l\u00e1tni a Traefik konfigur\u00e1ci\u00f3j\u00e1t \u00e9s m\u0171k\u00f6d\u00e9s\u00e9t. Mivel ez a klaszter bels\u0151 \u00e1llapot\u00e1t publik\u00e1lja, production \u00fczemben valamilyen m\u00f3don authentik\u00e1lnunk kellene. Ezt most megker\u00fclve kubect seg\u00edts\u00e9g\u00e9vel egy helyi portra tov\u00e1bb\u00edtjuk a Traefik dashboard-ot. A k\u00f6vetkez\u0151 parancsot egy \u00faj konzolban adjuk ki, \u00e9s ut\u00e1na hagyjuk nyitva a konzolt: kubectl port-forward $( kubectl get pods --selector \"app.kubernetes.io/name=traefik\" --output = name ) 9000 :9000 Ha friss\u00edteni szeretn\u00e9nk k\u00e9s\u0151bb a Traefik-et, akkor azt a helm upgrade traefik traefik/traefik ... paranccsal tudjuk megtenni.","title":"Ingress Controller (api gateway) telep\u00edt\u00e9se Helm charttal"},{"location":"Kubernetes/Kubernetes-alkalmazas-telepites/#adatbazisok-telepitese","text":"Az adatb\u00e1zisainkat saj\u00e1t magunk \u00e1ltal meg\u00edrt yaml le\u00edr\u00f3val telep\u00edtj\u00fck. Ez a le\u00edr\u00f3 f\u00e1jl m\u00e1r rendelkez\u00e9s\u00fcnkre \u00e1ll a todoapp alkalmaz\u00e1s repositoryj\u00e1ban. Navig\u00e1ljunk el a forr\u00e1sk\u00f3d repository-j\u00e1ban a kubernetes/db k\u00f6nyvt\u00e1rba. N\u00e9zz\u00fck meg a yaml le\u00edr\u00f3kat. Telep\u00edts\u00fck az adatb\u00e1zisokat: kubectl apply -f db Kapcsol\u00f3djunk a Kubernetes Dashboard-hoz a kor\u00e1bban ismertetett m\u00f3don (ill. telep\u00edts\u00fck a klaszterbe, ha nem lenne). Ellen\u0151rizz\u00fck, hogy az adatb\u00e1zis podok elindulnak-e. Minden a default n\u00e9vt\u00e9rbe kellett telep\u00fclj\u00f6n. N\u00e9zz\u00fck meg a Persistent Volume \u00e9s Persistent Volumen Claim -eket.","title":"Adatb\u00e1zisok telep\u00edt\u00e9se"},{"location":"Kubernetes/Kubernetes-alkalmazas-telepites/#alkalmazasunk-telepitese","text":"Az alkalmaz\u00e1sunk telep\u00edt\u00e9s\u00e9hez szint\u00e9n yaml le\u00edr\u00f3kat tal\u00e1lunk a kubernetes/app k\u00f6nyvt\u00e1rban. N\u00e9zz\u00fck meg a le\u00edr\u00f3kat. Az el\u0151bb l\u00e1tott Deployment \u00e9s Service mellet Ingress-t is l\u00e1tni fogunk. Telep\u00edts\u00fck az adatb\u00e1zisokat: kubectl apply -f app Kubernetes dashboard seg\u00edts\u00e9g\u00e9vel n\u00e9zz\u00fck meg, hogy l\u00e9trej\u00f6ttek a Deployment-ek podok, stb. Viszont vegy\u00fck \u00e9szre, hogy piros lesz m\u00e9g p\u00e1r dolog. A hiba oka, hogy a hivatkozott image-eket nem tal\u00e1lja a rendszer. Navig\u00e1ljunk el az src/Docker k\u00f6nyvt\u00e1rba, \u00e9s buildelj\u00fck le az alkalmaz\u00e1st docker-compose seg\u00edts\u00e9g\u00e9vel \u00fagy, hogy a megfelel\u0151 taggel ell\u00e1tjuk az image-eket. Ehhez a compose f\u00e1jlunk az IMAGE_TAG k\u00f6rnyezeti v\u00e1ltoz\u00f3 be\u00e1ll\u00edt\u00e1s\u00e1t v\u00e1rja. Powershell-ben $env:IMAGE_TAG = \"v1\" docker-compose build Windows Command Prompt-ban setx IMAGE_TAG \"v1\" docker-compose build A build folyamat v\u00e9g\u00e9n el\u0151\u00e1llnak helyben az image-ek, pl. todoapp/web:v1 taggel. Ha t\u00e1voli registry-be szeretn\u00e9nk felt\u00f6lteni \u0151ket, akkor taggelhetn\u00e9nk \u0151ket a registry-nek megfelel\u0151en. A helyi fejleszt\u00e9shez nincs sz\u00fcks\u00e9g\u00fcnk ehhez, mert a helyben elind\u00edtott Kubernetes \"l\u00e1tja\" a Docker helyi image-eit. Menj\u00fcnk vissza a Kubernetes dashboard-ra. Egy kicsit v\u00e1rjunk, \u00e9s azt kell l\u00e1ssuk, hogy az eddig piros elemek kiz\u00f6ld\u00fclnek. A Kubernetes folyamatosan t\u00f6rekszik az elv\u00e1rt \u00e1llapot el\u00e9r\u00e9s\u00e9re, ez\u00e9rt a nem el\u00e9rhet\u0151 image-einket \u00fajra \u00e9s \u00fajra megpr\u00f3b\u00e1lta el\u00e9rni, m\u00edg nem siker\u00fclt. Pr\u00f3b\u00e1ljuk ki az alkalmaz\u00e1st a http://localhost:30080 c\u00edmen","title":"Alkalmaz\u00e1sunk telep\u00edt\u00e9se"},{"location":"Kubernetes/Kubernetes-alkalmazas-telepites/#alkalmazas-frissitese","text":"Tegy\u00fck fel, hogy az alkalmaz\u00e1sunkb\u00f3l \u00fajabb verzi\u00f3 k\u00e9sz\u00fcl, \u00e9s szeretn\u00e9nk friss\u00edteni. A fentebb haszn\u00e1lt yaml le\u00edr\u00f3kat az\u00e9rt (is) a verzi\u00f3kezel\u0151ben t\u00e1roljuk, mert \u00edgy a telep\u00edt\u00e9si \"\u00fatmutat\u00f3k\" is verzi\u00f3zottak. Teh\u00e1t nincs m\u00e1s dolgunk, mit a kont\u00e9ner image-ek elk\u00e9sz\u00edt\u00e9se ut\u00e1n a Deployment-ekben a megfelel\u0151 tag-ek lecser\u00e9l\u00e9se, \u00e9s a kubectl apply paranccsal a telep\u00edt\u00e9s friss\u00edt\u00e9se.","title":"Alkalmaz\u00e1s friss\u00edt\u00e9se"},{"location":"Kubernetes/Kubernetes-alkalmazas-telepites/#helm-chart-keszitese","text":"Az alkalmaz\u00e1s fent ismertetett friss\u00edt\u00e9s\u00e9hez a yaml f\u00e1jlokba minden alkalommal bele kell \u00edrnunk. J\u00f3 lenne, ha az image tag-et mint egy v\u00e1ltoz\u00f3 tudn\u00e1nk atelep\u00edt\u00e9s sor\u00e1n \u00e1tadni. Erre szolg\u00e1l a Helm. K\u00e9sz\u00edts\u00fcnk egy chart -ot a szolg\u00e1ltat\u00e1sainknak. A chart a telep\u00edt\u00e9s le\u00edr\u00f3 neve, ami gyakorlatilag yaml f\u00e1jlok gy\u0171jtem\u00e9nye egy speci\u00e1lis szintaktik\u00e1val kieg\u00e9sz\u00edtva. Konzolban navig\u00e1ljunk egy el tetsz\u0151leges munkak\u00f6nyvt\u00e1rba. K\u00e9sz\u00edts\u00fcnk egy \u00faj, \u00fcres chart-ot: helm create todoapp . Ez l\u00e9trehoz egy todoapp nev\u0171 chartot egy azonos nev\u0171 k\u00f6nyvt\u00e1rban. N\u00e9zz\u00fck meg a chart f\u00e1jljait. Chart.yaml a metaadatokat \u00edrja le. values.yaml \u00edrja le a v\u00e1ltoz\u00f3ink alap\u00e9rtelmezett \u00e9rt\u00e9keit. .helmignore azon f\u00e1jlokat list\u00e1zza, amelyeket a chart \u00e9rtelmez\u00e9sekor nem kell figyelembe venni. templates k\u00f6nyvt\u00e1rban vannak a template f\u00e1jlok, amik a gener\u00e1l\u00e1s alapj\u00e1ul szolg\u00e1lnak. A Helm egy olyan template nyelvet haszn\u00e1l, amelyben v\u00e1ltoz\u00f3 behelyettes\u00edt\u00e9sek, ciklusok, egyszer\u0171 sz\u00f6vegm\u0171veletek t\u00e1mogatottak. Mi most csak a v\u00e1ltoz\u00f3 behelyettes\u00edt\u00e9st fogjuk haszn\u00e1lni. T\u00f6r\u00f6lj\u00fck ki a templates k\u00f6nyvt\u00e1rb\u00f3l az \u00f6sszes f\u00e1jlt a _helpers.tpl kiv\u00e9tel\u00e9vel. M\u00e1soljuk helyette be ide a kor\u00e1bban a telep\u00edt\u00e9shez haszn\u00e1lt yaml f\u00e1jljainkat (3 darab). Szerkessz\u00fck meg a todos.yaml f\u00e1jl tartalm\u00e1t. Leegyszer\u0171s\u00edtve az al\u00e1bbiakra lesz sz\u00fcks\u00e9g: Ha Visual Studio Code -ot haszn\u00e1lunk, akkor telep\u00edts\u00fck a ms-kubernetes-tools.vscode-kubernetes-tools extension-t. \u00cdgy kapunk n\u00e9mi seg\u00edts\u00e9get a szintaktik\u00e1val. Mindenhol, ahol labels vagy matchLabels szerepel, m\u00e9g egy sort fel kell venn\u00fcnk: app.kubernetes.io/instance: {{ .Release.Name }} Ez egy implicit v\u00e1ltoz\u00f3t helyettes\u00edt be: a release nev\u00e9t. Ezzel azonos\u00edtja a Helm a telep\u00edt\u00e9s \u00e9s friss\u00edt\u00e9s sor\u00e1n, hogy mely elemeket kell friss\u00edtenie, melyek tartoznak a fennhat\u00f3s\u00e1ga al\u00e1. A pod-ban az image be\u00e1ll\u00edt\u00e1s\u00e1hoz haszn\u00e1ljunk v\u00e1ltoz\u00f3t: image: todoapp/todos: {{ .Values.todos.tag }} Defini\u00e1ljuk az el\u0151bbi v\u00e1ltoz\u00f3 alap\u00e9rtelmezett \u00e9rt\u00e9k\u00e9t. A values.yaml f\u00e1jlban (egy k\u00f6nyvt\u00e1rral feljebb) t\u00f6r\u00f6lj\u00fcnk ki mindent, \u00e9s vegy\u00fck fel ezt a kulcsot: todos : tag : v1 A m\u00e1sik k\u00e9t komponens yaml le\u00edr\u00f3ival is hasonl\u00f3an kell elj\u00e1rnunk. A tov\u00e1bbiakhoz el kell t\u00e1vol\u00edtanunk az el\u0151bb telep\u00edtett alkalmaz\u00e1sunkat, mert \u00f6sszeakadna a Helm-mel. Ezt a parancsot a telep\u00edt\u00e9shez kor\u00e1bban haszn\u00e1lt app k\u00f6nyvt\u00e1rban adjuk ki: kubectl delete -f app N\u00e9zz\u00fck meg a template-eket ki\u00e9rt\u00e9kelve. A chartunk k\u00f6nyvt\u00e1r\u00e1b\u00f3l l\u00e9pj\u00fcnk eggyel feljebb, hogy a todoapp chart k\u00f6nyvt\u00e1r az aktu\u00e1lis k\u00f6nyvt\u00e1rban legyen. Futtassuk le csak a template gener\u00e1l\u00e1st a telep\u00edt\u00e9s n\u00e9lk\u00fcl: helm install todoapp --debug --dry-run todoapp Konzolra megkapjuk a ki\u00e9rt\u00e9kelt yaml-\u00f6ket. Telep\u00edts\u00fck \u00fajra az alkalmaz\u00e1st a chart seg\u00edts\u00e9g\u00e9vel: helm upgrade todoapp --install todoapp A release-nek todoapp nevet v\u00e1lasztottunk. Ez a Helm release azonos\u00edt\u00f3ja. Az upgrade parancs \u00e9s az --install kapcsol\u00f3 telep\u00edt, ha nem l\u00e9tezik, ill. friss\u00edt, ha m\u00e1r l\u00e9tezik ilyen telep\u00edt\u00e9s. N\u00e9zz\u00fck meg, hogy a Helm szerint l\u00e9tezik-e a release: helm list Pr\u00f3b\u00e1ljuk ki az alkalmaz\u00e1st a http://localhost:30080 c\u00edmen. Ezen chart seg\u00edts\u00e9g\u00e9vel a Docker image tag-et telep\u00edt\u00e9si param\u00e9terben adhatjuk \u00e1t, pl. ha a \"v2\" az \u00faj tag, akkor egy paranccsal tudjuk friss\u00edteni: helm upgrade todoapp --install todoapp --set todos.tag=v2","title":"Helm chart k\u00e9sz\u00edt\u00e9se"},{"location":"Serverless/serverless/","text":"El\u0151ad\u00e1s \u00b6 Serverless, Functionasa Service (FaaS) Azure Functions referenciaarchitekt\u00far\u00e1k mikroszolg\u00e1ltat\u00e1sokhoz \u00b6 Content Reactor Sample Building microservices on Azure Orchestrator or serverless Azure Functions feladatok \u00b6 Monolitikus alkalmaz\u00e1s refaktor\u00e1l\u00e1sa serverless mikroszolg\u00e1ltat\u00e1ss\u00e1 T\u00f6bb Azure Function \u00f6sszefog\u00e1sa API Management szolg\u00e1ltat\u00e1ssal","title":"Mikroszolg\u00e1ltat\u00e1sok serverless architekt\u00fara f\u00f6l\u00f6tt"},{"location":"Serverless/serverless/#eloadas","text":"Serverless, Functionasa Service (FaaS)","title":"El\u0151ad\u00e1s"},{"location":"Serverless/serverless/#azure-functions-referenciaarchitekturak-mikroszolgaltatasokhoz","text":"Content Reactor Sample Building microservices on Azure Orchestrator or serverless","title":"Azure Functions referenciaarchitekt\u00far\u00e1k mikroszolg\u00e1ltat\u00e1sokhoz"},{"location":"Serverless/serverless/#azure-functions-feladatok","text":"Monolitikus alkalmaz\u00e1s refaktor\u00e1l\u00e1sa serverless mikroszolg\u00e1ltat\u00e1ss\u00e1 T\u00f6bb Azure Function \u00f6sszefog\u00e1sa API Management szolg\u00e1ltat\u00e1ssal","title":"Azure Functions feladatok"}]}